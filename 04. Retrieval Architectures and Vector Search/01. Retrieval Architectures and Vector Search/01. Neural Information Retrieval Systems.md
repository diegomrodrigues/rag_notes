## Arquiteturas de Recupera√ß√£o e Busca Vetorial em Sistemas Neurais de IR: Implementa√ß√£o e Otimiza√ß√£o

### Introdu√ß√£o
Este cap√≠tulo explora a implementa√ß√£o e otimiza√ß√£o de arquiteturas de recupera√ß√£o em sistemas de Neural Information Retrieval (IR), expandindo sobre a se√ß√£o 4 do contexto fornecido [^3]. Em particular, analisaremos como os sistemas neurais de IR, descritos at√© agora, s√£o efetivamente integrados em sistemas completos, com foco no uso de *cross-encoders* e *bi-encoders* em arquiteturas de ranking. Abordaremos ainda a import√¢ncia da pr√©-computa√ß√£o de *document embeddings* para permitir um armazenamento e busca eficientes, elementos essenciais para a escalabilidade de sistemas de IR baseados em redes neurais [^21].

### Arquiteturas de Recupera√ß√£o e o Uso de Cross-encoders e Bi-encoders
Os sistemas neurais de IR, conforme mencionado na introdu√ß√£o [^5], utilizam redes neurais para aprender as fun√ß√µes de ranking e as representa√ß√µes abstratas de documentos e *queries*. Para realizar o ranking em larga escala, √© fundamental empregar arquiteturas eficientes. Uma abordagem comum √© o uso de *cross-encoders* e *bi-encoders*.

Conforme citado na se√ß√£o 4.1 [^28], os modelos de linguagem pr√©-treinados melhoram significativamente a efic√°cia dos sistemas de IR, mas s√£o computacionalmente caros. Devido a esse custo computacional, os sistemas focados na intera√ß√£o n√£o s√£o aplicados diretamente na cole√ß√£o de documentos, ou seja, para classificar todos os documentos que correspondem a uma *query*. Eles s√£o implementados em uma arquitetura *pipelined* [^28].

Os *cross-encoders*, detalhados na se√ß√£o 2.3 e 2.4 [^16, 18], recebem como entrada um par de textos (*query* e documento) e calculam uma pontua√ß√£o de relev√¢ncia. Dada a necessidade de se combinar a *query* com todos os documentos, esses modelos n√£o s√£o adequados para a classifica√ß√£o inicial [^17].

Os *bi-encoders*, conforme discutido na se√ß√£o 3 [^22], computam representa√ß√µes separadas para *queries* e documentos, permitindo o pr√©-c√°lculo e o armazenamento dos *embeddings* dos documentos. No momento do processamento da *query*, apenas o *embedding* da *query* precisa ser calculado, e os documentos s√£o classificados com base na similaridade entre os *embeddings* da *query* e do documento.

A arquitetura de *pipeline* descrita na Figura 7 [^29] envolve duas etapas principais:
1. **Recupera√ß√£o de candidatos**: Uma etapa inicial de ranking preliminar recupera um n√∫mero limitado de candidatos (normalmente 1000 documentos) [^28]. Esta etapa √© realizada usando m√©todos eficientes, como a busca em √≠ndices invertidos ou algoritmos de vizinhos mais pr√≥ximos aproximados (ANN).
2. **Re-ranking neural**: Os candidatos recuperados s√£o ent√£o reclassificados por um sistema neural de reclassifica√ß√£o mais caro, como um *cross-encoder* [^29]. Essa etapa permite uma modelagem mais precisa da relev√¢ncia, aproveitando a capacidade dos modelos neurais de capturar intera√ß√µes complexas entre a *query* e o documento.

![Re-ranking pipeline architecture for interaction-focused neural IR systems.](./../images/image1.png)

O benef√≠cio mais importante dos *bi-encoders*, discutido na se√ß√£o 3, √© a possibilidade de pr√©-calcular e armazenar em *cache* as representa√ß√µes de um grande *corpus* de documentos com o codificador de representa√ß√£o de documento aprendido œà(d) [^29]. No momento do processamento da *query*, o codificador de representa√ß√£o de *query* aprendido deve calcular apenas a representa√ß√£o da *query* œÜ(q); em seguida, os documentos s√£o classificados de acordo com o produto interno de sua representa√ß√£o com o *embedding* da *query*, e os k documentos superiores cujos *embeddings* t√™m o maior produto interno em rela√ß√£o ao *embedding* da *query* s√£o retornados ao usu√°rio (Figura 8) [^29].

![Dense retrieval architecture using representation-focused neural networks.](./../images/image2.png)

Para formalizar a etapa de ranking com bi-encoders, podemos definir a fun√ß√£o de similaridade *s(q, d)* entre uma *query* *q* e um documento *d* como:

$s(q, d) = \varphi(q) \cdot \psi(d)$

onde *œÜ(q)* √© o *embedding* da *query* e *œà(d)* √© o *embedding* do documento, e "‚ãÖ" representa o produto interno.

> üí° **Exemplo Num√©rico:**
> Suponha que temos uma *query* *q* e dois documentos, *d1* e *d2*. Ap√≥s passar pelo *bi-encoder*, obtemos os seguintes *embeddings* (vetores simplificados para fins ilustrativos):
> *   œÜ(q) = [0.8, 0.6]
> *   œà(d1) = [0.7, 0.3]
> *   œà(d2) = [0.9, 0.5]
>
> Podemos calcular a similaridade usando o produto interno:
>
> *   s(q, d1) = (0.8 * 0.7) + (0.6 * 0.3) = 0.56 + 0.18 = 0.74
> *   s(q, d2) = (0.8 * 0.9) + (0.6 * 0.5) = 0.72 + 0.30 = 1.02
>
> Neste caso, o documento *d2* seria classificado como mais relevante para a *query* *q* do que o documento *d1*, pois *s(q, d2) > s(q, d1)*.

**Teorema 1** A complexidade da etapa de ranking utilizando bi-encoders com produto interno √© $O(N*D)$, onde $N$ √© a dimens√£o dos *embeddings* e $D$ √© o n√∫mero de documentos.

*Prova*: O c√°lculo do produto interno entre o *embedding* da *query* e cada *embedding* de documento requer $N$ opera√ß√µes. Como isso precisa ser feito para cada um dos $D$ documentos, a complexidade total √© $O(N*D)$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Se tivermos $N$ = 768 (a dimens√£o do *embedding*) e $D$ = 1 milh√£o de documentos (1,000,000), a complexidade da opera√ß√£o de ranking seria proporcional a 768 * 1,000,000 = 768,000,000 opera√ß√µes. Isso demonstra a necessidade de otimiza√ß√µes como ANN.

**Lema 1** Utilizando algoritmos de vizinhos mais pr√≥ximos aproximados (ANN), a complexidade da etapa de ranking pode ser reduzida para sublinear em rela√ß√£o ao n√∫mero de documentos $D$.

*Prova*: Algoritmos ANN, como HNSW (Hierarchical Navigable Small World) ou Faiss, constroem estruturas de √≠ndice que permitem a busca aproximada dos vizinhos mais pr√≥ximos em tempo sublinear. A complexidade da busca depende da estrutura do √≠ndice e dos par√¢metros de aproxima√ß√£o, mas tipicamente √© $O(\log D)$ ou $O(D^\alpha)$, onde $\alpha$ < 1. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Usando HNSW, a complexidade pode ser reduzida para $O(\log D)$. Para $D$ = 1,000,000, $log(1,000,000)$ ‚âà 6 (base 10) ou ‚âà 13.8 (base e).  Isto significa que em vez de comparar a *query* com todos os 1,000,000 documentos, a busca se concentra em uma fra√ß√£o muito menor, drasticamente reduzindo o tempo de busca. Note que o valor exato de $log D$ depender√° da base do logaritmo utilizada, que por sua vez est√° relacionada aos par√¢metros espec√≠ficos do algoritmo ANN.

### Pr√©-computa√ß√£o e Armazenamento de Document Embeddings
Um dos desafios centrais na implementa√ß√£o de sistemas de IR baseados em redes neurais √© o custo computacional associado √† infer√™ncia, especialmente quando se trata de grandes cole√ß√µes de documentos [^22]. Para mitigar esse problema, a maioria dos sistemas de *dense retrieval* pr√©-computa os *document embeddings*. Essa abordagem envolve o c√°lculo offline dos *embeddings* para todos os documentos na cole√ß√£o e o armazenamento dessas representa√ß√µes em uma estrutura de dados especializada, conhecida como *embedding index* [^29].

A escolha da estrutura de dados para o *embedding index* √© crucial para o desempenho do sistema. Al√©m dos algoritmos ANN mencionados anteriormente, outras op√ß√µes incluem √°rvores KD e t√©cnicas de *hashing*. A sele√ß√£o depende do tamanho da cole√ß√£o de documentos, da dimens√£o dos *embeddings* e dos requisitos de precis√£o e velocidade da busca.

![Ranking pipeline architecture for multiple representation systems using learned embeddings and ANN search.](./../images/image3.png)

**Teorema 1.1** O uso de quantiza√ß√£o de vetores para compress√£o dos *document embeddings* pode reduzir significativamente os requisitos de armazenamento, com um impacto aceit√°vel na precis√£o da busca.

*Prova*: A quantiza√ß√£o de vetores agrupa *embeddings* similares em *clusters* e representa cada *embedding* pelo centroide do seu *cluster*. Isso reduz o n√∫mero de bits necess√°rios para armazenar cada *embedding*. A perda de precis√£o depende do n√∫mero de *clusters* e da vari√¢ncia dentro de cada *cluster*, mas pode ser controlada atrav√©s de t√©cnicas como a quantiza√ß√£o de produto. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Considere um *embedding* de dimens√£o 768 armazenado como *float32* (4 bytes por dimens√£o), ocupando 768 * 4 = 3072 bytes. Se aplicarmos quantiza√ß√£o de vetores para reduzir a representa√ß√£o para *int8* (1 byte por dimens√£o), o espa√ßo ocupado passa a ser 768 * 1 = 768 bytes, uma redu√ß√£o de aproximadamente 75% no tamanho do armazenamento, com uma poss√≠vel (e control√°vel) perda de precis√£o.
>
> | Tipo de Dado  | Bytes por Dimens√£o | Tamanho do Embedding (768 dimens√µes) |
> |---------------|--------------------|--------------------------------------|
> | float32       | 4                  | 3072 bytes                           |
> | float16       | 2                  | 1536 bytes                           |
> | int8 (Quantizado) | 1                  | 768 bytes                            |

A pr√©-computa√ß√£o dos *document embeddings* permite que o sistema execute a busca e o ranking em tempo real de forma mais eficiente. No momento da *query*, o sistema calcula o *embedding* da *query* e, em seguida, utiliza o *embedding index* para identificar os documentos mais relevantes com base na similaridade entre os *embeddings* da *query* e do documento [^29].

### Conclus√£o

As arquiteturas de recupera√ß√£o e busca vetorial desempenham um papel crucial na implementa√ß√£o eficaz de sistemas neurais de IR. A combina√ß√£o de *cross-encoders* e *bi-encoders*, juntamente com a pr√©-computa√ß√£o e o armazenamento eficiente de *document embeddings*, permite que esses sistemas alcancem um desempenho de √∫ltima gera√ß√£o em tarefas de ranking, mantendo a escalabilidade e a efici√™ncia necess√°rias para lidar com grandes cole√ß√µes de documentos.

### Refer√™ncias
[^3]: Tabela de Conte√∫do do documento.
[^5]: Introdu√ß√£o do documento.
[^16]: Se√ß√£o 2.3 do documento, "Ranking with Encoder-only Models".
[^17]: Se√ß√£o 2.3 do documento, "Ranking with Encoder-only Models".
[^18]: Se√ß√£o 2.4 do documento, "Ranking with Encoder-decoder Models".
[^21]: Se√ß√£o 3 do documento, "Representation-focused Systems".
[^22]: Se√ß√£o 3.1 do documento, "Single Representations".
[^28]: Se√ß√£o 4.1 do documento, "Retrieval architectures".
[^29]: Se√ß√£o 4.2 do documento, "MIP and NN Search Problems".
<!-- END -->