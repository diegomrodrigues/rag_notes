## Busca Eficiente em Espa√ßos Vetoriais em Sistemas de Dense Retrieval

### Introdu√ß√£o
Este cap√≠tulo aprofunda a discuss√£o sobre **retrieval architectures and vector search**, especificamente em sistemas de *dense retrieval*. Expandindo os conceitos introduzidos na se√ß√£o anterior sobre *representation-focused systems* [^21], exploraremos como esses sistemas abordam o problema de busca em espa√ßos vetoriais e a import√¢ncia do *embedding index*. Sistemas de *dense retrieval* s√£o caracterizados pelo uso de representa√ß√µes vetoriais densas para documentos e queries, permitindo a aplica√ß√£o de opera√ß√µes alg√©bricas para calcular a relev√¢ncia. Uma etapa crucial nesses sistemas √© a busca eficiente de documentos relevantes no espa√ßo vetorial, que discutiremos em detalhes.

### Problemas de Busca em Espa√ßos Vetoriais e √çndices de Embeddings
Conforme mencionado anteriormente, os sistemas de *dense retrieval* dependem da capacidade de calcular eficientemente a similaridade entre representa√ß√µes vetoriais de queries e documentos [^22]. Essa similaridade √© tipicamente medida pelo *inner product* (produto interno).
Dado um *query embedding* $\phi \in \mathbb{R}^l$ e um conjunto de *document embeddings* $\Psi = \{\psi_1, ..., \psi_n\}$, onde $\psi_i \in \mathbb{R}^l$ para $i = 1, ..., n$, o objetivo do *Maximum Inner Product Search (MIP)* √© encontrar o *document embedding* $\psi^* \in \Psi$ que maximize o produto interno com $\phi$ [^29]:

$$
\psi^* = \arg \max_{\psi \in \Psi} \langle \phi, \psi \rangle \qquad (18)
$$

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um *query embedding* $\phi = [0.8, 0.6]$ e um conjunto de dois *document embeddings*: $\psi_1 = [0.7, 0.3]$ e $\psi_2 = [0.2, 0.9]$.  Calcularemos o produto interno para cada documento:
>
> $\langle \phi, \psi_1 \rangle = (0.8 * 0.7) + (0.6 * 0.3) = 0.56 + 0.18 = 0.74$
> $\langle \phi, \psi_2 \rangle = (0.8 * 0.2) + (0.6 * 0.9) = 0.16 + 0.54 = 0.70$
>
> Neste caso, $\psi^* = \psi_1$ pois tem o maior produto interno com a query.

Para tornar essa busca eficiente, os *document embeddings* s√£o armazenados em uma estrutura de dados chamada *embedding index* [^29]. A forma mais simples de *embedding index* √© o *flat index*, que armazena os *document embeddings* explicitamente e realiza uma busca exaustiva para identificar $\psi^*$. A complexidade dessa abordagem √© $O(nl)$ tanto em espa√ßo quanto em tempo, tornando-a ineficiente para valores grandes de $n$ (n√∫mero de documentos) ou $l$ (dimensionalidade dos embeddings).

> üí° **Exemplo Num√©rico:**
>
> Imagine que temos 1 milh√£o de documentos ($n = 1,000,000$) e cada embedding tem 128 dimens√µes ($l = 128$).  Um *flat index* exigiria armazenar $1,000,000 * 128 = 128,000,000$ valores. Se cada valor for um *float32* (4 bytes), isso corresponde a 512MB de apenas embeddings. Uma busca exaustiva envolveria $1,000,000$ c√°lculos de produto interno, cada um com 128 multiplica√ß√µes e 127 adi√ß√µes. Essa opera√ß√£o se torna muito lenta com o aumento do n√∫mero de documentos.

Uma abordagem comum para melhorar a efici√™ncia espacial e temporal do *flat index* √© converter o problema do *maximum inner product search* em um problema de *nearest neighbor (NN) search*. O objetivo do *NN search* √© encontrar o *document embedding* $\psi^+$ que minimize a dist√¢ncia entre o *query embedding* e os *document embeddings* [^30]:

$$
\psi^+ = \arg \min_{\psi \in \Psi} ||\phi - \psi|| \qquad (19)
$$

> üí° **Exemplo Num√©rico:**
>
> Usando os mesmos embeddings $\phi = [0.8, 0.6]$, $\psi_1 = [0.7, 0.3]$, e $\psi_2 = [0.2, 0.9]$, calcularemos a dist√¢ncia Euclidiana para cada documento:
>
> $|| \phi - \psi_1 || = \sqrt{(0.8 - 0.7)^2 + (0.6 - 0.3)^2} = \sqrt{0.01 + 0.09} = \sqrt{0.1} \approx 0.316$
> $|| \phi - \psi_2 || = \sqrt{(0.8 - 0.2)^2 + (0.6 - 0.9)^2} = \sqrt{0.36 + 0.09} = \sqrt{0.45} \approx 0.671$
>
> Neste caso, $\psi^+ = \psi_1$ pois tem a menor dist√¢ncia Euclidiana em rela√ß√£o √† query.

Existem diversas estruturas de dados indexadas eficientes para *NN search*. Para utiliz√°-las com *embedding indexes*, o *MIP search* deve ser adaptado para usar a dist√¢ncia Euclidiana e o *NN search*. Isso pode ser alcan√ßado aplicando a seguinte transforma√ß√£o de $\mathbb{R}^l$ para $\mathbb{R}^{l+1}$ [^30]:

$$
\hat{\phi} = \begin{bmatrix} 0 \\ \phi / M \end{bmatrix}, \quad
\hat{\psi} = \begin{bmatrix} \sqrt{1 - ||\psi||^2 / M^2} \\ \psi / M \end{bmatrix} \qquad (20)
$$

onde $M = \max_{\psi \in \Psi} ||\psi||$ [^30]. Usando essa transforma√ß√£o, a solu√ß√£o do *MIP search* $\psi^*$ coincide com a solu√ß√£o do *NN search* $\hat{\psi}^+$.

> üí° **Exemplo Num√©rico:**
>
> Continuemos com $\phi = [0.8, 0.6]$ e $\psi_1 = [0.7, 0.3]$, $\psi_2 = [0.2, 0.9]$. Primeiro, calculemos as normas:
>
> $|| \psi_1 || = \sqrt{0.7^2 + 0.3^2} = \sqrt{0.49 + 0.09} = \sqrt{0.58} \approx 0.76$
> $|| \psi_2 || = \sqrt{0.2^2 + 0.9^2} = \sqrt{0.04 + 0.81} = \sqrt{0.85} \approx 0.92$
>
> Ent√£o, $M = \max(0.76, 0.92) = 0.92$. Agora transformemos os vetores:
>
> $\hat{\phi} = \begin{bmatrix} 0 \\ 0.8 / 0.92 \\ 0.6 / 0.92 \end{bmatrix} \approx \begin{bmatrix} 0 \\ 0.87 \\ 0.65 \end{bmatrix}$
> $\hat{\psi_1} = \begin{bmatrix} \sqrt{1 - (0.76^2 / 0.92^2)} \\ 0.7 / 0.92 \\ 0.3 / 0.92 \end{bmatrix} \approx \begin{bmatrix} \sqrt{1 - 0.679 / 0.846} \\ 0.76 \\ 0.33 \end{bmatrix} \approx \begin{bmatrix} 0.40 \\ 0.76 \\ 0.33 \end{bmatrix}$
> $\hat{\psi_2} = \begin{bmatrix} \sqrt{1 - (0.92^2 / 0.92^2)} \\ 0.2 / 0.92 \\ 0.9 / 0.92 \end{bmatrix} \approx \begin{bmatrix} 0 \\ 0.22 \\ 0.98 \end{bmatrix}$
>
> Calculemos a dist√¢ncia Euclidiana entre $\hat{\phi}$ e cada $\hat{\psi}$:
>
> $|| \hat{\phi} - \hat{\psi_1} || = \sqrt{(0 - 0.40)^2 + (0.87 - 0.76)^2 + (0.65 - 0.33)^2} = \sqrt{0.16 + 0.0121 + 0.1024} = \sqrt{0.2745} \approx 0.52$
> $|| \hat{\phi} - \hat{\psi_2} || = \sqrt{(0 - 0)^2 + (0.87 - 0.22)^2 + (0.65 - 0.98)^2} = \sqrt{0 + 0.4225 + 0.1089} = \sqrt{0.5314} \approx 0.73$
>
> $\hat{\psi}^+ = \hat{\psi_1}$ pois tem a menor dist√¢ncia. Notavelmente, usando a transforma√ß√£o e a dist√¢ncia Euclidiana, encontramos o mesmo documento mais similar que encontramos usando o produto interno original. Este exemplo ilustra como a transforma√ß√£o permite usar algoritmos de NN search para resolver problemas de MIP search.

**Prova:**
Minimizar a dist√¢ncia Euclidiana entre os embeddings transformados √© equivalente a maximizar o produto interno:

$$
\min ||\hat{\phi} - \hat{\psi}||^2 = \min (||\hat{\phi}||^2 + ||\hat{\psi}||^2 - 2 \langle \hat{\phi}, \hat{\psi} \rangle) = \min (2 - 2 \langle \phi, \psi \rangle / M) = \max \langle \phi, \psi \rangle.
$$

$\blacksquare$

As estruturas de dados de √≠ndice para *NN search* exato em espa√ßos de baixa dimens√£o t√™m sido bem-sucedidas, mas s√£o ineficientes com dados de alta dimens√£o, como no nosso caso, devido √† *curse of dimensionality*. Portanto, √© necess√°rio fazer um compromisso entre a precis√£o da busca e a velocidade da busca, e os m√©todos de busca mais recentes mudaram para a *approximate nearest neighbor (ANN) search* [^30].

**Observa√ß√£o:** A transforma√ß√£o apresentada em (20) assume que todos os vetores $\psi \in \Psi$ possuem norma menor ou igual a $M$. √â importante garantir que essa condi√ß√£o seja satisfeita para que a equival√™ncia entre MIP e NN search se mantenha. Al√©m disso, a escolha de $M$ influencia na magnitude dos vetores transformados, o que pode afetar o desempenho de algumas estruturas de dados para NN search.

**Teorema 1** (Rela√ß√£o entre Produto Interno e Dist√¢ncia Euclidiana). Dados dois vetores $u, v \in \mathbb{R}^d$, a seguinte rela√ß√£o √© v√°lida:

$$||u - v||^2 = ||u||^2 + ||v||^2 - 2\langle u, v \rangle$$

**Prova:**
Expandindo o quadrado da norma Euclidiana:

$$||u - v||^2 = \langle u - v, u - v \rangle = \langle u, u \rangle - 2\langle u, v \rangle + \langle v, v \rangle = ||u||^2 + ||v||^2 - 2\langle u, v \rangle$$

$\blacksquare$

Essa rela√ß√£o √© fundamental para entender a conex√£o entre a busca por similaridade via produto interno e a busca por vizinhos mais pr√≥ximos via dist√¢ncia Euclidiana.

> üí° **Exemplo Num√©rico:**
>
> Suponha $u = [1, 2]$ e $v = [3, 4]$.
>
> $||u - v||^2 = ||[1-3, 2-4]||^2 = ||[-2, -2]||^2 = (-2)^2 + (-2)^2 = 4 + 4 = 8$
>
> $||u||^2 = 1^2 + 2^2 = 1 + 4 = 5$
> $||v||^2 = 3^2 + 4^2 = 9 + 16 = 25$
>
> $\langle u, v \rangle = (1 * 3) + (2 * 4) = 3 + 8 = 11$
>
> $||u||^2 + ||v||^2 - 2\langle u, v \rangle = 5 + 25 - 2 * 11 = 30 - 22 = 8$
>
> Como demonstrado, $||u - v||^2 = ||u||^2 + ||v||^2 - 2\langle u, v \rangle$

### Conclus√£o
Esta se√ß√£o focou nos desafios de busca em espa√ßos vetoriais para sistemas de *dense retrieval*. A convers√£o do problema de *MIP search* em *NN search*, juntamente com o uso de t√©cnicas de *approximate nearest neighbor (ANN) search* [^30], representa uma abordagem fundamental para lidar com a escalabilidade e a efici√™ncia nesses sistemas. Nas pr√≥ximas se√ß√µes, exploraremos solu√ß√µes espec√≠ficas para o armazenamento e a busca eficientes de vetores em detalhes.

### Refer√™ncias
[^21]: Nicola Tonellotto, Lecture Notes on Neural Information Retrieval, *Representation-focused Systems*, p. 21
[^22]: Nicola Tonellotto, Lecture Notes on Neural Information Retrieval, *Representation-focused Systems*, p. 22
[^29]: Nicola Tonellotto, Lecture Notes on Neural Information Retrieval, *MIP and NN Search Problems*, p. 29
[^30]: Nicola Tonellotto, Lecture Notes on Neural Information Retrieval, *MIP and NN Search Problems*, p. 30
<!-- END -->