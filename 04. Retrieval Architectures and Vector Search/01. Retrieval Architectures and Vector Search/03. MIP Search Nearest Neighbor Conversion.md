Como discutido em [^30], a busca pelo **Maximum Inner Product (MIP)** busca encontrar o documento $Ïˆ^*$ dentro de um conjunto de $n$ documentos ($Y = \{Ïˆ_1, \ldots, Ïˆ_n\}$), tal que maximize o produto interno com a *query embedding* Ï†, ou seja:

$$Ïˆ^* = \underset{Ïˆ \in Î¨}{\operatorname{arg\,max}} \langle Ï†, Ïˆ \rangle$$

Para melhorar a eficiÃªncia da busca, o texto [^30] introduz uma tÃ©cnica que converte o problema de MIP em um problema de **Nearest Neighbor (NN)**, cujo objetivo Ã© encontrar o documento $Ïˆ^\dagger$ que minimiza a distÃ¢ncia euclidiana em relaÃ§Ã£o Ã  *query embedding*:

$$Ïˆ^\dagger = \underset{Ïˆ \in Î¨}{\operatorname{arg\,min}} ||Ï† - Ïˆ||$$

Essa conversÃ£o Ã© crucial porque existem estruturas de dados de Ã­ndice altamente otimizadas para a busca de vizinhos mais prÃ³ximos. No entanto, para que essa conversÃ£o seja vÃ¡lida, Ã© necessÃ¡rio transformar os *embeddings* originais de forma que a soluÃ§Ã£o para o problema de NN corresponda Ã  soluÃ§Ã£o do problema de MIP.

A transformaÃ§Ã£o envolve o mapeamento dos *embeddings* de um espaÃ§o original $\mathbb{R}^l$ para um novo espaÃ§o $\mathbb{R}^{l+1}$ [^30], conforme as seguintes equaÃ§Ãµes:

$$\hat{Ï†} = \begin{bmatrix}
0 \\
\frac{Ï†}{M}
\end{bmatrix}, \quad
\hat{Ïˆ} = \begin{bmatrix}
\sqrt{1 - \frac{||Ïˆ||^2}{M^2}} \\
\frac{Ïˆ}{M}
\end{bmatrix}$$

onde $M = \underset{Ïˆ \in Î¨}{\operatorname{max}} ||Ïˆ||$ representa o maior valor absoluto das normas dos *embeddings* dos documentos no conjunto $Y$. A constante $M$ Ã© crucial para garantir que a transformaÃ§Ã£o seja bem definida e que todos os termos dentro da raiz quadrada sejam nÃ£o-negativos.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos trÃªs documentos representados por seus embeddings: $Ïˆ_1 = [1, 2]$, $Ïˆ_2 = [2, 1]$, e $Ïˆ_3 = [0, -1]$.
>
> $\text{Passo 1: Calcular a norma de cada embedding.}$
>
> $||Ïˆ_1|| = \sqrt{1^2 + 2^2} = \sqrt{5} \approx 2.24$
>
> $||Ïˆ_2|| = \sqrt{2^2 + 1^2} = \sqrt{5} \approx 2.24$
>
> $||Ïˆ_3|| = \sqrt{0^2 + (-1)^2} = 1$
>
> $\text{Passo 2: Determinar } M, \text{ que Ã© o mÃ¡ximo das normas.}$
>
> $M = \max(\sqrt{5}, \sqrt{5}, 1) = \sqrt{5} \approx 2.24$
>
> Agora, suponha uma query com embedding $Ï† = [1, 0]$. Podemos calcular o produto interno entre a query e cada documento:
>
> $\langle Ï†, Ïˆ_1 \rangle = (1)(1) + (0)(2) = 1$
>
> $\langle Ï†, Ïˆ_2 \rangle = (1)(2) + (0)(1) = 2$
>
> $\langle Ï†, Ïˆ_3 \rangle = (1)(0) + (0)(-1) = 0$
>
> Pelo MIP, $Ïˆ_2$ Ã© o documento mais similar.
>
> Agora vamos transformar os embeddings usando o $M$ calculado.
>
> $\hat{Ï†} = \begin{bmatrix} 0 \\ \frac{[1, 0]}{\sqrt{5}} \end{bmatrix} = \begin{bmatrix} 0 \\ [\frac{1}{\sqrt{5}}, 0] \end{bmatrix} \approx \begin{bmatrix} 0 \\ [0.45, 0] \end{bmatrix}$
>
> $\hat{Ïˆ_1} = \begin{bmatrix} \sqrt{1 - \frac{(\sqrt{5})^2}{(\sqrt{5})^2}} \\ \frac{[1, 2]}{\sqrt{5}} \end{bmatrix} = \begin{bmatrix} 0 \\ [\frac{1}{\sqrt{5}}, \frac{2}{\sqrt{5}}] \end{bmatrix} \approx \begin{bmatrix} 0 \\ [0.45, 0.89] \end{bmatrix}$
>
> $\hat{Ïˆ_2} = \begin{bmatrix} \sqrt{1 - \frac{(\sqrt{5})^2}{(\sqrt{5})^2}} \\ \frac{[2, 1]}{\sqrt{5}} \end{bmatrix} = \begin{bmatrix} 0 \\ [\frac{2}{\sqrt{5}}, \frac{1}{\sqrt{5}}] \end{bmatrix} \approx \begin{bmatrix} 0 \\ [0.89, 0.45] \end{bmatrix}$
>
> $\hat{Ïˆ_3} = \begin{bmatrix} \sqrt{1 - \frac{(1)^2}{(\sqrt{5})^2}} \\ \frac{[0, -1]}{\sqrt{5}} \end{bmatrix} = \begin{bmatrix} \sqrt{1 - \frac{1}{5}} \\ [0, -\frac{1}{\sqrt{5}}] \end{bmatrix} = \begin{bmatrix} \sqrt{\frac{4}{5}} \\ [0, -\frac{1}{\sqrt{5}}] \end{bmatrix} \approx \begin{bmatrix} 0.89 \\ [0, -0.45] \end{bmatrix}$
>
> Agora, calculemos a distÃ¢ncia Euclidiana ao quadrado entre $\hat{Ï†}$ e cada $\hat{Ïˆ_i}$.
>
> $||\hat{Ï†} - \hat{Ïˆ_1}||^2 = (0-0)^2 + (0.45 - 0.45)^2 + (0 - 0.89)^2 = 0 + 0 + 0.7921 = 0.7921$
>
> $||\hat{Ï†} - \hat{Ïˆ_2}||^2 = (0-0)^2 + (0.45 - 0.89)^2 + (0 - 0.45)^2 = 0 + 0.1936 + 0.2025 = 0.3961$
>
> $||\hat{Ï†} - \hat{Ïˆ_3}||^2 = (0-0.89)^2 + (0.45 - 0)^2 + (0 - (-0.45))^2 = 0.7921 + 0.2025 + 0.2025 = 1.1971$
>
> O menor valor Ã© $0.3961$, correspondente a $Ïˆ_2$. Isso confirma que a transformaÃ§Ã£o preserva a ordem de similaridade.

**ObservaÃ§Ã£o:** Ã‰ importante notar que a escolha de $M$ como o *mÃ¡ximo* da norma dos vetores $Ïˆ$ garante que $1 - \frac{||Ïˆ||^2}{M^2} \geq 0$ para todo $Ïˆ \in Î¨$, assegurando que a raiz quadrada seja sempre um nÃºmero real. Uma escolha inadequada de $M$ poderia levar a valores imaginÃ¡rios, invalidando a transformaÃ§Ã£o para um espaÃ§o vetorial real.

A justificativa para essa transformaÃ§Ã£o reside na relaÃ§Ã£o entre a distÃ¢ncia euclidiana no espaÃ§o transformado e o produto interno no espaÃ§o original [^30]. Especificamente, mostrar que minimizar a distÃ¢ncia euclidiana entre os *embeddings* transformados $\hat{Ï†}$ e $\hat{Ïˆ}$ Ã© equivalente a maximizar o produto interno entre $Ï†$ e $Ïˆ$.

**Teorema 1** A transformaÃ§Ã£o definida pelas equaÃ§Ãµes acima preserva a ordem de similaridade entre os documentos e a *query*, ou seja, o documento mais similar Ã  *query* no espaÃ§o original (medido pelo produto interno) continua sendo o mais similar no espaÃ§o transformado (medido pela distÃ¢ncia Euclidiana).

**Prova:**

Considere a distÃ¢ncia euclidiana ao quadrado entre $\hat{Ï†}$ e $\hat{Ïˆ}$:

$$||\hat{Ï†} - \hat{Ïˆ}||^2 = ||\hat{Ï†}||^2 + ||\hat{Ïˆ}||^2 - 2 \langle \hat{Ï†}, \hat{Ïˆ} \rangle$$

Substituindo as definiÃ§Ãµes de $\hat{Ï†}$ e $\hat{Ïˆ}$:

$$||\hat{Ï†} - \hat{Ïˆ}||^2 = 0 + \frac{||Ï†||^2}{M^2} + 1 - \frac{||Ïˆ||^2}{M^2} - 2 \left( 0 \cdot \sqrt{1 - \frac{||Ïˆ||^2}{M^2}} + \frac{\langle Ï†, Ïˆ \rangle}{M^2} \right)$$
$$||\hat{Ï†} - \hat{Ïˆ}||^2 = \frac{||Ï†||^2}{M^2} + 1 - \frac{||Ïˆ||^2}{M^2} - 2 \frac{\langle Ï†, Ïˆ \rangle}{M^2} $$

Como $||Ï†||^2$, $||Ïˆ||^2$ e $M$ sÃ£o constantes para uma dada *query* e conjunto de documentos, minimizar $||\hat{Ï†} - \hat{Ïˆ}||^2$ Ã© equivalente a maximizar $\frac{\langle Ï†, Ïˆ \rangle}{M^2}$, e consequentemente, maximizar $\langle Ï†, Ïˆ \rangle$.  Matematicamente:

$$\underset{Ïˆ \in Î¨}{\operatorname{arg\,min}} ||\hat{Ï†} - \hat{Ïˆ}||^2 = \underset{Ïˆ \in Î¨}{\operatorname{arg\,max}} \langle Ï†, Ïˆ \rangle$$

Portanto, a soluÃ§Ã£o para o problema de NN no espaÃ§o transformado coincide com a soluÃ§Ã£o para o problema de MIP no espaÃ§o original.  $\blacksquare$

**CorolÃ¡rio 1.1** A complexidade da busca pelo vizinho mais prÃ³ximo no espaÃ§o transformado depende da estrutura de dados de indexaÃ§Ã£o utilizada. Estruturas como Ã¡rvores k-d ou HNSW (Hierarchical Navigable Small World) podem reduzir significativamente o tempo de busca em comparaÃ§Ã£o com uma busca exaustiva.

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um cenÃ¡rio onde a busca exaustiva leva 1 segundo para encontrar o vizinho mais prÃ³ximo em 10.000 documentos. Usando uma estrutura de dados como HNSW, o tempo de busca pode ser reduzido para 10 milissegundos (0.01 segundos). Isso representa uma melhoria de 100 vezes na velocidade de busca.
>
> | MÃ©todo de Busca | Tempo de Busca (s) |
> |-----------------|----------------------|
> | Busca Exaustiva | 1                    |
> | HNSW            | 0.01                 |

**Teorema 1.1** Seja $d(\hat{Ï†}, \hat{Ïˆ}) = ||\hat{Ï†} - \hat{Ïˆ}||^2$ a distÃ¢ncia euclidiana ao quadrado entre os embeddings transformados. EntÃ£o, existe uma constante $C = \frac{||Ï†||^2}{M^2} + 1$ tal que $d(\hat{Ï†}, \hat{Ïˆ}) = C - \frac{||Ïˆ||^2}{M^2} - 2 \frac{\langle Ï†, Ïˆ \rangle}{M^2}$.
*Proof.* Segue diretamente da prova do Teorema 1.
<!-- END -->