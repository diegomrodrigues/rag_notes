## Arquiteturas de Recupera√ß√£o e Busca Vetorial com PLMs

### Introdu√ß√£o

A utiliza√ß√£o de **Pre-trained Language Models (PLMs)** tem demonstrado um aumento significativo na efic√°cia dos sistemas de Information Retrieval (IR) [^41]. No entanto, o custo computacional associado ao emprego de PLMs, especialmente em tarefas de *ad-hoc ranking*, imp√µe desafios pr√°ticos na sua implementa√ß√£o direta sobre grandes cole√ß√µes de documentos [^28]. Este cap√≠tulo explora as arquiteturas de recupera√ß√£o que mitigam este problema, focando na combina√ß√£o de estrat√©gias de ranking preliminar com re-ranking utilizando modelos neurais mais complexos, e nas t√©cnicas de busca vetorial otimizadas para lidar com *embeddings* de documentos em larga escala.

### Arquiteturas de Recupera√ß√£o Pipelined

A computa√ß√£o intensiva inerente aos PLMs inviabiliza a sua aplica√ß√£o direta em toda a cole√ß√£o de documentos para cada consulta [^28]. Assim, adota-se uma arquitetura *pipelined* [^28] que divide o processo em etapas:

1.  **Ranking Preliminar:** Esta etapa tem como objetivo recuperar um subconjunto limitado de candidatos (t√≠picamente 1000 documentos) [^28] relevantes para a consulta. S√£o utilizados modelos menos complexos, como BM25, ou *bi-encoders*, para gerar uma lista inicial de candidatos de forma eficiente.

2.  **Re-Ranking Neural:** A lista de candidatos recuperada na etapa anterior √© ent√£o processada por um modelo neural mais sofisticado, como um *cross-encoder* [^28], para refinar o ranking e selecionar os documentos mais relevantes para o usu√°rio.

Esta abordagem *pipelined* permite equilibrar a precis√£o e o custo computacional, aproveitando a efici√™ncia dos modelos cl√°ssicos para a sele√ß√£o inicial e a capacidade dos modelos neurais para uma an√°lise mais profunda da relev√¢ncia [^28]. A Figura 7 [^29] ilustra esta arquitetura de re-ranking.

> A arquitetura *pipelined* permite equilibrar a precis√£o e o custo computacional, aproveitando a efici√™ncia dos modelos cl√°ssicos para a sele√ß√£o inicial e a capacidade dos modelos neurais para uma an√°lise mais profunda da relev√¢ncia.

![Re-ranking pipeline architecture for interaction-focused neural IR systems.](./../images/image1.png)

> üí° **Exemplo Num√©rico: Ranking Preliminar com BM25**
>
> Considere uma consulta "melhor restaurante italiano em S√£o Paulo" e tr√™s documentos:
>
> *   Documento 1: "O La Trattoria √© um restaurante italiano tradicional em S√£o Paulo, conhecido por suas massas frescas."
> *   Documento 2: "Comida italiana excelente e vinhos em um ambiente agrad√°vel. O Famiglia Mancini √© imperd√≠vel."
> *   Documento 3: "Este artigo fala sobre a hist√≥ria da culin√°ria brasileira."
>
> Usando BM25 com par√¢metros $k_1 = 1.2$ e $b = 0.75$, podemos calcular os scores de cada documento. Simplificando, vamos considerar apenas a frequ√™ncia dos termos "italiano" e "S√£o Paulo". Suponha que o tamanho m√©dio dos documentos na cole√ß√£o seja 100 palavras.
>
> | Termo       | Documento 1 | Documento 2 | Documento 3 |
> |-------------|-------------|-------------|-------------|
> | Italiano    | 1           | 1           | 0           |
> | S√£o Paulo   | 1           | 0           | 0           |
> | Comprimento | 20          | 15          | 25          |
>
> $\text{IDF(italiano)} = \log(\frac{N - n(italiano) + 0.5}{n(italiano) + 0.5})$, onde $N$ √© o n√∫mero total de documentos e $n(italiano)$ √© o n√∫mero de documentos que cont√™m "italiano".  Assumindo $N=1000$ e $n(italiano)=50$, $\text{IDF(italiano)} \approx 3$.
> $\text{IDF(S√£o Paulo)} = \log(\frac{N - n(S√£o Paulo) + 0.5}{n(S√£o Paulo) + 0.5})$.  Assumindo $N=1000$ e $n(S√£o Paulo)=100$, $\text{IDF(S√£o Paulo)} \approx 2.3$.
>
> $\text{BM25(Documento 1)} = \text{IDF(italiano)} \cdot \frac{(k_1 + 1) \cdot 1}{1 + k_1 \cdot (1 - b + b \cdot \frac{20}{100})} + \text{IDF(S√£o Paulo)} \cdot \frac{(k_1 + 1) \cdot 1}{1 + k_1 \cdot (1 - b + b \cdot \frac{20}{100})} \approx 3 \cdot \frac{2.2}{1 + 1.2 \cdot (1 - 0.75 + 0.75 \cdot 0.2)} + 2.3 \cdot \frac{2.2}{1 + 1.2 \cdot (1 - 0.75 + 0.75 \cdot 0.2)} \approx 2.7$
> $\text{BM25(Documento 2)} = \text{IDF(italiano)} \cdot \frac{(k_1 + 1) \cdot 1}{1 + k_1 \cdot (1 - b + b \cdot \frac{15}{100})} + \text{IDF(S√£o Paulo)} \cdot \frac{(k_1 + 1) \cdot 0}{1 + k_1 \cdot (1 - b + b \cdot \frac{15}{100})} \approx 3 \cdot \frac{2.2}{1 + 1.2 \cdot (1 - 0.75 + 0.75 \cdot 0.15)} \approx 2.8$
> $\text{BM25(Documento 3)} = 0$
>
> Ranking Preliminar: Documento 2 > Documento 1 > Documento 3.  Somente Documentos 1 e 2 seriam passados para o re-ranker.

Expandindo o conceito apresentado na Se√ß√£o 3, onde os *bi-encoders* s√£o utilizados para precomputar e armazenar em cache representa√ß√µes de um grande *corpus* de documentos [^29], a etapa de ranking preliminar pode ser realizada atrav√©s da busca no espa√ßo vetorial dessas representa√ß√µes. Neste contexto, o *learned query representation encoder* computa a representa√ß√£o da consulta, e os documentos s√£o ranqueados de acordo com o produto interno entre suas representa√ß√µes e a representa√ß√£o da consulta [^29]. A Figura 8 [^29] ilustra esta arquitetura de *dense retrieval*.

![Dense retrieval architecture using representation-focused neural networks.](./../images/image2.png)

> üí° **Exemplo Num√©rico: Dense Retrieval com Bi-encoders**
>
> Suponha que a consulta "melhor restaurante italiano em S√£o Paulo" seja codificada como o vetor $\phi = [0.1, 0.2, 0.3]$ e os documentos 1 e 2 (do exemplo anterior) sejam codificados como $\psi_1 = [0.2, 0.1, 0.4]$ e $\psi_2 = [0.3, 0.2, 0.1]$, respectivamente.  Esses vetores foram produzidos por um bi-encoder treinado.
>
> $\text{Produto Interno}(\phi, \psi_1) = (0.1 \cdot 0.2) + (0.2 \cdot 0.1) + (0.3 \cdot 0.4) = 0.02 + 0.02 + 0.12 = 0.16$
> $\text{Produto Interno}(\phi, \psi_2) = (0.1 \cdot 0.3) + (0.2 \cdot 0.2) + (0.3 \cdot 0.1) = 0.03 + 0.04 + 0.03 = 0.10$
>
> Neste caso, o Documento 1 teria um ranking mais alto do que o Documento 2, com base no produto interno de seus *embeddings* e o *embedding* da consulta. Este resultado difere do BM25, demonstrando como representa√ß√µes sem√¢nticas podem capturar nuances diferentes.

### Busca Vetorial: MIP e Nearest Neighbor

A busca eficiente no espa√ßo vetorial √© fundamental para a etapa de ranking preliminar. Formalmente, dado um *embedding* da consulta $\phi \in \mathbb{R}^l$ e um conjunto de *embeddings* de documentos $\Psi = \{\psi_1, \ldots, \psi_n\}$, onde $\psi_i \in \mathbb{R}^l$ para $i = 1,\ldots,n$, o objetivo da busca por **Maximum Inner Product (MIP)** √© encontrar o *embedding* do documento $\psi^* \in \Psi$ que maximize o produto interno com $\phi$ [^30]:

$$\psi^* = \arg \max_{\psi \in \Psi} (\phi, \psi)$$

Uma estrutura de dados projetada para armazenar $\Psi$ √© chamada de **embedding index**. Uma abordagem simples √© o **flat index**, que armazena os *embeddings* dos documentos explicitamente e realiza uma busca exaustiva para identificar $\psi^*$ [^30]. No entanto, a complexidade de $O(nl)$ tanto em espa√ßo quanto em tempo torna esta abordagem ineficiente para grandes valores de $n$ ou $l$ [^30].

Uma alternativa comum √© converter o problema de MIP em uma busca por **Nearest Neighbor (NN)**, que busca o *embedding* do documento $\psi^\dagger$ mais pr√≥ximo de $\phi$ [^30]:

$$\psi^\dagger = \arg \min_{\psi \in \Psi} ||\phi - \psi||$$

Existem diversas estruturas de dados eficientes para busca NN [^30]. Para utilizar estas estruturas com *embedding indexes*, a busca MIP precisa ser adaptada para utilizar a dist√¢ncia Euclidiana e busca NN. Isso pode ser feito atrav√©s da seguinte transforma√ß√£o de $\mathbb{R}^l$ para $\mathbb{R}^{l+1}$ [^30]:

$$\hat{\phi} = \begin{bmatrix} 0 \\ \phi / M \end{bmatrix}, \quad \hat{\psi} = \begin{bmatrix} \sqrt{1 - ||\psi||^2 / M^2} \\ \psi / M \end{bmatrix},$$

onde $M = \max_{\psi \in \Psi} ||\psi||$ [^30]. Com esta transforma√ß√£o, a solu√ß√£o para o problema de MIP $\psi^*$ coincide com a solu√ß√£o para o problema de busca NN $\hat{\psi}^\dagger$ [^30]. A busca NN com a dist√¢ncia Euclidiana √© dada por:

$$min ||\hat{\phi} - \hat{\psi}||^2 = min (||\hat{\phi}||^2 + ||\hat{\psi}||^2 - 2 \langle \hat{\phi}, \hat{\psi} \rangle) = max \langle \phi,\psi \rangle$$. $\blacksquare$

Ap√≥s essa transforma√ß√£o, pode-se considerar a busca por MIP como uma busca NN baseada na dist√¢ncia Euclidiana entre os *embeddings* transformados $\hat{\phi}$ e $\hat{\psi}$ em $\mathbb{R}^{l+1}$ [^30]. Para simplificar a nota√ß√£o, o s√≠mbolo $\hat{ }$ √© removido, considerando $l + 1$ como a nova dimens√£o $l$, ou seja, $l + 1 \rightarrow l$ [^30].

Embora as estruturas de dados para busca NN exata sejam eficientes em espa√ßos de baixa dimensionalidade, elas se tornam ineficientes em dados de alta dimensionalidade devido √† *maldi√ß√£o da dimensionalidade* [^30]. Portanto, m√©todos de busca aproximada de vizinhos mais pr√≥ximos (Approximate Nearest Neighbor - ANN) [^31] s√£o empregados para equilibrar a precis√£o e a velocidade de busca.

Para complementar a discuss√£o sobre a convers√£o de MIP para NN, podemos introduzir formalmente a rela√ß√£o entre o produto interno e a dist√¢ncia Euclidiana.

**Lema 1** Dado dois vetores $u, v \in \mathbb{R}^d$, a dist√¢ncia Euclidiana entre eles est√° relacionada ao seu produto interno pela seguinte equa√ß√£o:
$$||u - v||^2 = ||u||^2 + ||v||^2 - 2\langle u, v \rangle$$
*Prova:*
Expandindo o lado esquerdo, temos:
$$||u - v||^2 = \langle u - v, u - v \rangle = \langle u, u \rangle - \langle u, v \rangle - \langle v, u \rangle + \langle v, v \rangle = ||u||^2 + ||v||^2 - 2\langle u, v \rangle$$
Uma vez que $\langle u, v \rangle = \langle v, u \rangle$. $\blacksquare$

Portanto, maximizar o produto interno $\langle u, v \rangle$ √© equivalente a minimizar $||u - v||^2$ se $||u||^2$ e $||v||^2$ forem constantes ou normalizados. A transforma√ß√£o apresentada anteriormente normaliza os vetores, permitindo essa convers√£o.

### Otimiza√ß√µes e T√©cnicas de Busca Aproximada

Diversas t√©cnicas de busca aproximada de vizinhos mais pr√≥ximos (ANN) s√£o comumente empregadas em *dense retrieval*, incluindo:

*   **Locality Sensitive Hashing (LSH):** LSH [^31] baseia-se na ideia de que, se dois *embeddings* est√£o pr√≥ximos, eles permanecer√£o pr√≥ximos ap√≥s uma "proje√ß√£o" usando uma fun√ß√£o hash. Uma fam√≠lia de fun√ß√µes LSH deve garantir que *embeddings* pr√≥ximos tenham alta probabilidade de colidir no mesmo *bucket* e *embeddings* distantes tenham baixa probabilidade de colidir [^31].

*   **Vector Quantization:** Em vez de particionar o espa√ßo de entrada aleatoriamente como no LSH, a quantiza√ß√£o vetorial [^32] particiona o espa√ßo de acordo com a distribui√ß√£o dos dados. O algoritmo *k-means* √© usado para calcular $k$ centr√≥ides, que representam o *codebook*. Um *embedding* √© mapeado para o centr√≥ide mais pr√≥ximo, reduzindo o custo da computa√ß√£o da dist√¢ncia.

*   **Graph Approaches:** As dist√¢ncias entre os vetores em um conjunto de dados podem ser armazenadas eficientemente em uma estrutura de dados baseada em grafo, chamada *kNN graph* [^33]. Cada ponto de dado √© um n√≥, e as arestas conectam os $k$ vizinhos mais pr√≥ximos. A busca por vizinhos aproximados √© realizada atrav√©s de uma busca *greedy* no grafo.

Implementa√ß√µes dessas t√©cnicas est√£o dispon√≠veis em bibliotecas como FAISS [^34], que oferece implementa√ß√µes de *flat index*, LSH, IVF, PQ e HNSW.

Para expandir sobre as otimiza√ß√µes, vamos adicionar uma breve descri√ß√£o da t√©cnica IVF:

*   **Inverted File with Flat Index (IVF):** IVF [^34] combina a ideia de quantiza√ß√£o vetorial com um √≠ndice invertido. Primeiro, os vetores s√£o particionados em *clusters* usando *k-means*. Ent√£o, para cada *cluster*, um √≠ndice *flat* √© constru√≠do. Durante a busca, apenas os vetores nos *clusters* mais pr√≥ximos da consulta s√£o pesquisados, reduzindo significativamente o n√∫mero de compara√ß√µes necess√°rias. O n√∫mero de *clusters* a serem pesquisados √© um par√¢metro ajust√°vel que permite controlar o equil√≠brio entre precis√£o e velocidade.

Al√©m das t√©cnicas de ANN mencionadas, a escolha da m√©trica de similaridade desempenha um papel crucial na efic√°cia da busca vetorial. Embora a dist√¢ncia Euclidiana e o produto interno sejam amplamente utilizados, outras m√©tricas podem ser mais adequadas dependendo da natureza dos *embeddings*.

**Proposi√ß√£o 1** A similaridade do cosseno √© uma m√©trica amplamente utilizada em IR, especialmente quando os comprimentos dos vetores n√£o s√£o informativos. √â definida como:

$$cos(\phi, \psi) = \frac{\langle \phi, \psi \rangle}{||\phi|| \cdot ||\psi||}$$

A similaridade do cosseno √© equivalente ao produto interno quando os vetores s√£o normalizados a unidade.

> üí° **Exemplo Num√©rico: Compara√ß√£o de M√©tricas de Similaridade**
>
> Vamos usar os mesmos vetores da consulta e documentos do exemplo anterior: $\phi = [0.1, 0.2, 0.3]$, $\psi_1 = [0.2, 0.1, 0.4]$, $\psi_2 = [0.3, 0.2, 0.1]$.
>
> 1.  **Produto Interno (j√° calculado):**
>     *   $\langle \phi, \psi_1 \rangle = 0.16$
>     *   $\langle \phi, \psi_2 \rangle = 0.10$
>
> 2.  **Similaridade do Cosseno:**
>     *   $||\phi|| = \sqrt{0.1^2 + 0.2^2 + 0.3^2} = \sqrt{0.14} \approx 0.37$
>     *   $||\psi_1|| = \sqrt{0.2^2 + 0.1^2 + 0.4^2} = \sqrt{0.21} \approx 0.46$
>     *   $||\psi_2|| = \sqrt{0.3^2 + 0.2^2 + 0.1^2} = \sqrt{0.14} \approx 0.37$
>
>     *   $cos(\phi, \psi_1) = \frac{0.16}{0.37 \cdot 0.46} \approx \frac{0.16}{0.17} \approx 0.94$
>     *   $cos(\phi, \psi_2) = \frac{0.10}{0.37 \cdot 0.37} \approx \frac{0.10}{0.14} \approx 0.71$
>
> 3.  **Dist√¢ncia Euclidiana (sem normaliza√ß√£o):**
>     *   $||\phi - \psi_1|| = ||[-0.1, 0.1, -0.1]|| = \sqrt{(-0.1)^2 + 0.1^2 + (-0.1)^2} = \sqrt{0.03} \approx 0.17$
>     *   $||\phi - \psi_2|| = ||[-0.2, 0.0, 0.2]|| = \sqrt{(-0.2)^2 + 0.0^2 + 0.2^2} = \sqrt{0.08} \approx 0.28$
>
> | M√©trica             | Documento 1 | Documento 2 | Ranking |
> |----------------------|-------------|-------------|---------|
> | Produto Interno       | 0.16        | 0.10        | 1 > 2   |
> | Cosseno              | 0.94        | 0.71        | 1 > 2   |
> | Dist√¢ncia Euclidiana | 0.17        | 0.28        | 1 > 2   |
>
> Neste exemplo, todas as m√©tricas concordam com o ranking. No entanto, em situa√ß√µes onde os vetores t√™m comprimentos muito diferentes, a similaridade do cosseno pode fornecer resultados mais robustos.

### Conclus√£o
<!-- END -->