## 4.2.1 Maximum Inner Product (MIP) Search e √çndices de Embeddings

Em sistemas de recupera√ß√£o de informa√ß√£o (IR) baseados em representa√ß√µes densas, os *document embeddings* s√£o pr√©-computados para permitir buscas eficientes. Esses embeddings s√£o armazenados em uma estrutura de dados espec√≠fica chamada **√≠ndice**. O desafio central reside em projetar um **√≠ndice** que permita encontrar, de forma eficiente, o *document embedding* com o maior produto interno (dot product) em rela√ß√£o a um *query embedding* fornecido [^29]. Este problema √© formalmente conhecido como **Maximum Inner Product (MIP) search** [^30].

Formalmente, dado um *query embedding* $\phi \in \mathbb{R}^l$ e um conjunto de *document embeddings* $\Psi = \{\psi_1, ..., \psi_n\}$, onde $\psi_i \in \mathbb{R}^l$ para $i = 1, ..., n$, o objetivo do *MIP search* √© encontrar o *document embedding* $\psi^* \in \Psi$ tal que:

$$
\psi^* = \arg \max_{\psi \in \Psi} \langle \phi, \psi \rangle \qquad (18)
$$

A estrutura de dados projetada para armazenar $\Psi$ √© denominada **embedding index**. A forma mais simples de *embedding index* √© o **flat index**, que armazena os *document embeddings* em $\Psi$ explicitamente e realiza uma busca exaustiva para identificar $\psi^*$. No entanto, a complexidade do *flat index* √© $O(nl)$ tanto em espa√ßo quanto em tempo, tornando-o ineficiente para grandes valores de $n$ ou $l$ [^30].

> üí° **Exemplo Num√©rico:** Considere um cen√°rio onde temos 1000 documentos ($n = 1000$) e cada embedding tem dimens√£o 128 ($l = 128$).  Com um *flat index*, para cada busca, precisamos calcular 1000 produtos internos, cada um envolvendo 128 multiplica√ß√µes e adi√ß√µes. Isso resulta em 128,000 opera√ß√µes por busca. Se tivermos 100 buscas, ser√£o 1,280,000 opera√ß√µes.  Isto demonstra a inefici√™ncia do *flat index* para grandes volumes de dados.

Para melhorar a efici√™ncia do *MIP search*, diversas t√©cnicas de indexa√ß√£o aproximada (Approximate Nearest Neighbor search - ANN) t√™m sido adaptadas e aplicadas. Essas t√©cnicas visam reduzir o tempo de busca, sacrificando a precis√£o exata, ou seja, nem sempre retornam o *document embedding* com o maior produto interno verdadeiro.

![Ranking pipeline architecture for multiple representation systems using learned embeddings and ANN search.](./../images/image3.png)

**Proposi√ß√£o 1** A busca exaustiva realizada pelo *flat index* garante encontrar o *document embedding* com o maior produto interno, mas possui complexidade linear em rela√ß√£o ao n√∫mero de *document embeddings*.

*Prova:* A busca exaustiva calcula o produto interno entre o *query embedding* $\phi$ e cada *document embedding* $\psi_i \in \Psi$.  Como o n√∫mero de *document embeddings* √© $n$, e cada produto interno tem custo $O(l)$, a complexidade total √© $O(nl)$. Como a busca compara todos os elementos, o m√°ximo √© garantidamente encontrado. $\blacksquare$

Uma alternativa ao *flat index* que busca um compromisso entre precis√£o e efici√™ncia √© o uso de quantiza√ß√£o vetorial.

**Teorema 1** A quantiza√ß√£o vetorial pode ser utilizada para construir √≠ndices que permitem buscas aproximadas de vizinhos mais pr√≥ximos com complexidade sublinear em rela√ß√£o ao n√∫mero de *document embeddings*.

*Prova (Esbo√ßo):* A ideia central da quantiza√ß√£o vetorial √© agrupar os *document embeddings* em *clusters* representados por centroides.  Durante a busca, o *query embedding* √© comparado com os centroides, e apenas os *document embeddings* nos *clusters* mais pr√≥ximos s√£o considerados para o c√°lculo do produto interno. A escolha do n√∫mero de *clusters* afeta o balanceamento entre precis√£o e tempo de busca.  Um n√∫mero menor de *clusters* leva a uma busca mais r√°pida, mas potencialmente com menor precis√£o, enquanto um n√∫mero maior de *clusters* aumenta a precis√£o, mas tamb√©m o tempo de busca. A complexidade depende do n√∫mero de *clusters* $k$ e do n√∫mero de elementos por *cluster*. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que aplicamos quantiza√ß√£o vetorial com $k=100$ clusters aos 1000 embeddings do exemplo anterior.  Na busca, primeiro calculamos a dist√¢ncia do *query embedding* aos 100 centroides. Em seguida, selecionamos os $m=5$ centroides mais pr√≥ximos (esse valor de 'm' √© um hiperpar√¢metro ajust√°vel).  Finalmente, calculamos o produto interno entre o *query embedding* e os embeddings dentro dos 5 clusters selecionados. Assumindo uma distribui√ß√£o uniforme, cada cluster conter√° aproximadamente 10 embeddings (1000 embeddings / 100 clusters). Portanto, calcularemos o produto interno apenas para $5 \times 10 = 50$ embeddings, em vez de 1000. Isso representa uma redu√ß√£o significativa na complexidade computacional. No entanto, introduzimos um erro potencial, pois o documento com o maior produto interno verdadeiro pode estar em um cluster que n√£o foi selecionado.

Adicionalmente, podemos explorar uma t√©cnica relacionada √† normaliza√ß√£o dos embeddings para simplificar a busca.

**Lema 1** Se todos os *document embeddings* e o *query embedding* forem normalizados para terem norma unit√°ria, o problema de *Maximum Inner Product Search* se torna equivalente ao problema de busca do vizinho mais pr√≥ximo (Nearest Neighbor Search) utilizando a dist√¢ncia euclidiana.

*Prova:* Seja $\|\phi\| = \|\psi_i\| = 1$ para todo $i$.  Ent√£o $\langle \phi, \psi_i \rangle = \cos(\theta)$, onde $\theta$ √© o √¢ngulo entre os vetores $\phi$ e $\psi_i$. Maximizar o produto interno √© equivalente a minimizar o √¢ngulo $\theta$.  A dist√¢ncia euclidiana entre $\phi$ e $\psi_i$ √© dada por $\|\phi - \psi_i\|^2 = \|\phi\|^2 + \|\psi_i\|^2 - 2\langle \phi, \psi_i \rangle = 2 - 2\langle \phi, \psi_i \rangle$. Minimizar a dist√¢ncia euclidiana √©, portanto, equivalente a maximizar o produto interno. $\blacksquare$

> üí° **Exemplo Num√©rico:** Consideremos dois embeddings normalizados: o *query embedding* $\phi = [0.8, 0.6]$ e um *document embedding* $\psi_1 = [0.7, 0.714]$.
> $\text{Step 1: Calculate the dot product}$
> $\langle \phi, \psi_1 \rangle = (0.8 \times 0.7) + (0.6 \times 0.714) = 0.56 + 0.4284 = 0.9884$
> $\text{Step 2: Calculate the Euclidean distance}$
> $\|\phi - \psi_1\| = \sqrt{(0.8 - 0.7)^2 + (0.6 - 0.714)^2} = \sqrt{(0.1)^2 + (-0.114)^2} = \sqrt{0.01 + 0.012996} = \sqrt{0.022996} \approx 0.1516$
> Agora consideremos outro *document embedding* $\psi_2 = [0.9, 0.435]$.
> $\text{Step 1: Calculate the dot product}$
> $\langle \phi, \psi_2 \rangle = (0.8 \times 0.9) + (0.6 \times 0.435) = 0.72 + 0.261 = 0.981$
> $\text{Step 2: Calculate the Euclidean distance}$
> $\|\phi - \psi_2\| = \sqrt{(0.8 - 0.9)^2 + (0.6 - 0.435)^2} = \sqrt{(-0.1)^2 + (0.165)^2} = \sqrt{0.01 + 0.027225} = \sqrt{0.037225} \approx 0.1929$
> Observamos que o embedding $\psi_1$ tem um produto interno maior com $\phi$ (0.9884) e uma dist√¢ncia euclidiana menor (0.1516) do que $\psi_2$ (produto interno de 0.981 e dist√¢ncia euclidiana de 0.1929). Isso ilustra que, quando os embeddings s√£o normalizados, maximizar o produto interno √© equivalente a minimizar a dist√¢ncia euclidiana.

<!-- END -->