## 4.2.1 Maximum Inner Product Search (MIP) e √çndices de Embeddings

### Introdu√ß√£o
A efici√™ncia na recupera√ß√£o de documentos relevantes √© crucial em sistemas de Information Retrieval (IR). Como discutido anteriormente [^29], ap√≥s a computa√ß√£o dos *embeddings* dos documentos, o pr√≥ximo passo √© armazen√°-los de forma eficiente para que a busca possa ser realizada rapidamente. Esta se√ß√£o detalha o problema de Maximum Inner Product (MIP) Search, que √© fundamental para sistemas de *dense retrieval*, e como os *embedding indexes* s√£o usados para resolver este problema.

### Conceitos Fundamentais

O objetivo do **Maximum Inner Product Search (MIP)** √©, dado um *embedding* de consulta $\phi$ e um conjunto de *embeddings* de documentos $Y = \{\psi_1, ..., \psi_n\}$, encontrar o *embedding* de documento $\psi^* \in Y$ que maximiza o produto interno com $\phi$ [^30]. Formalmente, o problema √© definido como:

$$
\psi^* = \arg \max_{\psi \in Y} \langle \phi, \psi \rangle
$$

onde $\langle \phi, \psi \rangle$ representa o produto interno entre os vetores $\phi$ e $\psi$ [^30]. Este produto interno quantifica a similaridade entre a consulta e o documento no espa√ßo de *embeddings*.

> üí° **Exemplo Num√©rico:** Suponha que temos uma consulta representada pelo embedding $\phi = [0.8, 0.6]$ e dois documentos representados pelos embeddings $\psi_1 = [0.9, 0.1]$ e $\psi_2 = [0.7, 0.7]$.  Calculamos o produto interno para cada documento:
>
> $\langle \phi, \psi_1 \rangle = (0.8 * 0.9) + (0.6 * 0.1) = 0.72 + 0.06 = 0.78$
>
> $\langle \phi, \psi_2 \rangle = (0.8 * 0.7) + (0.6 * 0.7) = 0.56 + 0.42 = 0.98$
>
> Neste caso, $\psi_2$ √© o embedding que maximiza o produto interno com $\phi$, portanto $\psi^* = \psi_2$. Isso significa que o documento representado por $\psi_2$ √© considerado mais relevante para a consulta representada por $\phi$ com base no produto interno dos seus embeddings.

**Observa√ß√£o:** √â importante notar que, se todos os embeddings $\psi \in Y$ forem normalizados (i.e., $\|\psi\| = 1$ para todo $\psi$), ent√£o maximizar o produto interno $\langle \phi, \psi \rangle$ √© equivalente a maximizar o cosseno do √¢ngulo entre $\phi$ e $\psi$.  Nesse caso, o MIP se torna equivalente a Maximum Cosine Similarity search.

> üí° **Exemplo Num√©rico:** Se normalizarmos os embeddings do exemplo anterior, teremos:
>
>$\|\phi\| = \sqrt{0.8^2 + 0.6^2} = 1.0$
>
>$\|\psi_1\| = \sqrt{0.9^2 + 0.1^2} = \sqrt{0.82} \approx 0.905$
>
>$\|\psi_2\| = \sqrt{0.7^2 + 0.7^2} = \sqrt{0.98} \approx 0.99$
>
>Ap√≥s normaliza√ß√£o (dividindo cada vetor pela sua norma):
>
>$\phi_{norm} = [0.8, 0.6]$ (j√° normalizado pois a norma era 1.0)
>
>$\psi_{1, norm} = [0.9/0.905, 0.1/0.905] \approx [0.994, 0.110]$
>
>$\psi_{2, norm} = [0.7/0.99, 0.7/0.99] \approx [0.707, 0.707]$
>
>Recalculando o produto interno:
>
>$\langle \phi_{norm}, \psi_{1, norm} \rangle = (0.8 * 0.994) + (0.6 * 0.110) = 0.7952 + 0.066 = 0.8612$
>
>$\langle \phi_{norm}, \psi_{2, norm} \rangle = (0.8 * 0.707) + (0.6 * 0.707) = 0.5656 + 0.4242 = 0.9898$
>
>A ordem de similaridade se mant√©m, com $\psi_2$ sendo mais similar √† consulta $\phi$.

Um **embedding index** √© uma estrutura de dados projetada para armazenar o conjunto $Y$ de *embeddings* de documentos [^30]. A estrutura mais simples √© o **flat index**, que armazena os *embeddings* explicitamente e realiza uma busca exaustiva para identificar $\psi^*$ [^30].

### Desafios do Flat Index e Necessidade de Aproxima√ß√£o

O *flat index*, embora conceitualmente simples, enfrenta s√©rias limita√ß√µes em termos de escalabilidade [^30]. A complexidade da busca √© $O(nl)$, onde $n$ √© o n√∫mero de documentos e $l$ √© a dimensionalidade dos *embeddings* [^30]. Isso se torna proibitivo para grandes cole√ß√µes de documentos ou para *embeddings* de alta dimens√£o. A necessidade de estrat√©gias mais eficientes leva √† busca aproximada de vizinhos mais pr√≥ximos (Approximate Nearest Neighbor - ANN), como discutido em se√ß√µes subsequentes [^31, ^32, ^33].

> üí° **Exemplo Num√©rico:** Considere uma cole√ß√£o com $n = 1$ milh√£o de documentos e embeddings com dimensionalidade $l = 768$.  Com um *flat index*, cada busca exigiria o c√°lculo de 1 milh√£o de produtos internos, cada um envolvendo 768 multiplica√ß√µes e adi√ß√µes. Isso resulta em um custo computacional muito alto para cada consulta, tornando a busca lenta.  Se cada opera√ß√£o de produto interno levasse, por exemplo, 1 microsegundo, a busca completa levaria 1 segundo por consulta, o que √© inaceit√°vel para aplica√ß√µes interativas.

### Rela√ß√£o com Nearest Neighbor (NN) Search

Conforme mencionado no documento [^30], uma abordagem comum para melhorar a efici√™ncia do *flat index* √© converter o problema de MIP search em um problema de **Nearest Neighbor (NN) search**. O objetivo do NN search √© encontrar o *embedding* de documento $\psi^\dagger$ que minimiza a dist√¢ncia entre $\phi$ e $\psi$:

$$
\psi^\dagger = \arg \min_{\psi \in Y} \| \phi - \psi \|
$$

Antes de prosseguir, √© √∫til formalizar a rela√ß√£o entre MIP e NN com um resultado simples.

**Lema 1** Se todos os embeddings em $Y$ possuem a mesma norma, ou seja, $\|\psi\| = c$ para todo $\psi \in Y$ e algum $c > 0$, ent√£o resolver o MIP search √© equivalente a resolver um NN search com a dist√¢ncia dada por $d(\phi, \psi) = \|\phi - \psi\|^2$.

*Demonstra√ß√£o:* Expandindo a dist√¢ncia euclidiana ao quadrado, temos:
$$
\|\phi - \psi\|^2 = \|\phi\|^2 + \|\psi\|^2 - 2\langle \phi, \psi \rangle = \|\phi\|^2 + c^2 - 2\langle \phi, \psi \rangle
$$
Minimizar $\|\phi - \psi\|^2$ √© equivalente a maximizar $\langle \phi, \psi \rangle$, pois $\|\phi\|^2 + c^2$ √© constante com rela√ß√£o a $\psi$.

### Transforma√ß√£o para NN Search

A convers√£o do MIP search para NN search requer uma transforma√ß√£o dos *embeddings* para que a dist√¢ncia euclidiana reflita a similaridade do produto interno [^30]. Uma transforma√ß√£o comum √© a seguinte [^30]:

$$
\hat{\phi} = \begin{bmatrix} \phi / M \\ \sqrt{1 - \|\phi\|^2 / M^2} \end{bmatrix}, \quad
\hat{\psi} = \begin{bmatrix} \psi / M \\ \sqrt{1 - \|\psi\|^2 / M^2} \end{bmatrix}
$$

onde $M = \max_{\psi \in Y} \|\psi\|$ [^30]. Ao aplicar esta transforma√ß√£o, a solu√ß√£o do MIP search $\psi^*$ coincide com a solu√ß√£o do NN search $\hat{\psi}^\dagger$ no espa√ßo transformado [^30]. Essa transforma√ß√£o permite que usemos estruturas de dados e algoritmos otimizados para NN search, como *locality sensitive hashing* (LSH), *vector quantization*, e *graph-based approaches*, para resolver o problema de MIP search de forma mais eficiente [^31, ^32, ^33].

> üí° **Exemplo Num√©rico:** Vamos aplicar a transforma√ß√£o aos embeddings do exemplo anterior (normalizados). Primeiro, precisamos calcular $M$. Supondo que esses s√£o todos os documentos, $M = \max(\|\psi_1\|, \|\psi_2\|) = \max(0.905, 0.99) = 0.99$.
>
>$\hat{\phi} = \begin{bmatrix} 0.8/0.99 \\ 0.6/0.99 \\ \sqrt{1 - (0.8^2 + 0.6^2) / 0.99^2} \end{bmatrix} \approx \begin{bmatrix} 0.808 \\ 0.606 \\ \sqrt{1 - 1/0.9801} \end{bmatrix} \approx \begin{bmatrix} 0.808 \\ 0.606 \\ \sqrt{-0.02} \end{bmatrix}$
>
> Note que o terceiro elemento de $\hat{\phi}$ resulta em um n√∫mero imagin√°rio. Isso acontece porque $\|\phi\| > M$, o que viola a condi√ß√£o para a transforma√ß√£o. Precisamos garantir que $\|\phi\| \le M$. Vamos assumir que $\phi = [0.5, 0.5]$ e recalculamos: $\|\phi\| = \sqrt{0.5^2 + 0.5^2} \approx 0.707$.
>
> Agora,
>
>$\hat{\phi} = \begin{bmatrix} 0.5/0.99 \\ 0.5/0.99 \\ \sqrt{1 - 0.707^2 / 0.99^2} \end{bmatrix} \approx \begin{bmatrix} 0.505 \\ 0.505 \\ \sqrt{1 - 0.50/0.9801} \end{bmatrix} \approx \begin{bmatrix} 0.505 \\ 0.505 \\ \sqrt{0.489} \end{bmatrix} \approx \begin{bmatrix} 0.505 \\ 0.505 \\ 0.699 \end{bmatrix}$
>
>$\hat{\psi_1} = \begin{bmatrix} 0.994/0.99 \\ 0.110/0.99 \\ \sqrt{1 - 0.905^2 / 0.99^2} \end{bmatrix} \approx \begin{bmatrix} 1.004 \\ 0.111 \\ \sqrt{1 - 0.819/0.9801} \end{bmatrix} \approx \begin{bmatrix} 1.004 \\ 0.111 \\ \sqrt{0.164} \end{bmatrix} \approx \begin{bmatrix} 1.004 \\ 0.111 \\ 0.405 \end{bmatrix}$
>
>$\hat{\psi_2} = \begin{bmatrix} 0.707/0.99 \\ 0.707/0.99 \\ \sqrt{1 - 0.99^2 / 0.99^2} \end{bmatrix} \approx \begin{bmatrix} 0.714 \\ 0.714 \\ 0 \end{bmatrix}$
>
>Calculando as dist√¢ncias euclidianas ao quadrado:
>
>$\| \hat{\phi} - \hat{\psi_1} \|^2 \approx (0.505-1.004)^2 + (0.505-0.111)^2 + (0.699-0.405)^2 \approx 0.249 + 0.155 + 0.086 \approx 0.49$
>
>$\| \hat{\phi} - \hat{\psi_2} \|^2 \approx (0.505-0.714)^2 + (0.505-0.714)^2 + (0.699-0)^2 \approx 0.044 + 0.044 + 0.489 \approx 0.577$
>
>Neste caso, $\hat{\psi_1}$ √© mais pr√≥ximo de $\hat{\phi}$ no espa√ßo transformado, indicando que o documento representado por $\psi_1$ √© mais relevante para a consulta $\phi$.

**Teorema 1** (Equival√™ncia entre MIP e NN ap√≥s Transforma√ß√£o) Seja $\psi^* = \arg \max_{\psi \in Y} \langle \phi, \psi \rangle$ a solu√ß√£o do MIP search, e seja $\hat{\psi}^\dagger = \arg \min_{\psi \in Y} \| \hat{\phi} - \hat{\psi} \|$ a solu√ß√£o do NN search no espa√ßo transformado. Ent√£o, $\psi^* = \hat{\psi}^\dagger$.

*Demonstra√ß√£o:* A demonstra√ß√£o da equival√™ncia segue diretamente da an√°lise do produto interno no espa√ßo transformado.  Calculando a dist√¢ncia euclidiana ao quadrado entre $\hat{\phi}$ e $\hat{\psi}$, obtemos:
$$
\| \hat{\phi} - \hat{\psi} \|^2 = \| \hat{\phi} \|^2 + \| \hat{\psi} \|^2 - 2 \langle \hat{\phi}, \hat{\psi} \rangle
$$
Calculando o produto interno $\langle \hat{\phi}, \hat{\psi} \rangle$:
$$
\langle \hat{\phi}, \hat{\psi} \rangle = \frac{\langle \phi, \psi \rangle}{M^2} + \sqrt{1 - \frac{\|\phi\|^2}{M^2}} \sqrt{1 - \frac{\|\psi\|^2}{M^2}}
$$
Minimizar $\| \hat{\phi} - \hat{\psi} \|^2$ √© equivalente a maximizar $\langle \hat{\phi}, \hat{\psi} \rangle$.  Maximizar $\langle \hat{\phi}, \hat{\psi} \rangle$ implica maximizar $\frac{\langle \phi, \psi \rangle}{M^2} + \sqrt{1 - \frac{\|\phi\|^2}{M^2}} \sqrt{1 - \frac{\|\psi\|^2}{M^2}}$. Como $M$ √© definido como o m√°ximo da norma dos embeddings em $Y$, a parte $\sqrt{1 - \frac{\|\psi\|^2}{M^2}}$ √© sempre n√£o-negativa.  A maximiza√ß√£o desse termo √© dominada pela maximiza√ß√£o de $\langle \phi, \psi \rangle$. Portanto, $\psi^* = \hat{\psi}^\dagger$.

### Conclus√£o

A busca eficiente de *embeddings* de documentos √© um componente cr√≠tico em sistemas de IR modernos. A formula√ß√£o do problema como um MIP search e a subsequente transforma√ß√£o para NN search abrem caminho para a utiliza√ß√£o de uma variedade de t√©cnicas de indexa√ß√£o e busca aproximada, permitindo a constru√ß√£o de sistemas de IR escal√°veis e eficientes. As se√ß√µes seguintes detalham essas t√©cnicas, explorando suas vantagens e desvantagens no contexto de *dense retrieval* [^31, ^32, ^33].

### Refer√™ncias
[^29]: Se√ß√£o 4 do documento original.
[^30]: Se√ß√£o 4.2 do documento original.
[^31]: Se√ß√£o 4.3 do documento original.
[^32]: Se√ß√£o 4.4 do documento original.
[^33]: Se√ß√£o 4.5 do documento original.
<!-- END -->