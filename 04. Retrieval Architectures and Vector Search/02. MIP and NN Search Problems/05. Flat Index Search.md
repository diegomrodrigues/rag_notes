## Otimiza√ß√£o de Busca por Similaridade: Do MIP ao Nearest Neighbor com Transforma√ß√µes de Embeddings

### Introdu√ß√£o
No contexto de *Retrieval Architectures and Vector Search*, a efici√™ncia na busca por documentos relevantes √© crucial para o desempenho dos sistemas de Information Retrieval (IR) baseados em representa√ß√µes densas [^29]. Como vimos anteriormente, o pr√©-c√°lculo de *document embeddings* permite a cria√ß√£o de √≠ndices que aceleram significativamente o processo de busca [^29]. No entanto, a escolha da estrutura de dados (*embedding index*) e do algoritmo de busca impacta diretamente a escalabilidade e a lat√™ncia do sistema. Esta se√ß√£o aprofunda-se no problema do *Maximum Inner Product (MIP)* e explora a transforma√ß√£o deste em um problema de *Nearest Neighbor (NN)*, discutindo as implica√ß√µes e t√©cnicas para otimizar a busca por similaridade.

### Conceitos Fundamentais
O problema do *Maximum Inner Product (MIP)* surge naturalmente quando se utiliza o produto interno (dot product) como medida de similaridade entre *query embeddings* e *document embeddings* [^30]. Formalmente, dado um *query embedding* $\phi \in \mathbb{R}^l$ e um conjunto de *document embeddings* $\mathbb{Y} = \{\psi_1, ..., \psi_n\}$, onde $\psi_i \in \mathbb{R}^l$ para $i = 1, ..., n$, o objetivo do MIP search √© encontrar o *document embedding* $\psi^* \in \mathbb{Y}$ que maximiza o produto interno:
$$
\psi^* = \arg \max_{\psi \in \mathbb{Y}} \langle \phi, \psi \rangle
$$[^30]
A solu√ß√£o mais direta para o MIP √© o uso de um *flat index*, que armazena os *document embeddings* em $\mathbb{Y}$ explicitamente e realiza uma busca exaustiva para identificar $\psi^*$ [^30]. No entanto, a complexidade dessa abordagem √© $O(nl)$ tanto em espa√ßo quanto em tempo, tornando-a ineficiente para grandes conjuntos de dados e altas dimens√µes ($n$ ou $l$) [^30].

> üí° **Exemplo Num√©rico:** Suponha que temos 3 documentos com embeddings de dimens√£o 2: $\psi_1 = [0.8, 0.6]$, $\psi_2 = [0.2, 0.9]$, $\psi_3 = [0.7, 0.7]$, e uma query embedding $\phi = [0.6, 0.8]$. Vamos calcular o produto interno entre a query e cada documento:
> *   $\langle \phi, \psi_1 \rangle = (0.6 * 0.8) + (0.8 * 0.6) = 0.48 + 0.48 = 0.96$
> *   $\langle \phi, \psi_2 \rangle = (0.6 * 0.2) + (0.8 * 0.9) = 0.12 + 0.72 = 0.84$
> *   $\langle \phi, \psi_3 \rangle = (0.6 * 0.7) + (0.8 * 0.7) = 0.42 + 0.56 = 0.98$
>
> Neste caso, $\psi_3$ tem o maior produto interno com $\phi$, portanto, seria o resultado da busca MIP. Um flat index realizaria esses tr√™s c√°lculos. Para um milh√£o de documentos, seriam um milh√£o de c√°lculos.

**Transforma√ß√£o do MIP em Nearest Neighbor Search:**
Para contornar as limita√ß√µes do *flat index*, uma abordagem comum √© converter o problema do MIP em um problema de *Nearest Neighbor (NN)*, cujo objetivo √© encontrar o *document embedding* $\psi^\dagger$ que minimiza a dist√¢ncia Euclidiana em rela√ß√£o a $\phi$:
$$
\psi^\dagger = \arg \min_{\psi \in \mathbb{Y}} ||\phi - \psi||
$$[^30]
Essa transforma√ß√£o permite o uso de estruturas de dados e algoritmos eficientes projetados especificamente para a busca de vizinhos mais pr√≥ximos [^30].

**Transforma√ß√£o para Dist√¢ncia Euclidiana:**
Para aplicar t√©cnicas de NN search ao problema do MIP, √© necess√°rio transformar os embeddings de forma que a dist√¢ncia Euclidiana reflita a similaridade do produto interno. Uma transforma√ß√£o comum [Bachrach et al. 2014, Neyshabur and Srebro 2015] mapeia os embeddings de $\mathbb{R}^l$ para $\mathbb{R}^{l+1}$ da seguinte forma [^30]:
$$
\hat{\phi} = \begin{bmatrix} \phi / M \\ \sqrt{1 - ||\phi||^2 / M^2} \end{bmatrix}, \quad \hat{\psi} = \begin{bmatrix} \psi / M \\ \sqrt{1 - ||\psi||^2 / M^2} \end{bmatrix}
$$
Onde $M = \max_{\psi \in \mathbb{Y}} ||\psi||$ [^30]. Utilizando essa transforma√ß√£o, a solu√ß√£o do MIP, $\psi^*$, coincide com a solu√ß√£o do NN search, $\hat{\psi}^\dagger$ [^30]. De fato, temos [^30]:
$$
\min ||\hat{\phi} - \hat{\psi}||^2 = \min (||\hat{\phi}||^2 + ||\hat{\psi}||^2 - 2 \langle \hat{\phi}, \hat{\psi} \rangle) = \min (2 - 2 \langle \phi, \psi \rangle / M) = \max \langle \phi, \psi \rangle
$$

> üí° **Exemplo Num√©rico:** Usando os mesmos embeddings do exemplo anterior: $\psi_1 = [0.8, 0.6]$, $\psi_2 = [0.2, 0.9]$, $\psi_3 = [0.7, 0.7]$, $\phi = [0.6, 0.8]$. Primeiro, calculamos as normas:
> *   $||\psi_1|| = \sqrt{0.8^2 + 0.6^2} = \sqrt{0.64 + 0.36} = 1$
> *   $||\psi_2|| = \sqrt{0.2^2 + 0.9^2} = \sqrt{0.04 + 0.81} = \sqrt{0.85} \approx 0.92$
> *   $||\psi_3|| = \sqrt{0.7^2 + 0.7^2} = \sqrt{0.49 + 0.49} = \sqrt{0.98} \approx 0.99$
> *   $||\phi|| = \sqrt{0.6^2 + 0.8^2} = \sqrt{0.36 + 0.64} = 1$
>
> Ent√£o, $M = \max(1, 0.92, 0.99) = 1$. Agora, transformamos os embeddings para $\mathbb{R}^{3}$:
> *   $\hat{\phi} = [0.6/1, 0.8/1, \sqrt{1 - 1^2/1^2}] = [0.6, 0.8, 0]$
> *   $\hat{\psi_1} = [0.8/1, 0.6/1, \sqrt{1 - 1^2/1^2}] = [0.8, 0.6, 0]$
> *   $\hat{\psi_2} = [0.2/1, 0.9/1, \sqrt{1 - 0.92^2/1^2}] = [0.2, 0.9, \sqrt{1 - 0.8464}] = [0.2, 0.9, \sqrt{0.1536}] \approx [0.2, 0.9, 0.39]$
> *   $\hat{\psi_3} = [0.7/1, 0.7/1, \sqrt{1 - 0.99^2/1^2}] = [0.7, 0.7, \sqrt{1 - 0.9801}] = [0.7, 0.7, \sqrt{0.0199}] \approx [0.7, 0.7, 0.14]$
>
> Finalmente, calculamos as dist√¢ncias Euclidianas ao quadrado:
> *   $||\hat{\phi} - \hat{\psi_1}||^2 = (0.6-0.8)^2 + (0.8-0.6)^2 + (0-0)^2 = 0.04 + 0.04 + 0 = 0.08$
> *   $||\hat{\phi} - \hat{\psi_2}||^2 = (0.6-0.2)^2 + (0.8-0.9)^2 + (0-0.39)^2 = 0.16 + 0.01 + 0.1521 = 0.3221$
> *   $||\hat{\phi} - \hat{\psi_3}||^2 = (0.6-0.7)^2 + (0.8-0.7)^2 + (0-0.14)^2 = 0.01 + 0.01 + 0.0196 = 0.0396$
>
> O documento $\psi_3$ tem a menor dist√¢ncia Euclidiana ao quadrado em rela√ß√£o a $\phi$ ap√≥s a transforma√ß√£o, confirmando que ele √© o vizinho mais pr√≥ximo.  Observe que a ordem dos documentos por similaridade √© preservada.

**Observa√ß√£o:**
A transforma√ß√£o acima demonstra como a minimiza√ß√£o da dist√¢ncia Euclidiana entre os embeddings transformados equivale √† maximiza√ß√£o do produto interno original.

**Lema:**
A transforma√ß√£o descrita preserva a ordena√ß√£o de similaridade entre os documentos e uma dada query.

*Prova:*
Sejam $\psi_1$ e $\psi_2$ dois document embeddings e $\phi$ um query embedding. Se $\langle \phi, \psi_1 \rangle > \langle \phi, \psi_2 \rangle$, ent√£o, ap√≥s a transforma√ß√£o, teremos $||\hat{\phi} - \hat{\psi_1}||^2 < ||\hat{\phi} - \hat{\psi_2}||^2$, preservando a rela√ß√£o de ordena√ß√£o. $\blacksquare$

Ap√≥s essa transforma√ß√£o, o problema de encontrar o documento mais similar √† query se torna um problema de Nearest Neighbor na dist√¢ncia Euclidiana [^30]. Para simplificar a nota√ß√£o, a partir deste ponto, omitiremos o "chap√©u" sobre os embeddings, ou seja, $\hat{\phi} \rightarrow \phi$ e $\hat{\psi} \rightarrow \psi$, e consideraremos $l+1$ como a nova dimens√£o $l$ [^30].

**Proposi√ß√£o 1:**
A escolha de $M = \max_{\psi \in \mathbb{Y}} ||\psi||$ garante que a componente adicional introduzida pela transforma√ß√£o seja sempre um n√∫mero real n√£o-negativo.

*Prova:*
Por defini√ß√£o, $||\psi|| \leq M$ para todo $\psi \in \mathbb{Y}$. Portanto, $||\psi||^2 / M^2 \leq 1$, e assim $1 - ||\psi||^2 / M^2 \geq 0$. Consequentemente, $\sqrt{1 - ||\psi||^2 / M^2}$ √© sempre um n√∫mero real n√£o-negativo. O mesmo racioc√≠nio se aplica a $\phi$ se $||\phi|| \leq M$. Caso $||\phi|| > M$, a query est√° fora do espa√ßo de representa√ß√£o dos documentos, e a transforma√ß√£o ainda √© v√°lida, embora possa levar a resultados menos intuitivos, sendo importante garantir que $||\phi|| \leq M$ ou utilizar uma estrat√©gia de tratamento de outliers. $\blacksquare$

**Teorema 1:**
Se todos os embeddings (query e documentos) s√£o normalizados (i.e., $||\phi|| = ||\psi|| = 1$ para todo $\phi$ e $\psi$), ent√£o o problema de MIP √© equivalente ao problema de Nearest Neighbor na dist√¢ncia Euclidiana, sem a necessidade da transforma√ß√£o explicitada acima.

*Prova:*
Se $||\phi|| = ||\psi|| = 1$, ent√£o:
$$
||\phi - \psi||^2 = ||\phi||^2 + ||\psi||^2 - 2 \langle \phi, \psi \rangle = 1 + 1 - 2 \langle \phi, \psi \rangle = 2 - 2 \langle \phi, \psi \rangle
$$
Minimizar $||\phi - \psi||^2$ √© equivalente a minimizar $2 - 2 \langle \phi, \psi \rangle$, que por sua vez √© equivalente a maximizar $\langle \phi, \psi \rangle$. Portanto, $\arg \min_{\psi \in \mathbb{Y}} ||\phi - \psi|| = \arg \max_{\psi \in \mathbb{Y}} \langle \phi, \psi \rangle$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Normalizando os embeddings originais:
>
> $\psi_1 = [0.8, 0.6]$, $\psi_2 = [0.2, 0.9]$, $\psi_3 = [0.7, 0.7]$, $\phi = [0.6, 0.8]$. J√° calculamos as normas anteriormente: $||\psi_1|| = 1$, $||\psi_2|| \approx 0.92$, $||\psi_3|| \approx 0.99$, $||\phi|| = 1$.
>
> * Normalizando:
>
> $\psi'_1 = [0.8/1, 0.6/1] = [0.8, 0.6]$
>
> $\psi'_2 = [0.2/0.92, 0.9/0.92] \approx [0.22, 0.98]$
>
> $\psi'_3 = [0.7/0.99, 0.7/0.99] \approx [0.71, 0.71]$
>
> $\phi' = [0.6/1, 0.8/1] = [0.6, 0.8]$
>
> Calculando a dist√¢ncia Euclidiana ao quadrado entre os embeddings normalizados (sem a transforma√ß√£o para l+1 dimens√µes) e a query normalizada:
>
> $||\phi' - \psi'_1||^2 = (0.6-0.8)^2 + (0.8-0.6)^2 = 0.04 + 0.04 = 0.08$
>
> $||\phi' - \psi'_2||^2 = (0.6-0.22)^2 + (0.8-0.98)^2 = 0.1444 + 0.0324 = 0.1768$
>
> $||\phi' - \psi'_3||^2 = (0.6-0.71)^2 + (0.8-0.71)^2 = 0.0121 + 0.0081 = 0.0202$
>
> $\psi'_3$ continua sendo o vizinho mais pr√≥ximo, demonstrando a equival√™ncia entre MIP e NN quando os embeddings s√£o normalizados.

√â importante notar que a normaliza√ß√£o dos embeddings √© uma pr√°tica comum em diversas aplica√ß√µes de Information Retrieval, simplificando o processo de busca e, conforme demonstrado no Teorema 1, eliminando a necessidade da transforma√ß√£o para o espa√ßo $\mathbb{R}^{l+1}$ quando se deseja utilizar a dist√¢ncia Euclidiana.

### Conclus√£o
A transforma√ß√£o do problema de MIP em um problema de NN √© uma t√©cnica fundamental para otimizar a busca por similaridade em sistemas de IR baseados em representa√ß√µes densas [^30]. Ao mapear os embeddings para um espa√ßo onde a dist√¢ncia Euclidiana reflita a similaridade do produto interno, √© poss√≠vel aproveitar as estruturas de dados e algoritmos eficientes desenvolvidos para NN search [^30]. Essa abordagem permite reduzir significativamente a complexidade computacional e escalar a busca por documentos relevantes para grandes conjuntos de dados. As se√ß√µes subsequentes explorar√£o diferentes estruturas de dados e algoritmos para NN search, como *Locality Sensitive Hashing (LSH)*, *Vector Quantization* e *Graph-based approaches* [^31, 32, 33], que s√£o essenciais para a constru√ß√£o de sistemas de IR eficientes e escal√°veis.

### Refer√™ncias
[^29]: Section 4: Retrieval Architectures and Vector Search
[^30]: Section 4.2: MIP and NN Search Problems
[^31]: Section 4.3: Locality sensitive hashing approaches
[^32]: Section 4.4: Vector quantisation approaches
[^33]: Section 4.5: Graph approaches
<!-- END -->