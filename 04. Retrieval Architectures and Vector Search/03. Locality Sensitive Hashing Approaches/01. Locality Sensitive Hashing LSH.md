## Locality Sensitive Hashing for Efficient Neural Information Retrieval

### Introdu√ß√£o
Este cap√≠tulo aprofunda o conceito de **Locality Sensitive Hashing (LSH)** como uma t√©cnica fundamental para acelerar a busca por vizinhos mais pr√≥ximos (Nearest Neighbor Search - NNS) em sistemas de Neural Information Retrieval (NIR). Conforme introduzido na se√ß√£o 4.3 [^31], LSH explora a ideia de que pontos de dados similares, representados como embeddings, t√™m maior probabilidade de serem mapeados para o mesmo "bucket" por uma fam√≠lia de fun√ß√µes hash sens√≠veis √† localidade. Este cap√≠tulo expande essa base, detalhando os mecanismos, varia√ß√µes e considera√ß√µes pr√°ticas para a implementa√ß√£o eficaz de LSH em contextos de NIR.

### Conceitos Fundamentais

**Princ√≠pios do LSH**
O Locality Sensitive Hashing (LSH) [^31] √© uma t√©cnica probabil√≠stica para encontrar os vizinhos mais pr√≥ximos em espa√ßos de alta dimens√£o. A ideia central do LSH reside em projetar os pontos de dados (embeddings) de forma que a proximidade relativa entre os pontos seja preservada, aumentando a probabilidade de que pontos similares colidam no mesmo bucket hash.

Como definido [^31], o LSH √© fundamentado em duas propriedades principais:
1.  *Alta probabilidade de colis√£o para pontos similares:* Se dois embeddings, digamos $\psi_1$ e $\psi_2$, s√£o "pr√≥ximos" (de acordo com alguma m√©trica de similaridade), existe uma alta probabilidade $p_1$ de que eles sejam mapeados para o mesmo bucket hash.
2.  *Baixa probabilidade de colis√£o para pontos dissimilares:* Se os embeddings $\psi_1$ e $\psi_2$ s√£o "distantes", existe uma probabilidade significativamente menor $p_2$ (onde $p_2 < p_1$) de que colidam no mesmo bucket hash.

Essas propriedades s√£o cruciais para realizar buscas de vizinhos pr√≥ximos eficientes em grandes datasets. A efici√™ncia do LSH reside na redu√ß√£o da necessidade de comparar cada query embedding com todos os embeddings no √≠ndice.

> üí° **Exemplo Num√©rico:**
>
> Suponha que definimos "pr√≥ximo" como uma dist√¢ncia Euclidiana menor que 0.5, e "distante" como uma dist√¢ncia maior que 1.0. Para dois embeddings $\psi_1$ e $\psi_2$:
>
> *   Se $d(\psi_1, \psi_2) = 0.3$ (pr√≥ximos), a probabilidade de colis√£o $p_1$ pode ser 0.8.
> *   Se $d(\psi_1, \psi_2) = 1.2$ (distantes), a probabilidade de colis√£o $p_2$ pode ser 0.2.
>
> Isso significa que embeddings similares t√™m uma chance muito maior de serem colocadas no mesmo bucket hash do que embeddings dissimilares.

**Teorema 1** [Formaliza√ß√£o das propriedades do LSH]
Uma fam√≠lia de fun√ß√µes hash $\mathcal{H}$ √© $(r, cr, p_1, p_2)$-sens√≠vel para uma m√©trica de dist√¢ncia $d$ se para quaisquer dois pontos $v, u$:
1.  Se $d(v, u) \leq r$, ent√£o $P_{\mathcal{H}}[h(v) = h(u)] \geq p_1$.
2.  Se $d(v, u) \geq cr$, ent√£o $P_{\mathcal{H}}[h(v) = h(u)] \leq p_2$.

onde $h$ √© uma fun√ß√£o hash escolhida aleatoriamente de $\mathcal{H}$, e $c > 1$ √© um fator de aproxima√ß√£o.

Essa formaliza√ß√£o quantifica a sensibilidade √† localidade, definindo a probabilidade de colis√£o em termos da dist√¢ncia entre os pontos e um fator de aproxima√ß√£o.

**Fam√≠lias de Fun√ß√µes Hash**
Um componente essencial do LSH √© a fam√≠lia de fun√ß√µes hash sens√≠veis √† localidade. Essa fam√≠lia, denotada como $\mathcal{H}$, √© projetada de tal forma que, para uma fun√ß√£o $h \in \mathcal{H}$ escolhida aleatoriamente, a probabilidade de colis√£o entre dois pontos reflita sua similaridade. A escolha da fam√≠lia $\mathcal{H}$ depende da m√©trica de similaridade utilizada.

**Random Projection LSH**
Para a dist√¢ncia Euclidiana, uma fam√≠lia de fun√ß√µes hash LSH popular √© a de proje√ß√£o aleat√≥ria [^31]. Nesta abordagem, uma fun√ß√£o hash $h(v)$ √© definida como:
$$
h(v) = \text{sign}(v \cdot r)
$$
onde $v$ √© o vetor de entrada e $r$ √© um vetor aleat√≥rio gerado a partir de uma distribui√ß√£o normal. Esta fun√ß√£o projeta o vetor $v$ em uma linha definida por $r$, e o sinal do resultado determina o bucket hash.

> üí° **Exemplo Num√©rico:**
>
> Suponha que tenhamos um vetor de entrada $v = [0.5, -0.2, 0.8]$ e um vetor aleat√≥rio $r = [-0.1, 0.3, 0.4]$.  O produto vetorial √©:
>
> $v \cdot r = (0.5 * -0.1) + (-0.2 * 0.3) + (0.8 * 0.4) = -0.05 - 0.06 + 0.32 = 0.21$
>
> Como $v \cdot r = 0.21 > 0$, ent√£o $h(v) = \text{sign}(0.21) = 1$. Portanto, o vetor $v$ seria colocado no bucket correspondente ao sinal positivo.

**Teorema 1.1** [Probabilidade de colis√£o para Random Projection LSH]
Para Random Projection LSH, a probabilidade de colis√£o entre dois vetores $v$ e $u$ √© dada por:
$$
P[h(v) = h(u)] = 1 - \frac{\theta}{\pi}
$$
onde $\theta$ √© o √¢ngulo entre os vetores $v$ e $u$.

*Proof:* A probabilidade de colis√£o √© a probabilidade de que $v \cdot r$ e $u \cdot r$ tenham o mesmo sinal. Isso ocorre quando o vetor aleat√≥rio $r$ cai no mesmo semi-espa√ßo definido por $v$ e $u$. A probabilidade √© ent√£o $1 - \frac{\theta}{\pi}$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que o √¢ngulo $\theta$ entre dois vetores $v$ e $u$ seja $\frac{\pi}{4}$ (45 graus). A probabilidade de colis√£o seria:
>
> $P[h(v) = h(u)] = 1 - \frac{\pi/4}{\pi} = 1 - \frac{1}{4} = 0.75$
>
> Isso significa que h√° 75% de chance de que esses dois vetores colidam no mesmo bucket hash. Se o √¢ngulo fosse maior, digamos $\frac{\pi}{2}$ (90 graus), a probabilidade de colis√£o seria $1 - \frac{1}{2} = 0.5$, refletindo a menor similaridade.

**Lema 1.1** [Sensibilidade da Random Projection LSH]
A fam√≠lia de fun√ß√µes hash Random Projection LSH √© $(r, cr, 1 - \frac{\theta_1}{\pi}, 1 - \frac{\theta_2}{\pi})$-sens√≠vel, onde $\theta_1$ √© o √¢ngulo m√°ximo entre vetores a uma dist√¢ncia $r$, e $\theta_2$ √© o √¢ngulo m√≠nimo entre vetores a uma dist√¢ncia $cr$.

Este lema formaliza a sensibilidade da fam√≠lia Random Projection LSH, vinculando a probabilidade de colis√£o ao √¢ngulo entre os vetores e, portanto, √† sua dist√¢ncia.

**√çndices LSH**
Para construir um √≠ndice LSH [^31], m√∫ltiplas fun√ß√µes hash s√£o utilizadas para aumentar a probabilidade de recuperar os vizinhos mais pr√≥ximos. Tipicamente, $m$ fun√ß√µes hash $h_1, h_2, ..., h_m$ s√£o amostradas independentemente de $\mathcal{H}$. Essas fun√ß√µes s√£o combinadas para formar uma fun√ß√£o $g(v) = (h_1(v), h_2(v), ..., h_m(v))$. O processo √© repetido $r$ vezes, criando $r$ fun√ß√µes $g_1, g_2, ..., g_r$. Cada fun√ß√£o $g_i$ define uma tabela hash $T_i$.

Durante a indexa√ß√£o, cada vetor $v$ no dataset √© inserido em cada tabela hash $T_i$ no bucket $g_i(v)$. Na fase de busca, dado um vetor de consulta $q$, o √≠ndice LSH computa $g_i(q)$ para cada $i$ e recupera os vetores do bucket correspondente em cada tabela. Esses vetores recuperados formam um conjunto candidato, que √© ent√£o pesquisado exaustivamente para encontrar os vizinhos mais pr√≥ximos.

> üí° **Exemplo Num√©rico:**
>
> Suponha que usemos $m = 3$ fun√ß√µes hash e $r = 2$ tabelas hash. Para um vetor $v$, calculamos:
>
> *   $g_1(v) = (h_1(v), h_2(v), h_3(v)) = (1, 0, 1)$
> *   $g_2(v) = (h_4(v), h_5(v), h_6(v)) = (0, 1, 0)$
>
> O vetor $v$ √© ent√£o inserido na tabela $T_1$ no bucket "(1, 0, 1)" e na tabela $T_2$ no bucket "(0, 1, 0)".  Durante a busca com uma query $q$, calculamos $g_1(q)$ e $g_2(q)$, recuperando os vetores nos buckets correspondentes de $T_1$ e $T_2$ para formar o conjunto candidato.

**Corol√°rio 1** [Redu√ß√£o do espa√ßo de busca com LSH]
O √≠ndice LSH reduz o espa√ßo de busca de $N$ (tamanho total do dataset) para $r \cdot |B|$, onde $r$ √© o n√∫mero de tabelas hash e $|B|$ √© o tamanho m√©dio do bucket recuperado.

Este corol√°rio quantifica a redu√ß√£o no espa√ßo de busca alcan√ßada pelo uso do LSH. A busca exaustiva √© agora realizada apenas em um subconjunto dos dados, contido nos buckets recuperados.

> üí° **Exemplo Num√©rico:**
>
> Se tivermos um dataset com $N = 1,000,000$ vetores, e usarmos $r = 10$ tabelas hash, com um tamanho m√©dio de bucket $|B| = 100$, o espa√ßo de busca √© reduzido para $10 * 100 = 1,000$. Isso representa uma redu√ß√£o significativa em compara√ß√£o com a busca exaustiva em 1,000,000 vetores.

**Trade-off entre Precis√£o e Desempenho**
Um desafio com LSH √© equilibrar precis√£o e desempenho [^31]. Usar mais tabelas hash ($r$ grande) aumenta a probabilidade de encontrar os vizinhos mais pr√≥ximos verdadeiros, mas tamb√©m aumenta o consumo de mem√≥ria e o tempo de computa√ß√£o. De forma similar, usar mais fun√ß√µes hash por tabela ($m$ grande) reduz o n√∫mero de falsos positivos, mas tamb√©m pode aumentar a probabilidade de perder vizinhos pr√≥ximos verdadeiros. A escolha de $m$ e $r$ depende das caracter√≠sticas espec√≠ficas do dataset e dos requisitos da aplica√ß√£o.

**Lema 2** [Influ√™ncia de m e r no recall e precis√£o]
Aumentar o n√∫mero de fun√ß√µes hash por tabela ($m$) tende a aumentar a precis√£o (reduz falsos positivos), mas pode diminuir o recall (aumenta falsos negativos). Aumentar o n√∫mero de tabelas hash ($r$) tende a aumentar o recall, mas diminui a precis√£o e aumenta o custo computacional.

Este lema descreve a influ√™ncia dos par√¢metros $m$ e $r$ nas m√©tricas de recall e precis√£o, fornecendo um guia para ajustar esses par√¢metros na pr√°tica.

> üí° **Exemplo Num√©rico:**
>
> Considere um cen√°rio onde precisamos recuperar os top-10 vizinhos mais pr√≥ximos. Avaliamos diferentes configura√ß√µes de $m$ e $r$ e observamos os seguintes resultados:
>
> | Configura√ß√£o | m   | r   | Precis√£o@10 | Recall@10 | Tempo de Busca (ms) |
> | :----------- | :-- | :-- | :---------- | :-------- | :------------------ |
> | 1            | 5   | 5   | 0.6         | 0.7       | 50                  |
> | 2            | 10  | 5   | 0.75        | 0.6       | 60                  |
> | 3            | 5   | 10  | 0.5         | 0.8       | 100                 |
>
> Configura√ß√£o 1 oferece um bom balanceamento. Configura√ß√£o 2 aumenta a precis√£o, mas reduz o recall. Configura√ß√£o 3 aumenta o recall, mas diminui a precis√£o e aumenta o tempo de busca. A escolha ideal depende dos requisitos da aplica√ß√£o.

### Otimiza√ß√µes e Varia√ß√µes

Diversas otimiza√ß√µes e varia√ß√µes do LSH foram propostas para melhorar sua efici√™ncia e precis√£o [^31]:

*   **Multi-Probe LSH:** O Multi-Probe LSH [Lv et al. 2007, citado em ^31] explora m√∫ltiplos buckets hash para uma √∫nica query, em vez de apenas um, aumentando a probabilidade de encontrar os vizinhos mais pr√≥ximos, principalmente quando a fun√ß√£o hash n√£o mapeia o ponto de query para o bucket "correto" devido a ru√≠do ou varia√ß√µes nos dados.
*   **LSH C√¥nico:** Esta varia√ß√£o concentra-se em refinar o processo de hash para melhor preservar as dist√¢ncias angulares, especialmente √∫til em dados de alta dimens√£o, onde as dist√¢ncias euclidianas podem ser menos discriminativas.

**Proposi√ß√£o 1** [Redu√ß√£o do vi√©s no Multi-Probe LSH]
O Multi-Probe LSH reduz o vi√©s introduzido pela discretiza√ß√£o do espa√ßo em buckets hash, explorando buckets vizinhos e compensando erros de quantiza√ß√£o.

Essa proposi√ß√£o destaca a capacidade do Multi-Probe LSH de mitigar o efeito da discretiza√ß√£o, melhorando a precis√£o da busca.

### Conclus√£o

Locality Sensitive Hashing (LSH) oferece uma solu√ß√£o pr√°tica e eficiente para o problema de busca de vizinhos mais pr√≥ximos em Neural Information Retrieval [^31]. Ao projetar dados em buckets hash sens√≠veis √† localidade, LSH reduz drasticamente o n√∫mero de compara√ß√µes necess√°rias para encontrar os vizinhos mais pr√≥ximos, tornando-o adequado para aplica√ß√µes em larga escala. Apesar da necessidade de equilibrar precis√£o e desempenho, as diversas otimiza√ß√µes e varia√ß√µes do LSH permitem que ele seja adaptado para atender aos requisitos espec√≠ficos de diferentes datasets e aplica√ß√µes de NIR.
### Refer√™ncias
[^31]: Consulte a se√ß√£o 4.3 (Locality sensitive hashing approaches) do texto fornecido.
<!-- END -->