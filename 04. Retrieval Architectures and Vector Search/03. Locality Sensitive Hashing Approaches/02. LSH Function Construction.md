## 4.3.1 Constru√ß√£o de Fun√ß√µes LSH para Diferentes M√©tricas de Dist√¢ncia

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre **Locality Sensitive Hashing (LSH)** e sua aplica√ß√£o em *Approximate Nearest Neighbor (ANN) search* [^31], este subcap√≠tulo aprofunda-se na constru√ß√£o de fun√ß√µes LSH para diferentes m√©tricas de dist√¢ncia. Conforme mencionado anteriormente, o LSH baseia-se na ideia de que embeddings pr√≥ximos permanecem pr√≥ximos ap√≥s uma "proje√ß√£o" usando uma fun√ß√£o hash [^31]. A escolha da fun√ß√£o hash √© crucial e depende da m√©trica de dist√¢ncia utilizada.

### Fun√ß√µes Hash para Diferentes M√©tricas de Dist√¢ncia
O objetivo central do LSH √© projetar embeddings de tal forma que a probabilidade de colis√£o (i.e., serem mapeados para o mesmo bucket hash) seja alta para embeddings similares e baixa para embeddings dissimilares [^31]. Para atingir esse objetivo, diferentes fun√ß√µes hash s√£o projetadas para diferentes m√©tricas de dist√¢ncia. O contexto atual menciona especificamente a **random projection** para a *dist√¢ncia Euclidiana* [^31]. Al√©m da dist√¢ncia Euclidiana, outras m√©tricas como a dist√¢ncia de Hamming, cosseno e Jaccard tamb√©m s√£o amplamente utilizadas, cada uma com suas respectivas fun√ß√µes LSH otimizadas.

#### Random Projection para Dist√¢ncia Euclidiana
A *random projection* √© uma t√©cnica comum para construir fun√ß√µes LSH quando se utiliza a dist√¢ncia Euclidiana como m√©trica de similaridade [^31]. A ideia central √© projetar os embeddings originais em um espa√ßo de dimens√£o inferior, preservando as dist√¢ncias relativas entre os pontos.

Para construir uma fun√ß√£o hash LSH baseada em random projection, seguimos os seguintes passos:

1.  **Gerar um vetor aleat√≥rio:** Crie um vetor aleat√≥rio **r** da mesma dimens√£o que os embeddings de entrada [^31]. Os componentes de **r** s√£o tipicamente amostrados de uma distribui√ß√£o normal padr√£o ou uma distribui√ß√£o uniforme.
2.  **Calcular o produto escalar:** Para um dado embedding **œà**, calcule o produto escalar entre **œà** e **r**.
    $$
    p = \psi \cdot r
    $$
3.  **Quantizar o produto escalar:** Utilize uma fun√ß√£o de quantiza√ß√£o para discretizar o produto escalar $p$. Uma fun√ß√£o de quantiza√ß√£o simples pode ser definida como:
    $$
    h(\psi) = \begin{cases}
    1, & \text{se } p \geq b \\
    0, & \text{se } p < b
    \end{cases}
    $$
    onde $b$ √© um limiar (threshold) escolhido aleatoriamente.

O processo acima define uma √∫nica fun√ß√£o hash. Para construir uma fam√≠lia de fun√ß√µes hash LSH, repete-se o processo, gerando diferentes vetores aleat√≥rios **r** e/ou usando diferentes limiares $b$ [^31].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos dois embeddings, $\psi_1 = [1.0, 2.0, 3.0]$ e $\psi_2 = [1.5, 2.5, 3.5]$, e queremos usar random projection para criar fun√ß√µes hash. Vamos definir um vetor aleat√≥rio $r = [0.1, -0.2, 0.3]$ e um limiar $b = 0.5$.
>
> $\text{Passo 1: Calcular o produto escalar para } \psi_1$:
> $$
> p_1 = \psi_1 \cdot r = (1.0)(0.1) + (2.0)(-0.2) + (3.0)(0.3) = 0.1 - 0.4 + 0.9 = 0.6
> $$
> $\text{Passo 2: Calcular o produto escalar para } \psi_2$:
> $$
> p_2 = \psi_2 \cdot r = (1.5)(0.1) + (2.5)(-0.2) + (3.5)(0.3) = 0.15 - 0.5 + 1.05 = 0.7
> $$
> $\text{Passo 3: Aplicar a fun√ß√£o de quantiza√ß√£o para } \psi_1$:
> $$
> h(\psi_1) = \begin{cases}
> 1, & \text{se } 0.6 \geq 0.5 \\
> 0, & \text{se } 0.6 < 0.5
> \end{cases}
> $$
> Portanto, $h(\psi_1) = 1$.
>
> $\text{Passo 4: Aplicar a fun√ß√£o de quantiza√ß√£o para } \psi_2$:
> $$
> h(\psi_2) = \begin{cases}
> 1, & \text{se } 0.7 \geq 0.5 \\
> 0, & \text{se } 0.7 < 0.5
> \end{cases}
> $$
> Portanto, $h(\psi_2) = 1$.
>
> Neste exemplo, tanto $\psi_1$ quanto $\psi_2$ s√£o mapeados para o mesmo bucket (1), indicando que esta fun√ß√£o hash os considera similares.
>
> Agora, vamos definir outro vetor aleat√≥rio $r' = [-0.3, 0.2, -0.1]$ e o mesmo limiar $b = 0.5$.
>
> $\text{Passo 1: Calcular o produto escalar para } \psi_1$:
> $$
> p_1' = \psi_1 \cdot r' = (1.0)(-0.3) + (2.0)(0.2) + (3.0)(-0.1) = -0.3 + 0.4 - 0.3 = -0.2
> $$
> $\text{Passo 2: Calcular o produto escalar para } \psi_2$:
> $$
> p_2' = \psi_2 \cdot r' = (1.5)(-0.3) + (2.5)(0.2) + (3.5)(-0.1) = -0.45 + 0.5 - 0.35 = -0.3
> $$
> $\text{Passo 3: Aplicar a fun√ß√£o de quantiza√ß√£o para } \psi_1$:
> $$
> h(\psi_1) = \begin{cases}
> 1, & \text{se } -0.2 \geq 0.5 \\
> 0, & \text{se } -0.2 < 0.5
> \end{cases}
> $$
> Portanto, $h(\psi_1) = 0$.
>
> $\text{Passo 4: Aplicar a fun√ß√£o de quantiza√ß√£o para } \psi_2$:
> $$
> h(\psi_2) = \begin{cases}
> 1, & \text{se } -0.3 \geq 0.5 \\
> 0, & \text{se } -0.3 < 0.5
> \end{cases}
> $$
> Portanto, $h(\psi_2) = 0$.
>
> Com este segundo vetor aleat√≥rio, ambos ainda s√£o mapeados para o mesmo bucket (0). Usando *m* fun√ß√µes hash e concatenando os resultados ($g(\psi) = (h_1(\psi), ..., h_m(\psi))$) aumenta a probabilidade de discriminar entre documentos mais ou menos similares.

**Teorema 1** A random projection preserva aproximadamente as dist√¢ncias euclidianas. Formalmente, para dois vetores $\psi_1$ e $\psi_2$ em $\mathbb{R}^d$, e uma proje√ß√£o aleat√≥ria para $\mathbb{R}^k$ com $k << d$, a dist√¢ncia euclidiana entre as proje√ß√µes √© aproximadamente proporcional √† dist√¢ncia euclidiana original, com alta probabilidade.

*Prova (Esbo√ßo):* Este resultado decorre do Lema de Johnson-Lindenstrauss, que garante a exist√™ncia de uma proje√ß√£o linear de baixa dimens√£o que preserva as dist√¢ncias entre pontos com alta probabilidade.

#### Constru√ß√£o de Data Structures ANN
Conforme indicado em [^31], um conjunto de *random projections* define uma fam√≠lia de fun√ß√µes hash $H$ que podem ser usadas para construir uma estrutura de dados para busca ANN. Os passos s√£o os seguintes:

1. **Sampling de fun√ß√µes hash:** Amostre $m$ fun√ß√µes hash $h_1(\psi), ..., h_m(\psi)$ independentemente e uniformemente ao acaso a partir de $H$ [^31].
2. **Definir a fam√≠lia de fun√ß√µes:** Defina a fam√≠lia de fun√ß√µes $G = \{g : \mathbb{R}^l \rightarrow \mathbb{Z}^m\}$, onde $g(\psi) = (h_1(\psi), ..., h_m(\psi))$ [^31]. Ou seja, $g$ √© a concatena√ß√£o de $m$ fun√ß√µes hash de $H$.
3. **Sampling de fun√ß√µes de $G$:** Amostre $r$ fun√ß√µes $g_1(\psi), ..., g_r(\psi)$ independentemente e uniformemente ao acaso a partir de $G$ [^31]. Cada fun√ß√£o $g_i$ √© usada para construir uma hash table $H_i$ [^31].
4. **Constru√ß√£o do √≠ndice LSH:** Dado o conjunto de embeddings de documento $\Psi$, e selecionados os valores dos par√¢metros $r$ e $m$, um √≠ndice LSH √© composto de $r$ hash tables, cada uma contendo $m$ proje√ß√µes aleat√≥rias concatenadas [^31]. Para cada $\psi \in \Psi$, $\psi$ √© inserido no bucket $g_i(\psi)$ para cada hash table $H_i$, para $i = 1, ..., r$ [^31].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de embeddings de documento $\Psi = \{\psi_1, \psi_2, \psi_3\}$, onde $\psi_1 = [1.0, 2.0]$, $\psi_2 = [1.5, 2.5]$, e $\psi_3 = [5.0, 6.0]$. Vamos escolher $m = 2$ (n√∫mero de fun√ß√µes hash por tabela) e $r = 2$ (n√∫mero de tabelas hash).
>
> $\text{Passo 1: Sampling de } m \text{ fun√ß√µes hash para a primeira tabela } H_1$:
>
> *   $h_{11}(\psi)$: Usa vetor aleat√≥rio $r_{11} = [0.1, -0.2]$ e limiar $b_{11} = 0.5$.
> *   $h_{12}(\psi)$: Usa vetor aleat√≥rio $r_{12} = [-0.3, 0.2]$ e limiar $b_{12} = 0.0$.
>
> $\text{Passo 2: Sampling de } m \text{ fun√ß√µes hash para a segunda tabela } H_2$:
>
> *   $h_{21}(\psi)$: Usa vetor aleat√≥rio $r_{21} = [0.2, 0.1]$ e limiar $b_{21} = 1.0$.
> *   $h_{22}(\psi)$: Usa vetor aleat√≥rio $r_{22} = [-0.1, -0.3]$ e limiar $b_{22} = -1.0$.
>
> $\text{Passo 3: Constru√ß√£o das fun√ß√µes } g_i(\psi)$:
>
> *   $g_1(\psi) = (h_{11}(\psi), h_{12}(\psi))$
> *   $g_2(\psi) = (h_{21}(\psi), h_{22}(\psi))$
>
> $\text{Passo 4: Inser√ß√£o dos embeddings nas tabelas hash}$:
>
> Primeiro, precisamos calcular os valores das fun√ß√µes hash para cada embedding.  Usaremos os c√°lculos j√° demonstrados no exemplo anterior e simplificaremos.
>
> * Para $\psi_1$:
>     *   $h_{11}(\psi_1) = 1$
>     *   $h_{12}(\psi_1) = 0$ (Calculado similarmente aos passos anteriores, omitido aqui para brevidade)
>     *   $h_{21}(\psi_1) = 0$
>     *   $h_{22}(\psi_1) = 1$
> * Para $\psi_2$:
>     *   $h_{11}(\psi_2) = 1$
>     *   $h_{12}(\psi_2) = 0$
>     *   $h_{21}(\psi_2) = 0$
>     *   $h_{22}(\psi_2) = 1$
> * Para $\psi_3$:
>     *   $h_{11}(\psi_3) = 1$
>     *   $h_{12}(\psi_3) = 0$
>     *   $h_{21}(\psi_3) = 1$
>     *   $h_{22}(\psi_3) = 0$
>
> Agora, inserimos os embeddings nos buckets correspondentes:
>
> *   $H_1$:
>     *   Bucket (1, 0): Cont√©m $\psi_1$ e $\psi_2$.
> *   $H_2$:
>     *   Bucket (0, 1): Cont√©m $\psi_1$ e $\psi_2$.
>     *   Bucket (1, 0): Cont√©m $\psi_3$.
>
> Quando uma query chega, calculamos seus hashes usando as mesmas fun√ß√µes $h_{ij}$. Digamos que a query tem o mesmo hash que $\psi_1$ e $\psi_2$ na tabela $H_1$, ent√£o esses embeddings s√£o retornados como candidatos vizinhos mais pr√≥ximos. A similaridade √© calculada para cada candidato para gerar o ranking final.
>
> Esta √© uma vers√£o simplificada. Na pr√°tica, $m$ e $r$ seriam muito maiores e os vetores $r$ teriam a mesma dimensionalidade dos embeddings.

Para otimizar ainda mais a busca ANN, podemos considerar estrat√©gias de indexa√ß√£o hier√°rquica sobre as hash tables.

**Proposi√ß√£o 1** √â poss√≠vel construir um √≠ndice hier√°rquico sobre as hash tables $H_1, ..., H_r$ para acelerar a busca ANN.

*Prova (Esbo√ßo):* Podemos agrupar as hash tables com base em similaridades entre as fun√ß√µes hash $g_i$. Em seguida, podemos construir uma estrutura de √°rvore sobre esses grupos, permitindo uma busca mais eficiente por meio da poda de ramos irrelevantes.

### Considera√ß√µes Finais
A constru√ß√£o de fun√ß√µes LSH √© um passo fundamental na implementa√ß√£o de algoritmos de busca aproximada por vizinhos mais pr√≥ximos [^31]. A escolha apropriada da fun√ß√£o hash, alinhada √† m√©trica de dist√¢ncia relevante, impacta diretamente a efici√™ncia e a efic√°cia do processo de busca. O random projection para a dist√¢ncia Euclidiana √© um exemplo amplamente utilizado, mas outras fun√ß√µes hash s√£o aplic√°veis para diferentes m√©tricas, como a dist√¢ncia de Hamming ou o produto interno [^31]. Al√©m disso, a otimiza√ß√£o da estrutura de dados ANN, como a constru√ß√£o de √≠ndices hier√°rquicos, pode melhorar significativamente o desempenho da busca. $\blacksquare$
<!-- END -->