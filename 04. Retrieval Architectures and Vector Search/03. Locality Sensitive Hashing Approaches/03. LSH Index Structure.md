## 4.3 Locality Sensitive Hashing Approaches

### Introdu√ß√£o
Em continuidade com a discuss√£o sobre arquiteturas de *retrieval* e *vector search*, esta se√ß√£o se aprofunda em uma t√©cnica espec√≠fica para busca aproximada de vizinhos mais pr√≥ximos (ANN): *Locality Sensitive Hashing* (LSH). Como mencionado anteriormente [^30], as estruturas de dados de √≠ndice para a busca exata de NN em espa√ßos de baixa dimensionalidade t√™m se mostrado bem-sucedidas, mas ineficientes em dados de alta dimensionalidade. LSH oferece uma forma de mitigar esse problema, comprometendo a precis√£o da busca em prol da velocidade.

### Conceitos Fundamentais
**Locality Sensitive Hashing (LSH)** √© uma t√©cnica baseada na ideia intuitiva de que, se duas *embeddings* est√£o pr√≥ximas, ent√£o ap√≥s uma "proje√ß√£o" usando uma fun√ß√£o *hash*, essas *embeddings* permanecer√£o pr√≥ximas [^31]. Mais formalmente, [Indyk and Motwani 1998] definem LSH como possuindo as seguintes propriedades [^31]:
*   Para quaisquer duas *embeddings* œà‚ÇÅ e œà‚ÇÇ que est√£o pr√≥ximas uma da outra, existe uma alta probabilidade $p‚ÇÅ$ de que elas caiam no mesmo *hash bucket*.
*   Para quaisquer duas *embeddings* œà‚ÇÅ e œà‚ÇÇ que est√£o distantes uma da outra, existe uma baixa probabilidade $p‚ÇÇ$ < $p‚ÇÅ$ de que elas caiam no mesmo *hash bucket*.

O desafio central √©, portanto, projetar uma fam√≠lia de fun√ß√µes LSH que satisfa√ßam esses requisitos [^31]. Essas fun√ß√µes foram desenvolvidas para v√°rias m√©tricas de dist√¢ncia. Para a dist√¢ncia euclidiana, uma fun√ß√£o LSH popular √© a **random projection** [Datar et al. 2004] [^31].

Um conjunto de *random projections* define uma fam√≠lia de fun√ß√µes *hash* $\mathcal{H}$ que podem ser usadas para construir uma estrutura de dados para a busca ANN. O processo envolve as seguintes etapas [^31]:

1.  **Amostragem de Fun√ß√µes Hash:** Primeiro, amostramos *m* fun√ß√µes *hash* $h_1(œà), ..., h_m(œà)$ independentemente e uniformemente de $\mathcal{H}$.

2.  **Defini√ß√£o da Fam√≠lia de Fun√ß√µes:** Em seguida, definimos a fam√≠lia de fun√ß√µes $\mathcal{G} = \{g: \mathbb{R}^l \rightarrow \mathbb{Z}^m\}$, onde $g(œà) = (h_1(œà), ..., h_m(œà))$. Ou seja, *g* √© a concatena√ß√£o de *m* fun√ß√µes *hash* de $\mathcal{H}$.

3.  **Amostragem de Fun√ß√µes Compostas:** Amostramos *r* fun√ß√µes $g_1(œà), ..., g_r(œà)$ independentemente e uniformemente de $\mathcal{G}$, e cada fun√ß√£o $g_i$ √© usada para construir uma tabela *hash* $H_i$.

Dado o conjunto de *embeddings* de documentos $\Psi$ e os valores selecionados dos par√¢metros *r* e *m*, um √≠ndice LSH √© composto por *r* tabelas *hash*, cada uma contendo *m* *random projections* concatenadas [^31]. Para cada œà ‚àà Œ®, œà √© inserido no *bucket* $g_i(œà)$ para cada tabela *hash* $H_i$, para *i* = 1, ..., *r*. No momento do processamento da consulta, dada uma *embedding* de consulta, primeiro geramos um conjunto candidato de *embeddings* de documentos, tomando a uni√£o dos conte√∫dos de todos os *r* *buckets* nas *r* tabelas *hash* para as quais a consulta √© *hasheada*. A *embedding* do documento NN final √© computada realizando uma busca exata exaustiva dentro do conjunto candidato [^31].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo simplificado com *m* = 2 (duas fun√ß√µes hash) e *r* = 3 (tr√™s tabelas hash). Suponha que temos as seguintes embeddings de documentos:
>
> *   œà‚ÇÅ = [1.0, 2.0]
> *   œà‚ÇÇ = [1.5, 2.2]
> *   œà‚ÇÉ = [5.0, 1.0]
> *   œà‚ÇÑ = [5.2, 0.8]
>
> E as seguintes fun√ß√µes hash geradas aleatoriamente:
>
> *   h‚ÇÅ‚ÇÅ(œà) = œà[0] mod 5
> *   h‚ÇÅ‚ÇÇ(œà) = œà[1] mod 5
> *   h‚ÇÇ‚ÇÅ(œà) = œà[0] * 2 mod 5
> *   h‚ÇÇ‚ÇÇ(œà) = œà[1] * 2 mod 5
> *   h‚ÇÉ‚ÇÅ(œà) = œà[0] * 3 mod 5
> *   h‚ÇÉ‚ÇÇ(œà) = œà[1] * 3 mod 5
>
> Calculamos os valores hash para cada documento em cada tabela:
>
> **Tabela 1:**
>
> *   g‚ÇÅ(œà‚ÇÅ) = (1.0 mod 5, 2.0 mod 5) = (1, 2)
> *   g‚ÇÅ(œà‚ÇÇ) = (1.5 mod 5, 2.2 mod 5) = (1, 2)  (arredondando para inteiros)
> *   g‚ÇÅ(œà‚ÇÉ) = (5.0 mod 5, 1.0 mod 5) = (0, 1)
> *   g‚ÇÅ(œà‚ÇÑ) = (5.2 mod 5, 0.8 mod 5) = (0, 0)  (arredondando para inteiros)
>
> **Tabela 2:**
>
> *   g‚ÇÇ(œà‚ÇÅ) = (1.0 * 2 mod 5, 2.0 * 2 mod 5) = (2, 4)
> *   g‚ÇÇ(œà‚ÇÇ) = (1.5 * 2 mod 5, 2.2 * 2 mod 5) = (3, 4) (arredondando para inteiros)
> *   g‚ÇÇ(œà‚ÇÉ) = (5.0 * 2 mod 5, 1.0 * 2 mod 5) = (0, 2)
> *   g‚ÇÇ(œà‚ÇÑ) = (5.2 * 2 mod 5, 0.8 * 2 mod 5) = (0, 1) (arredondando para inteiros)
>
> **Tabela 3:**
>
> *   g‚ÇÉ(œà‚ÇÅ) = (1.0 * 3 mod 5, 2.0 * 3 mod 5) = (3, 1)
> *   g‚ÇÉ(œà‚ÇÇ) = (1.5 * 3 mod 5, 2.2 * 3 mod 5) = (4, 1) (arredondando para inteiros)
> *   g‚ÇÉ(œà‚ÇÉ) = (5.0 * 3 mod 5, 1.0 * 3 mod 5) = (0, 3)
> *   g‚ÇÉ(œà‚ÇÑ) = (5.2 * 3 mod 5, 0.8 * 3 mod 5) = (0, 2) (arredondando para inteiros)
>
> Se a consulta for œà = [1.2, 1.8], calculamos os valores hash para a consulta nas tr√™s tabelas:
>
> *   g‚ÇÅ(œà) = (1.2 mod 5, 1.8 mod 5) = (1, 1) (arredondando para inteiros)
> *   g‚ÇÇ(œà) = (1.2 * 2 mod 5, 1.8 * 2 mod 5) = (2, 4) (arredondando para inteiros)
> *   g‚ÇÉ(œà) = (1.2 * 3 mod 5, 1.8 * 3 mod 5) = (3, 1) (arredondando para inteiros)
>
> Os documentos candidatos seriam ent√£o œà‚ÇÅ, œà‚ÇÇ (da Tabela 1), œà‚ÇÅ (da Tabela 2) e œà‚ÇÅ (da Tabela 3).  Note que œà‚ÇÅ aparece tr√™s vezes, e œà‚ÇÇ uma vez.  Uma busca exaustiva entre estes candidatos (œà‚ÇÅ e œà‚ÇÇ) identificaria œà‚ÇÇ como o vizinho mais pr√≥ximo de œà.  Esta busca exaustiva √© crucial para garantir a precis√£o, pois o LSH √© uma t√©cnica de aproxima√ß√£o.

**Teorema 1** [Propriedades de Amplifica√ß√£o de Probabilidade]:
Dado que a probabilidade de colis√£o de dois vetores similares sob uma √∫nica fun√ß√£o hash √© *p‚ÇÅ*, e sob *m* fun√ß√µes hash concatenadas √© $p_1^m$. Analogamente, para vetores n√£o similares, a probabilidade √© $p_2^m$. Ao usar *r* tabelas hash independentes, a probabilidade de pelo menos uma das tabelas encontrar um vizinho pr√≥ximo √© $1 - (1-p_1^m)^r$ para vetores similares e $1 - (1-p_2^m)^r$ para vetores n√£o similares.  O objetivo do LSH √© maximizar $1 - (1-p_1^m)^r$ e minimizar $1 - (1-p_2^m)^r$.

*Proof Strategy:* A prova segue diretamente das propriedades de probabilidade independente. A probabilidade de falha em todas as *r* tabelas √© $(1-p_1^m)^r$ (similar) e $(1-p_2^m)^r$ (n√£o similar). A probabilidade de sucesso em pelo menos uma tabela √© o complemento dessas probabilidades.

> üí° **Exemplo Num√©rico:**
>
> Suponha que a probabilidade de colis√£o para vetores similares (*p‚ÇÅ*) sob uma √∫nica fun√ß√£o hash seja 0.8, e para vetores n√£o similares (*p‚ÇÇ*) seja 0.3.  Se usarmos *m* = 2 fun√ß√µes hash concatenadas e *r* = 3 tabelas hash:
>
> *   Probabilidade de colis√£o para vetores similares sob *m* fun√ß√µes hash: $p_1^m = 0.8^2 = 0.64$
> *   Probabilidade de colis√£o para vetores n√£o similares sob *m* fun√ß√µes hash: $p_2^m = 0.3^2 = 0.09$
> *   Probabilidade de pelo menos uma tabela encontrar um vizinho pr√≥ximo (similar): $1 - (1-0.64)^3 = 1 - (0.36)^3 = 1 - 0.046656 = 0.953344$
> *   Probabilidade de pelo menos uma tabela encontrar um vizinho pr√≥ximo (n√£o similar): $1 - (1-0.09)^3 = 1 - (0.91)^3 = 1 - 0.753571 = 0.246429$
>
> Observe que a probabilidade de encontrar vetores similares (0.953) √© muito maior do que a probabilidade de encontrar vetores n√£o similares (0.246), o que √© o objetivo do LSH. Este exemplo num√©rico demonstra como a escolha de *m* e *r* afeta as probabilidades de colis√£o e, consequentemente, o desempenho do LSH.

**Um exemplo de uso da fun√ß√£o Locality Sensitive Hashing (LSH)**:

> Considere o problema de identificar artigos similares a partir de um grande conjunto de artigos cient√≠ficos. Cada artigo √© representado por um vetor de alta dimens√£o, onde cada dimens√£o corresponde √† frequ√™ncia de uma palavra-chave espec√≠fica.
>
> Para aplicar LSH, podemos usar a t√©cnica de *random projection* [^31]. Essa t√©cnica envolve projetar cada vetor de artigo em um n√∫mero menor de dimens√µes usando uma matriz aleat√≥ria. Essa matriz √© criada de forma que vetores pr√≥ximos permane√ßam pr√≥ximos ap√≥s a proje√ß√£o.
>
> Durante a fase de indexa√ß√£o, cada artigo √© projetado usando a matriz aleat√≥ria e, em seguida, atribu√≠do a um *bucket* em uma tabela *hash* com base nos valores resultantes. V√°rios artigos podem acabar no mesmo *bucket* se seus vetores projetados forem semelhantes.
>
> Durante a fase de consulta, o vetor da consulta tamb√©m √© projetado usando a mesma matriz aleat√≥ria e, em seguida, procuramos o *bucket* correspondente na tabela *hash*. Os artigos encontrados neste *bucket* s√£o considerados candidatos potenciais e podem ser ainda mais refinados usando uma busca mais precisa, mas computacionalmente mais cara.
>
> Este m√©todo LSH permite uma pesquisa aproximada, mas r√°pida, por artigos semelhantes em um *dataset* grande, pois s√≥ precisamos comparar a consulta com os artigos no mesmo *bucket* em vez de todos os artigos no *dataset*.

**Proposi√ß√£o 1** [Sensibilidade aos Par√¢metros]: A escolha dos par√¢metros *m* e *r* afeta significativamente o desempenho do LSH. Aumentar *m* reduz a probabilidade de falsos positivos (vetores n√£o similares sendo considerados similares), mas tamb√©m pode aumentar a probabilidade de falsos negativos (vetores similares n√£o sendo encontrados). Aumentar *r* aumenta a probabilidade de encontrar vizinhos pr√≥ximos, mas tamb√©m aumenta o custo computacional e de mem√≥ria.

> üí° **Exemplo Num√©rico:**
>
> Considere um cen√°rio onde estamos buscando documentos relevantes para uma consulta. Temos duas configura√ß√µes LSH:
>
> *   Configura√ß√£o A: *m* = 3, *r* = 5
> *   Configura√ß√£o B: *m* = 5, *r* = 3
>
> Suponha que, ap√≥s avaliar ambas as configura√ß√µes em um conjunto de testes, obtemos os seguintes resultados:
>
> | Configura√ß√£o | Precis√£o | Recall | Tempo de Busca (ms) | Mem√≥ria Utilizada (MB) |
> |--------------|----------|--------|----------------------|-----------------------|
> | A (*m*=3, *r*=5) | 0.65     | 0.85   | 50                   | 200                   |
> | B (*m*=5, *r*=3) | 0.80     | 0.70   | 30                   | 150                   |
>
> An√°lise:
>
> *   A Configura√ß√£o A (mais tabelas hash, menos fun√ß√µes por tabela) oferece um recall maior (0.85), o que significa que ela encontra mais documentos relevantes, mas com menor precis√£o (0.65), indicando que alguns dos documentos retornados n√£o s√£o relevantes.  O tempo de busca √© maior, assim como o consumo de mem√≥ria.
> *   A Configura√ß√£o B (menos tabelas hash, mais fun√ß√µes por tabela) oferece uma precis√£o maior (0.80), significando que a maioria dos documentos retornados s√£o relevantes, mas com um recall menor (0.70), indicando que alguns documentos relevantes s√£o perdidos. O tempo de busca √© menor, assim como o consumo de mem√≥ria.
>
> Este exemplo ilustra o *trade-off* entre precis√£o e recall controlado pelos par√¢metros *m* e *r*. A escolha da configura√ß√£o ideal depende dos requisitos espec√≠ficos da aplica√ß√£o. Se a prioridade for encontrar todos os documentos relevantes, mesmo que alguns n√£o sejam t√£o precisos, a Configura√ß√£o A pode ser prefer√≠vel. Se a prioridade for retornar apenas documentos altamente relevantes, mesmo que alguns sejam perdidos, a Configura√ß√£o B pode ser prefer√≠vel.

**Lema 1** [Trade-off entre Precis√£o e Recall]: Existe um trade-off inerente entre precis√£o e recall no LSH. Aumentar o n√∫mero de tabelas hash (*r*) melhora o recall (a probabilidade de encontrar todos os vizinhos mais pr√≥ximos verdadeiros), mas tamb√©m diminui a precis√£o (a propor√ß√£o de vizinhos retornados que s√£o verdadeiramente os mais pr√≥ximos). Ajustar os par√¢metros *m* e *r* permite controlar este trade-off.

### Desvantagens

A principal desvantagem do √≠ndice LSH √© que ele pode exigir um grande n√∫mero de tabelas *hash* para cobrir a maioria dos vizinhos mais pr√≥ximos [^31]. Ele tamb√©m exige o armazenamento das *embeddings* originais para realizar a busca exata exaustiva. Embora algumas otimiza√ß√µes tenham sido propostas [Lv et al. 2007], o consumo de espa√ßo pode ser proibitivo com conjuntos de dados muito grandes [^31].

**Teorema 1.1** [Redu√ß√£o da Dimens√£o para Otimiza√ß√£o de Espa√ßo]: A combina√ß√£o do LSH com t√©cnicas de redu√ß√£o de dimensionalidade, como PCA (Principal Component Analysis) ou autoencoders, pode mitigar o problema do alto consumo de espa√ßo. Ao reduzir a dimensionalidade das *embeddings* antes de aplicar o LSH, o tamanho de cada *embedding* e, portanto, o espa√ßo necess√°rio para armazenar as *embeddings* originais, √© diminu√≠do.

*Proof Strategy:* A prova se baseia no fato de que PCA e autoencoders s√£o m√©todos comprovados para reduzir a dimensionalidade enquanto preservam a informa√ß√£o sem√¢ntica relevante. Ao aplicar esses m√©todos antes do LSH, √© poss√≠vel reduzir o espa√ßo de armazenamento sem afetar significativamente a precis√£o da busca.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados de 1 milh√£o de documentos, cada um representado por uma embedding de 1000 dimens√µes (float32).
>
> *   **Sem redu√ß√£o de dimensionalidade:** O espa√ßo total necess√°rio para armazenar as embeddings √© de 1,000,000 * 1000 * 4 bytes = 4 GB.
>
> Agora, aplicamos PCA para reduzir a dimensionalidade para 100 dimens√µes.
>
> *   **Com redu√ß√£o de dimensionalidade:** O espa√ßo total necess√°rio para armazenar as embeddings reduzidas √© de 1,000,000 * 100 * 4 bytes = 400 MB.
>
> Isso representa uma redu√ß√£o de 90% no espa√ßo de armazenamento. Embora a PCA possa resultar em alguma perda de informa√ß√£o, a redu√ß√£o significativa no espa√ßo de armazenamento pode compensar essa perda, especialmente se a aplica√ß√£o puder tolerar uma pequena diminui√ß√£o na precis√£o da busca. Al√©m disso, podemos quantizar as embeddings resultantes da PCA para reduzir o espa√ßo ainda mais.

### Conclus√£o
Locality Sensitive Hashing oferece uma abordagem pr√°tica para busca aproximada de vizinhos mais pr√≥ximos em dados de alta dimens√£o, sacrificando precis√£o em favor da velocidade. Embora o m√©todo LSH possua desvantagens como o consumo de mem√≥ria e a necessidade de muitas tabelas *hash* [^31], ele continua sendo uma ferramenta valiosa no *toolbox* de *Information Retrieval*. Outras t√©cnicas, como quantiza√ß√£o vetorial e abordagens baseadas em grafos, oferecem alternativas que tamb√©m equilibram a precis√£o da busca e a velocidade [^31].

### Refer√™ncias
[^30]: Se√ß√£o 4.2, "MIP and NN Search Problems".
[^31]: Se√ß√£o 4.3, "Locality sensitive hashing approaches".
<!-- END -->