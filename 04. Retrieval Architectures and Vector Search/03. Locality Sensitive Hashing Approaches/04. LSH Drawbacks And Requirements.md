## Locality Sensitive Hashing: Addressing the Curse of Dimensionality and Storage Challenges

### Introdu√ß√£o
Como discutido na Se√ß√£o 4.3, a **Locality Sensitive Hashing (LSH)** [^15] √© uma t√©cnica fundamental para busca aproximada de vizinhos mais pr√≥ximos (ANN) em sistemas de recupera√ß√£o densa. A ideia central do LSH √© que, se duas embeddings est√£o pr√≥ximas, ent√£o ap√≥s uma "proje√ß√£o" usando uma fun√ß√£o hash, elas permanecer√£o pr√≥ximas. No entanto, um dos principais obst√°culos para a ado√ß√£o generalizada do LSH √© a necessidade de um grande n√∫mero de tabelas hash para cobrir a maioria dos vizinhos mais pr√≥ximos, bem como a necessidade de armazenar as embeddings originais para a busca exata exaustiva [^15]. Esta se√ß√£o explora em profundidade esses desafios e as implica√ß√µes para sistemas de recupera√ß√£o em larga escala.

### Desafios Fundamentais do LSH
A efici√™ncia do LSH depende da capacidade de projetar dados de alta dimens√£o em representa√ß√µes de baixa dimens√£o, mantendo ao mesmo tempo as rela√ß√µes de proximidade. Isso √© alcan√ßado usando fam√≠lias de fun√ß√µes hash *locality-sensitive*, que atribuem alta probabilidade ($p_1$) para embeddings pr√≥ximas serem hasheadas no mesmo bucket, e baixa probabilidade ($p_2 < p_1$) para embeddings distantes [^15].

No entanto, essa abordagem enfrenta desafios intr√≠nsecos:
1.  **Sensibilidade aos par√¢metros:** A efic√°cia do LSH depende fortemente da escolha correta dos par√¢metros, como o n√∫mero de fun√ß√µes hash ($m$) e o n√∫mero de tabelas hash ($r$). Determinar esses par√¢metros pode ser computacionalmente caro e dependente do conjunto de dados.
2.  **Compromisso entre precis√£o e custo:** Para garantir alta precis√£o, o n√∫mero de tabelas hash ($r$) precisa ser suficientemente grande para cobrir a maioria dos vizinhos mais pr√≥ximos. No entanto, isso leva a um aumento no consumo de mem√≥ria e tempo de busca.
3.  **Armazenamento de embeddings originais:** O procedimento padr√£o do LSH envolve a realiza√ß√£o de uma busca exaustiva dentro do conjunto candidato recuperado para refinar os resultados. Isto exige armazenar as embeddings originais, o que pode ser proibitivo para conjuntos de dados muito grandes [^15].

Para mitigar esses desafios, v√°rias otimiza√ß√µes e extens√µes do LSH t√™m sido propostas. Estas incluem t√©cnicas para adapta√ß√£o autom√°tica de par√¢metros, m√©todos para reduzir o n√∫mero de tabelas hash necess√°rias e estrat√©gias para reduzir a necessidade de armazenar as embeddings originais. Exploraremos algumas dessas otimiza√ß√µes posteriormente nesta se√ß√£o.

**Teorema 1** (Amplia√ß√£o do bucket de colis√£o): Dado um conjunto de dados $D$ e uma consulta $q$, a probabilidade de encontrar pelo menos um vizinho pr√≥ximo de $q$ em pelo menos uma das $r$ tabelas hash aumenta com o aumento do tamanho do bucket de colis√£o, mas a precis√£o da busca diminui devido ao aumento de falsos positivos.

*Estrat√©gia da prova:* O aumento do tamanho do bucket aumenta a chance de um vizinho pr√≥ximo ser hasheado para o mesmo bucket que a consulta. No entanto, tamb√©m aumenta a chance de pontos de dados n√£o relevantes serem hasheados para o mesmo bucket, levando a falsos positivos e, portanto, menor precis√£o. O ponto ideal de tamanho do bucket depende da distribui√ß√£o dos dados e do compromisso desejado entre recall e precis√£o. $\blacksquare$

### O Problema do N√∫mero de Tabelas Hash

O n√∫mero de tabelas hash ($r$) em um √≠ndice LSH √© crucial para a precis√£o da busca. Para entender por que um grande n√∫mero de tabelas hash √© frequentemente necess√°rio, considere o seguinte cen√°rio:

*   Sejam $q$ uma query embedding e $d$ uma embedding de documento.
*   A probabilidade de $q$ e $d$ serem hasheadas para o mesmo bucket em uma √∫nica tabela hash √© $p$.
*   A probabilidade de $q$ e $d$ *n√£o* serem hasheadas para o mesmo bucket em uma √∫nica tabela hash √© $1-p$.
*   Com $r$ tabelas hash, a probabilidade de $q$ e $d$ *n√£o* serem hasheadas para o mesmo bucket em *nenhuma* das tabelas hash √© $(1-p)^r$.

Para garantir que a embedding de documento $d$ (que √© um vizinho mais pr√≥ximo verdadeiro de $q$) seja recuperada com alta probabilidade, $(1-p)^r$ deve ser mantido baixo. Isso requer um grande valor de $r$, especialmente quando $p$ √© pequeno.

**Exemplo:** Se a probabilidade $p$ de duas embeddings pr√≥ximas serem hasheadas no mesmo bucket √© 0.1 e queremos garantir uma probabilidade de pelo menos 90% de que a embedding seja encontrada em pelo menos uma tabela hash, precisamos calcular o n√∫mero de tabelas hash $r$ tal que:

$$1 - (1 - p)^r \geq 0.9$$
$$1 - (1 - 0.1)^r \geq 0.9$$
$$(0.9)^r \leq 0.1$$
$$r \geq \frac{log(0.1)}{log(0.9)} \approx 21.85$$

Portanto, precisamos de pelo menos 22 tabelas hash para atingir o n√≠vel de precis√£o desejado. Este exemplo ilustra como o n√∫mero de tabelas hash pode aumentar rapidamente √† medida que a probabilidade de hashing bem-sucedido em uma √∫nica tabela diminui.

> üí° **Exemplo Num√©rico:** Vamos considerar uma situa√ß√£o mais realista. Suponha que temos um sistema LSH com $r=10$ tabelas hash. Se a probabilidade de uma query e um documento relevante serem hasheados para o mesmo bucket em qualquer tabela √© $p=0.2$, ent√£o a probabilidade de *n√£o* serem hasheados no mesmo bucket em nenhuma das tabelas √© $(1-0.2)^{10} = 0.8^{10} \approx 0.107$. Isso significa que h√° aproximadamente 10.7% de chance de perdermos um documento relevante. Para reduzir essa chance, precisamos aumentar o n√∫mero de tabelas hash. Se aumentarmos para $r=30$, a probabilidade de perdermos o documento relevante cai para $(0.8)^{30} \approx 0.0012$, ou 0.12%. Esse exemplo demonstra o *trade-off* entre o n√∫mero de tabelas hash e a probabilidade de recall.

Para complementar essa an√°lise, podemos introduzir o conceito de *amplifica√ß√£o de LSH*. A amplifica√ß√£o de LSH visa aumentar o contraste entre a probabilidade de colis√£o de vizinhos pr√≥ximos ($p_1$) e a probabilidade de colis√£o de vizinhos distantes ($p_2$).

**Teorema 1.1** (Amplifica√ß√£o de LSH): Dada uma fam√≠lia de fun√ß√µes hash LSH com probabilidades de colis√£o $p_1$ e $p_2$ para vizinhos pr√≥ximos e distantes, respectivamente, √© poss√≠vel construir uma nova fam√≠lia de fun√ß√µes hash que amplifica a diferen√ßa entre essas probabilidades, reduzindo assim o n√∫mero de tabelas hash necess√°rias para atingir uma dada precis√£o.

*Estrat√©gia da prova:* A amplifica√ß√£o de LSH tipicamente envolve combinar m√∫ltiplas fun√ß√µes hash LSH para criar uma nova fun√ß√£o hash. Por exemplo, podemos usar a fun√ß√£o `OR` para combinar $k$ fun√ß√µes hash, de forma que duas embeddings s√£o consideradas colidindo se colidirem em pelo menos uma das $k$ fun√ß√µes hash. Alternativamente, podemos usar a fun√ß√£o `AND` para combinar $k$ fun√ß√µes hash, de forma que duas embeddings s√£o consideradas colidindo apenas se colidirem em todas as $k$ fun√ß√µes hash. A escolha da combina√ß√£o (OR vs AND) depende das caracter√≠sticas dos dados e do compromisso desejado entre recall e precis√£o. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que $p_1 = 0.8$ e $p_2 = 0.3$. Se usarmos a fun√ß√£o AND com $k=2$, as novas probabilidades ser√£o $p_1' = p_1^2 = 0.64$ e $p_2' = p_2^2 = 0.09$. A diferen√ßa entre $p_1$ e $p_2$ foi amplificada. Se usarmos a fun√ß√£o OR com $k=2$, as novas probabilidades ser√£o $p_1' = 1 - (1-p_1)^2 = 0.96$ e $p_2' = 1 - (1-p_2)^2 = 0.51$.  A escolha entre AND e OR depender√° do sistema de recupera√ß√£o desejado.

### Armazenamento de Embeddings Originais

Mesmo com m√∫ltiplas tabelas hash, o LSH normalmente retorna um conjunto de candidatos que requer verifica√ß√£o adicional. Essa verifica√ß√£o √© geralmente realizada comparando a query embedding com as embeddings originais dos documentos candidatos, um processo conhecido como *busca exaustiva*. A necessidade de busca exaustiva surge devido √† natureza aproximada do LSH, onde falsos positivos (i.e., embeddings que s√£o hasheadas no mesmo bucket, mas n√£o s√£o realmente vizinhos mais pr√≥ximos) s√£o inevit√°veis.

Armazenar as embeddings originais para busca exaustiva aumenta significativamente os requisitos de mem√≥ria, especialmente para conjuntos de dados em larga escala. Por exemplo, considere um conjunto de dados com 1 bilh√£o de documentos, onde cada embedding tem 768 dimens√µes (como em BERT [^8]) e cada dimens√£o requer 4 bytes de armazenamento (ponto flutuante de precis√£o √∫nica). O requisito total de armazenamento para as embeddings originais seria:

$$10^9 \text{ documentos} \times 768 \text{ dimens√µes} \times 4 \text{ bytes/dimens√£o} \approx 3 \text{ TB}$$

Este custo de armazenamento pode ser proibitivo, tornando o LSH impratic√°vel para certas aplica√ß√µes.

Para abordar o problema do armazenamento das embeddings originais, uma t√©cnica comum √© a *quantiza√ß√£o de vetores*. A quantiza√ß√£o de vetores envolve aproximar as embeddings originais por um conjunto menor de vetores representativos, chamados *centroides*.

**Teorema 2** (Quantiza√ß√£o de vetores para redu√ß√£o de armazenamento): Ao quantizar as embeddings originais usando um livro de c√≥digos de tamanho $k$, o requisito de armazenamento √© reduzido em um fator proporcional √† raz√£o entre a dimens√£o da embedding original e o n√∫mero de bits necess√°rios para representar o √≠ndice do centroide mais pr√≥ximo. No entanto, essa redu√ß√£o de armazenamento ocorre ao custo de alguma perda de precis√£o na busca de vizinhos mais pr√≥ximos.

*Estrat√©gia da prova:* Cada embedding original √© substitu√≠da pelo √≠ndice do centroide mais pr√≥ximo no livro de c√≥digos. Portanto, em vez de armazenar um vetor de alta dimens√£o para cada documento, armazenamos apenas um inteiro que representa o √≠ndice do centroide. O n√∫mero de bits necess√°rios para representar o √≠ndice √© $\log_2(k)$, onde $k$ √© o tamanho do livro de c√≥digos. A precis√£o √© afetada porque a dist√¢ncia entre a consulta e a embedding quantizada √© uma aproxima√ß√£o da dist√¢ncia entre a consulta e a embedding original. $\blacksquare$

**Lema 2.1:** (Impacto do tamanho do livro de c√≥digos na precis√£o) A precis√£o da busca de vizinhos mais pr√≥ximos usando embeddings quantizadas aumenta com o aumento do tamanho do livro de c√≥digos $k$, mas com retornos decrescentes.

*Estrat√©gia da prova:* √Ä medida que $k$ aumenta, a aproxima√ß√£o das embeddings originais pelas embeddings quantizadas se torna mais precisa, levando a uma maior precis√£o. No entanto, √† medida que $k$ se aproxima do n√∫mero de embeddings originais, o ganho em precis√£o diminui, e o custo computacional de encontrar o centroide mais pr√≥ximo aumenta. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos embeddings de dimens√£o 768 (como no exemplo anterior) e usamos quantiza√ß√£o de vetores com um livro de c√≥digos de tamanho $k = 256$. Cada √≠ndice requer $\log_2(256) = 8$ bits = 1 byte. Em vez de armazenar 768 * 4 bytes = 3072 bytes por embedding, armazenamos apenas 1 byte. A redu√ß√£o de armazenamento √© de um fator de 3072. No entanto, essa redu√ß√£o vem com uma perda de precis√£o. O qu√£o grande essa perda ser√° depende da qualidade do livro de c√≥digos e da distribui√ß√£o dos dados.
>
> Para entender o *trade-off* entre precis√£o e custo, podemos criar a seguinte tabela:
>
> | Tamanho do livro de c√≥digos ($k$) | Bits por embedding | Tamanho total (1 bilh√£o de documentos) | Precis√£o (aproximada) |
> |---------------------------------|--------------------|------------------------------------------|-----------------------|
> | 16                              | 4 bits             | 0.5 GB                                     | 0.60                  |
> | 64                              | 6 bits             | 0.75 GB                                    | 0.75                  |
> | 256                             | 8 bits             | 1 GB                                       | 0.85                  |
> | 1024                            | 10 bits            | 1.25 GB                                    | 0.92                  |
> | Sem quantiza√ß√£o                   | 3072 bytes         | 3 TB                                       | 0.99                  |
>
> Essa tabela ilustra como a precis√£o aumenta com o tamanho do livro de c√≥digos, mas com retornos decrescentes.

### Conclus√£o

Enquanto o LSH oferece uma solu√ß√£o eficiente para busca aproximada de vizinhos mais pr√≥ximos, a necessidade de um grande n√∫mero de tabelas hash e o armazenamento de embeddings originais apresentam desafios significativos, particularmente em conjuntos de dados em larga escala. O desafio de otimizar esses aspectos tem levado √† pesquisa cont√≠nua de t√©cnicas aprimoradas que equilibram a precis√£o da busca com os custos de armazenamento e computacionais [^24, 25].

No entanto, a combina√ß√£o desses dois fatores (muitas tabelas e reten√ß√£o dos vetores originais) faz com que o espa√ßo se torne rapidamente proibitivo [^15]. Isso levou a pesquisar abordagens alternativas para busca aproximada de vizinhos mais pr√≥ximos, como abordagens de quantiza√ß√£o de vetores (Se√ß√£o 4.4) e abordagens baseadas em grafos (Se√ß√£o 4.5).

### Refer√™ncias
[^15]: Indyk, P. and Motwani, R. 1998. Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality. In *Proc. STOC*, —Ä. 604‚Äì613.
[^8]: Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In *Proc. NAACL*, pp. 4171‚Äì4186.
[^24]: Lv, Q., Josephson, W., Wang, Z., Charikar, M., and Li, K. 2007. Multi-Probe LSH: Efficient Indexing for High-Dimensional Similarity Search . In *Proc. VLDB*, pp. 950‚Äì961.
[^25]: Datar, M., Immorlica, N., Indyk, P., and Mirrokni, V. S. 2004. Locality-Sensitive Hashing Scheme Based on p-Stable Distributions. In *Proc. SoCG*, p. 253‚Äì262.
<!-- END -->