## 4.4.1 Vector Quantisation with Data-Driven Partitioning

### Introdu√ß√£o
Expandindo sobre os m√©todos de indexa√ß√£o aproximada para **Nearest Neighbor Search** (ANN) em *dense retrieval*, exploraremos as abordagens de **Vector Quantisation** (VQ). Ao contr√°rio do **Locality Sensitive Hashing** (LSH) [^31] que particiona o espa√ßo de entrada aleatoriamente, o VQ visa particionar o espa√ßo de entrada $\Psi$ de acordo com a distribui√ß√£o dos dados [^32].

### Conceitos Fundamentais
A ideia central do VQ reside em adaptar a estrutura do √≠ndice √† distribui√ß√£o dos dados, prometendo, potencialmente, uma representa√ß√£o mais fiel e eficiente do espa√ßo vetorial. O processo come√ßa com o uso de algoritmos de *clustering*, como o *k-means*, para computar *k* centr√≥ides ($\mu_1, ..., \mu_k$) no conjunto de dados $\Psi$ [^32]. Cada centr√≥ide $\mu_i \in \mathbb{R}^l$ representa o centro de um cluster, onde *l* √© a dimens√£o dos vetores de embedding. O conjunto desses centr√≥ides, $M = \{\mu_1, ..., \mu_k\}$, forma o que √© conhecido como *codebook* [^32].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de dados $\Psi$ com apenas 4 vetores bidimensionais: $\psi_1 = [1, 2]$, $\psi_2 = [1.5, 1.8]$, $\psi_3 = [5, 8]$, $\psi_4 = [8, 8]$. Vamos usar o algoritmo k-means para criar um codebook com *k*=2 centr√≥ides. Ap√≥s executar o k-means, suponha que os centr√≥ides resultantes s√£o: $\mu_1 = [1.25, 1.9]$ e $\mu_2 = [6.5, 8]$. Portanto, nosso codebook √© $M = \{[1.25, 1.9], [6.5, 8]\}$.

Um *vector quantizer* $q: \mathbb{R}^l \rightarrow \mathbb{R}^l$ mapeia cada vetor $\psi \in \Psi$ para o centr√≥ide mais pr√≥ximo no codebook [^32]:
$$
q(\psi) = \arg \min_{\mu \in M} ||\psi - \mu||
$$
Este mapeamento efetivamente quantifica os vetores de entrada, representando-os atrav√©s de um conjunto discreto de vetores prot√≥tipos (os centr√≥ides).

> üí° **Exemplo Num√©rico:**
>
> Usando o codebook do exemplo anterior, vamos quantizar o vetor $\psi_1 = [1, 2]$. Precisamos calcular as dist√¢ncias euclidianas de $\psi_1$ para cada centr√≥ide em $M$:
>
> *   $||\psi_1 - \mu_1|| = ||[1, 2] - [1.25, 1.9]|| = \sqrt{(1-1.25)^2 + (2-1.9)^2} = \sqrt{0.0625 + 0.01} = \sqrt{0.0725} \approx 0.269$
> *   $||\psi_1 - \mu_2|| = ||[1, 2] - [6.5, 8]|| = \sqrt{(1-6.5)^2 + (2-8)^2} = \sqrt{30.25 + 36} = \sqrt{66.25} \approx 8.14$
>
> Como $||\psi_1 - \mu_1|| < ||\psi_1 - \mu_2||$, o vetor $\psi_1$ √© mapeado para o centr√≥ide $\mu_1 = [1.25, 1.9]$. Portanto, $q(\psi_1) = [1.25, 1.9]$.

A escolha do n√∫mero de centr√≥ides *k* √© crucial. Um valor *k* muito pequeno pode levar a uma quantiza√ß√£o grosseira, resultando em perda de informa√ß√£o significativa e, consequentemente, em menor precis√£o na busca. Por outro lado, um valor *k* muito grande pode aumentar a complexidade computacional e de armazenamento, diminuindo os benef√≠cios da quantiza√ß√£o.

Para complementar a discuss√£o sobre a escolha de *k*, podemos formalizar a no√ß√£o de erro de quantiza√ß√£o.

**Defini√ß√£o 1 (Erro de Quantiza√ß√£o).** Dado um vetor $\psi \in \Psi$ e um codebook $M$, o erro de quantiza√ß√£o $e(\psi)$ √© definido como a dist√¢ncia entre $\psi$ e o centr√≥ide mais pr√≥ximo em $M$:
$$
e(\psi) = ||\psi - q(\psi)||
$$
O erro de quantiza√ß√£o m√©dio sobre todo o conjunto de dados $\Psi$ √© dado por:
$$
E = \frac{1}{|\Psi|} \sum_{\psi \in \Psi} e(\psi)
$$

> üí° **Exemplo Num√©rico:**
>
> Continuando com o exemplo anterior, o erro de quantiza√ß√£o para $\psi_1$ √©:
>
> $e(\psi_1) = ||\psi_1 - q(\psi_1)|| = ||[1, 2] - [1.25, 1.9]|| \approx 0.269$
>
> Se quantizarmos todos os vetores em $\Psi$ e obtivermos os seguintes erros de quantiza√ß√£o: $e(\psi_1) = 0.269$, $e(\psi_2) = 0.0$, $e(\psi_3) = 2.1$, $e(\psi_4) = 1.5$, ent√£o o erro de quantiza√ß√£o m√©dio √©:
>
> $E = \frac{0.269 + 0.0 + 2.1 + 1.5}{4} = \frac{3.869}{4} \approx 0.967$.  Este valor representa a m√©dia da dist√¢ncia entre cada vetor e seu centr√≥ide atribu√≠do.  Um valor mais baixo de E indica uma melhor representa√ß√£o dos dados pelo codebook.

Intuitivamente, minimizar *E* implica em escolher um codebook *M* que represente bem os dados. A escolha de *k* influencia diretamente o valor de *E*; aumentar *k* tende a diminuir *E*, mas com custos computacionais e de armazenamento associados.

### Vantagens e Desafios
A principal vantagem do VQ √© a sua capacidade de adaptar a estrutura do √≠ndice √† distribui√ß√£o dos dados, o que pode resultar em melhor precis√£o na busca em compara√ß√£o com m√©todos de particionamento aleat√≥rio como o LSH. No entanto, o VQ tamb√©m apresenta desafios. A computa√ß√£o dos centr√≥ides atrav√©s do algoritmo *k-means* pode ser computacionalmente cara, especialmente para grandes conjuntos de dados. Al√©m disso, a escolha do valor ideal de *k* requer uma an√°lise cuidadosa do compromisso entre precis√£o e efici√™ncia.

![Ranking pipeline architecture for multiple representation systems using learned embeddings and ANN search.](./../images/image3.png)

Para mitigar o custo computacional do *k-means*, √© comum empregar varia√ß√µes aproximadas do algoritmo, ou m√©todos hier√°rquicos.

**Teorema 1 (Complexidade do k-means).** O algoritmo k-means padr√£o tem complexidade $O(nkl)$, onde *n* √© o n√∫mero de vetores de dados, *k* √© o n√∫mero de clusters e *l* √© o n√∫mero de itera√ß√µes.

*Prova.* Cada itera√ß√£o do k-means envolve atribuir cada um dos *n* vetores ao centr√≥ide mais pr√≥ximo (complexidade $O(nk)$) e recalcular os *k* centr√≥ides (complexidade $O(nk)$). Se o algoritmo converge em *l* itera√ß√µes, a complexidade total √© $O(nkl)$.

**Observa√ß√£o 1.** Devido √† complexidade do k-means, diversas otimiza√ß√µes s√£o frequentemente utilizadas na pr√°tica, como o uso de mini-batches para atualizar os centr√≥ides, resultando em algoritmos como o *mini-batch k-means*. Essas otimiza√ß√µes podem reduzir significativamente o tempo de execu√ß√£o, especialmente para grandes conjuntos de dados.

Al√©m disso, a sensibilidade do k-means √† inicializa√ß√£o dos centr√≥ides pode impactar a qualidade do codebook. Estrat√©gias como o k-means++ s√£o empregadas para melhorar a inicializa√ß√£o e evitar converg√™ncia para √≥timos locais ruins.

### Conclus√£o
O Vector Quantisation oferece uma alternativa promissora ao LSH para indexa√ß√£o aproximada em dense retrieval. Ao adaptar a estrutura do √≠ndice √† distribui√ß√£o dos dados, o VQ pode alcan√ßar maior precis√£o na busca. No entanto, a escolha do n√∫mero ideal de centr√≥ides e o custo computacional da computa√ß√£o dos centr√≥ides s√£o desafios importantes que devem ser considerados.

### Refer√™ncias
[^31]: Indyk, P. and Motwani, R. 1998. Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality. In Proc. STOC, —Ä. 604‚Äì613.
[^32]: Gersho, A. and Gray, R. M. 1992. Vector Quantization and Signal Compression. Kluwer.
<!-- END -->