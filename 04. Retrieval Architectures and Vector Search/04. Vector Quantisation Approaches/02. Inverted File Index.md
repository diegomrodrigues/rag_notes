## Vector Quantisation Approaches in Inverted File Structures for Neural Information Retrieval

### Introdu√ß√£o
Este cap√≠tulo aprofunda-se nas t√©cnicas de **vector quantisation** (VQ) aplicadas em conjunto com estruturas de **inverted file** (IVF) para otimizar a efici√™ncia da pesquisa em sistemas de *Neural Information Retrieval* (NIR). Particularmente, focaremos em como o √≠ndice IVF organiza embeddings de documentos em parti√ß√µes, explorando o *trade-off* entre precis√£o e tempo de pesquisa, e discutindo a limita√ß√£o do alto n√∫mero de *centroids* que podem ser necess√°rios [^32].

### Conceitos Fundamentais
Em contraste com o *Locality Sensitive Hashing* (LSH) [^31], que particiona o espa√ßo de entrada de maneira aleat√≥ria, a quantiza√ß√£o vetorial particiona o espa√ßo de entrada $\Psi$ de acordo com a distribui√ß√£o dos dados [^32]. Esta abordagem aproveita o algoritmo de *k-means clustering* para computar $k$ *centroids* $\mu_1, ..., \mu_k$, onde $\mu_i \in \mathbb{R}^l$ para $i = 1, ..., k$. Estes *centroids* s√£o ent√£o utilizados para particionar o espa√ßo de entrada $\Psi$. O conjunto $M = \{\mu_1, ..., \mu_k\}$ √© denominado *codebook* [^32].

> üí° **Exemplo Num√©rico:** Suponha que temos um espa√ßo de embeddings bidimensional (i.e., $l=2$) e queremos particion√°-lo usando k-means com $k=3$. Ap√≥s executar o algoritmo, obtemos os seguintes centroids: $\mu_1 = [1.0, 2.0]$, $\mu_2 = [4.0, 5.0]$, $\mu_3 = [7.0, 8.0]$. Estes centroids representam os centros de tr√™s clusters no espa√ßo de embeddings.

Dado um *codebook* $M$, um **vector quantiser** $q : \mathbb{R}^l \rightarrow \mathbb{R}^l$ mapeia um vetor $\psi$ para o seu *centroid* mais pr√≥ximo [^32]:

$$
q(\psi) = \underset{\mu \in M}{\operatorname{argmin}} \ ||\psi - \mu||
$$

Esta opera√ß√£o de quantiza√ß√£o √© fundamental para a constru√ß√£o de √≠ndices IVF.

**Proposi√ß√£o 1.** *O vector quantiser $q$ √© uma fun√ß√£o de proje√ß√£o, ou seja, $q(q(\psi)) = q(\psi)$ para todo $\psi \in \mathbb{R}^l$.*

*Proof.* Pela defini√ß√£o de $q(\psi)$, o resultado da quantiza√ß√£o √© sempre um centroid $\mu \in M$.  Portanto, aplicar $q$ novamente ao resultado simplesmente retorna o mesmo centroid, pois o centroid j√° √© um elemento do codebook. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere o vetor $\psi = [1.2, 2.3]$ e o codebook definido no exemplo anterior com os centroids $\mu_1 = [1.0, 2.0]$, $\mu_2 = [4.0, 5.0]$, $\mu_3 = [7.0, 8.0]$.
>
> Calculamos as dist√¢ncias euclidianas:
>
>  $||\psi - \mu_1|| = \sqrt{(1.2 - 1.0)^2 + (2.3 - 2.0)^2} = \sqrt{0.04 + 0.09} = \sqrt{0.13} \approx 0.36$
>
>  $||\psi - \mu_2|| = \sqrt{(1.2 - 4.0)^2 + (2.3 - 5.0)^2} = \sqrt{7.84 + 7.29} = \sqrt{15.13} \approx 3.89$
>
>  $||\psi - \mu_3|| = \sqrt{(1.2 - 7.0)^2 + (2.3 - 8.0)^2} = \sqrt{33.64 + 32.49} = \sqrt{66.13} \approx 8.13$
>
> Como $||\psi - \mu_1||$ √© a menor dist√¢ncia, $q(\psi) = \mu_1 = [1.0, 2.0]$. Aplicando a fun√ß√£o novamente, $q(q(\psi)) = q([1.0, 2.0]) = [1.0, 2.0]$, confirmando a propriedade de proje√ß√£o.

#### Inverted File Index com Quantiza√ß√£o Vetorial

Dado um *codebook* $M$, um √≠ndice *Inverted File* (IVF) constru√≠do sobre $M$ e $\Psi$ armazena o conjunto de *document embeddings* $\Psi$ em $k$ parti√ß√µes, ou *inverted lists* $L_1, ..., L_k$, onde $L_i = \{\psi \in \Psi : q(\psi) = \mu_i\}$ [^32]. Em outras palavras, cada *inverted list* $L_i$ cont√©m todos os *document embeddings* que s√£o mapeados para o *centroid* $\mu_i$ pelo *vector quantiser* $q$.

**Teorema 1.** Seja $\Psi \subset \mathbb{R}^l$ um conjunto de *document embeddings*. A complexidade de espa√ßo para armazenar um √≠ndice IVF √© $O(|\Psi| + k)$, onde $|\Psi|$ √© o n√∫mero de embeddings e $k$ √© o n√∫mero de centroids.

*Proof.* O √≠ndice IVF armazena cada embedding no conjunto $\Psi$, resultando em uma complexidade de $O(|\Psi|)$. Al√©m disso, √© necess√°rio armazenar os $k$ centroids, o que contribui com uma complexidade de $O(k)$. Portanto, a complexidade total de espa√ßo √© $O(|\Psi| + k)$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos 1000 document embeddings ($|\Psi|=1000$) e usamos k-means com 100 centroids ($k=100$). A complexidade de espa√ßo para o √≠ndice IVF √© $O(1000 + 100) = O(1100)$. Isso significa que o espa√ßo necess√°rio para armazenar o √≠ndice cresce linearmente com o n√∫mero de embeddings e centroids.

No momento da consulta, especificamos pesquisar pelos *document embeddings Nearest Neighbors* (NN) em $p > 0$ parti√ß√µes [^32]. Se $p = k$, a pesquisa √© exaustiva. No entanto, se $p < k$, a pesquisa √© realizada nas parti√ß√µes cujo *centroid* √© mais pr√≥ximo do *query embedding*. Ao fazer isso, a pesquisa n√£o tem garantia de ser exata, mas o tempo de pesquisa pode ser sensivelmente reduzido. Na verdade, um √≠ndice IVF n√£o melhora o consumo de espa√ßo, uma vez que ainda precisa armazenar todos os *document embeddings*, mas pode reduzir o tempo de pesquisa, dependendo do n√∫mero de parti√ß√µes processadas para cada consulta.

> üí° **Exemplo Num√©rico:** Se temos $k = 100$ centroids e definimos $p = 10$, procuramos apenas nas 10 parti√ß√µes mais pr√≥ximas do embedding da query. Isso significa que examinamos aproximadamente 10% dos dados, o que acelera a pesquisa, mas pode reduzir a precis√£o em compara√ß√£o com a pesquisa exaustiva ($p=k$).

Para complementar a discuss√£o sobre a efici√™ncia da pesquisa, podemos analisar o seguinte resultado:

**Teorema 1.1.** Seja $t_q$ o tempo para calcular o *vector quantiser* $q(\psi)$ e $t_d$ o tempo para calcular a dist√¢ncia entre dois vetores. O tempo de pesquisa para encontrar os vizinhos mais pr√≥ximos em $p$ parti√ß√µes √© $O(p \cdot (|L_{i_1}| + ... + |L_{i_p}|) \cdot t_d + k \cdot t_q)$, onde $L_{i_1}, ..., L_{i_p}$ s√£o as *inverted lists* correspondentes √†s $p$ parti√ß√µes selecionadas.

*Proof.* Primeiro, precisamos quantizar o *query embedding*, o que leva tempo $t_q$. Em seguida, selecionamos as $p$ parti√ß√µes cujo centroid √© mais pr√≥ximo do *query embedding*.  Essa sele√ß√£o requer calcular a dist√¢ncia entre o *query embedding* e cada um dos $k$ centroids, o que leva tempo $O(k \cdot t_q)$.  Finalmente, para cada uma das $p$ parti√ß√µes selecionadas, calculamos a dist√¢ncia entre o *query embedding* e cada *document embedding* na parti√ß√£o, resultando em um tempo total de $O(p \cdot (|L_{i_1}| + ... + |L_{i_p}|) \cdot t_d)$. Somando todos esses termos, obtemos o tempo total de pesquisa: $O(p \cdot (|L_{i_1}| + ... + |L_{i_p}|) \cdot t_d + k \cdot t_q)$. $\blacksquare$

> üí° **Exemplo Num√©rico:**  Suponha que $k = 100$, $p = 10$, e cada inverted list selecionada ($L_{i_1}, ..., L_{i_{10}}$) contenha em m√©dia 50 embeddings ($|L_i| = 50$).  Se $t_q = 0.01$ ms e $t_d = 0.001$ ms, o tempo total de pesquisa √©:
>
> $O(10 \cdot (50 \cdot 10) \cdot 0.001 + 100 \cdot 0.01) = O(5 + 1) = O(6)$ ms.
>
> Este exemplo ilustra como o tempo de pesquisa depende do n√∫mero de parti√ß√µes, do tamanho das inverted lists e do custo computacional das opera√ß√µes de quantiza√ß√£o e c√°lculo de dist√¢ncia.

#### Limita√ß√µes do IVF e a Necessidade de Produto de Quantiza√ß√£o
Uma limita√ß√£o principal dos √≠ndices IVF √© que eles podem exigir um grande n√∫mero de *centroids* [Gersho and Gray 1992, citado em ^32]. Para abordar essa limita√ß√£o, a t√©cnica de **product quantization** [J√©gou et al. 2011, citado em ^32] divide cada vetor $\psi \in \Psi$ em $m$ subvetores: $\psi = [\psi_1 | \psi_2 | ... | \psi_m]$. Cada subvetor $\psi_j \in \mathbb{R}^{l/m}$ com $j = 1, ..., m$ √© quantizado independentemente usando seu pr√≥prio *sub-vector quantiser* $q_j$. Cada *vector quantiser* $q_j$ tem seu pr√≥prio *codebook* $M_j = \{\mu_{j,1}, ..., \mu_{j,k}\}$ [^32].

Dados os *codebooks* $M_1, ..., M_m$, um **product quantiser** $p_q : \mathbb{R}^l \rightarrow \mathbb{R}^l$ mapeia um vetor $\psi$ para a concatena√ß√£o dos *centroids* de seus *sub-vector quantisers* [^32]:

$$
p_q(\psi) = [q_1(\psi_1) | q_2(\psi_2) | ... | q_m(\psi_m)] = [\mu_{1,i_1} | \mu_{2,i_2} | ... | \mu_{m,i_m}]
$$

Note que um *product quantiser* pode gerar qualquer uma das $k^m$ combina√ß√µes de *centroids* em $M_1 \times ... \times M_m$ [^32].

**Lema 2.** *O n√∫mero total de centroids gerados por um product quantiser com $m$ sub-vector quantisers, cada um com $k$ centroids, √© $k^m$.*

*Proof.* Cada sub-vector quantiser $q_j$ tem $k$ centroids em seu codebook $M_j$.  Como o product quantiser combina um centroid de cada sub-vector quantiser, o n√∫mero total de combina√ß√µes poss√≠veis √© o produto do n√∫mero de centroids em cada codebook, que √© $k \times k \times ... \times k$ ($m$ vezes), resultando em $k^m$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que dividimos cada vetor em $m = 4$ subvetores e usamos $k = 256$ centroids para cada subvetor. O n√∫mero total de centroids gerados pelo product quantiser √© $256^4 = 4,294,967,296$. Isso demonstra como o product quantization pode gerar um n√∫mero enorme de centroids com um n√∫mero relativamente pequeno de centroids por subvetor.

### Conclus√£o
O uso de *vector quantisation* em conjunto com estruturas de *inverted file* representa uma estrat√©gia eficaz para otimizar a pesquisa de vizinhos mais pr√≥ximos em sistemas de *Neural Information Retrieval*. Ao organizar os *document embeddings* em parti√ß√µes baseadas em *centroids* aprendidos, o √≠ndice IVF permite uma pesquisa mais direcionada e eficiente. Al√©m disso, t√©cnicas como *product quantization* ajudam a mitigar a demanda por um grande n√∫mero de *centroids*, tornando a abordagem escal√°vel para conjuntos de dados extensos. Essas otimiza√ß√µes s√£o cruciais para o desenvolvimento de sistemas de NIR pr√°ticos e eficientes [^34], permitindo a manipula√ß√£o e busca eficientes de informa√ß√µes em grandes cole√ß√µes de texto [^5].

### Refer√™ncias
[^31]: P. Indyk and R. Motwani. 1998. Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality. In Proc. STOC, —Ä. 604‚Äì613.
[^32]: A. Gersho and R. M. Gray. 1992. Vector Quantization and Signal Compression. Kluwer.
[^34]: N. Tonellotto, C. Macdonald, and I. Ounis. 2018. Efficient query processing for scalable web search. Foundations and Trends in Information Retrieval, 12(4‚Äì5): 319‚Äì492.
[^5]: Veja a Introdu√ß√£o do documento.
<!-- END -->