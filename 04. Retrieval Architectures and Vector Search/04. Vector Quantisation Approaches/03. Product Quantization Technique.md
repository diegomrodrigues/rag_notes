## 4.4.3 Product Quantization
### Introdu√ß√£o
Como vimos na se√ß√£o anterior, *Inverted File (IVF)* indexes utilizam *k-means clustering* para particionar o espa√ßo de entrada, onde o n√∫mero de centroides (*centroids*) $k$ pode ser uma limita√ß√£o [^32]. Para mitigar essa limita√ß√£o, uma t√©cnica chamada **product quantization (PQ)** foi introduzida [^32]. Esta se√ß√£o ir√° mergulhar nas profundezas da product quantization, explorando como ela supera as limita√ß√µes dos m√©todos anteriores.

### Conceitos Fundamentais
Product quantization oferece uma abordagem refinada para quantizar vetores de alta dimens√£o. Em vez de quantizar o vetor inteiro diretamente, o PQ decomp√µe o espa√ßo vetorial em subespa√ßos menores e quantiza cada um desses subespa√ßos independentemente [^32].

Formalmente, dado um vetor $\psi \in \Psi$, onde $\Psi$ representa o espa√ßo de entrada, o PQ divide $\psi$ em $m$ subvetores $\psi = [\psi_1 | \psi_2 | \ldots | \psi_m]$, onde cada subvetor $\psi_j \in \mathbb{R}^{l/m}$ com $j = 1, \ldots, m$ [^32]. Cada subvetor $\psi_j$ √© ent√£o quantizado independentemente usando seu pr√≥prio quantizador de subvetor $q_j$. Cada quantizador de subvetor $q_j$ tem seu pr√≥prio *codebook* $M_j = \{\mu_{j,1}, \ldots, \mu_{j,k}\}$ [^32].

Um **codebook** √© um conjunto de vetores representativos (centroides) para cada subespa√ßo. Dado os codebooks $M_1, \ldots, M_m$, um **product quantizer** $pq: \mathbb{R}^l \rightarrow \mathbb{R}^l$ mapeia um vetor $\psi$ para a concatena√ß√£o dos centroides de seus quantizadores de subvetor [^32]:

$$
pq(\psi) = [q_1(\psi_1) | q_2(\psi_2) | \ldots | q_m(\psi_m)] = [\mu_{1,i_1} | \mu_{2,i_2} | \ldots | \mu_{m,i_m}]
$$

onde $i_j$ √© o √≠ndice do centroide mais pr√≥ximo no codebook $M_j$ para o subvetor $\psi_j$ [^32].

> üí° **Exemplo Num√©rico:** Considere um vetor $\psi \in \mathbb{R}^8$. Dividimos este vetor em $m=2$ subvetores, cada um de dimens√£o $l/m = 8/2 = 4$. Assim, $\psi = [\psi_1 | \psi_2]$, onde $\psi_1, \psi_2 \in \mathbb{R}^4$. Para cada subvetor, usamos um codebook de tamanho $k=3$. Portanto, $M_1 = \{\mu_{1,1}, \mu_{1,2}, \mu_{1,3}\}$ e $M_2 = \{\mu_{2,1}, \mu_{2,2}, \mu_{2,3}\}$.
>
> Suponha que, ap√≥s encontrar o centroide mais pr√≥ximo para cada subvetor, temos $i_1 = 2$ e $i_2 = 1$. Ent√£o, a representa√ß√£o quantizada de $\psi$ √© $pq(\psi) = [\mu_{1,2} | \mu_{2,1}]$. Isto significa que, em vez de armazenar o vetor original $\psi$, armazenamos os √≠ndices $2$ e $1$, o que economiza espa√ßo.
>
> | Subvetor | Codebook                                        | √çndice Escolhido |
> | -------- | ----------------------------------------------- | --------------- |
> | $\psi_1$ | $M_1 = \{\mu_{1,1}, \mu_{1,2}, \mu_{1,3}\}$ | $i_1 = 2$       |
> | $\psi_2$ | $M_2 = \{\mu_{2,1}, \mu_{2,2}, \mu_{2,3}\}$ | $i_2 = 1$       |

Note que um product quantizer pode gerar qualquer uma das $k^m$ combina√ß√µes de centroides em $M_1 \times \ldots \times M_m$ [^32].

> üí° **Exemplo Num√©rico:** No exemplo anterior, com $m=2$ e $k=3$, o product quantizer pode gerar $3^2 = 9$ combina√ß√µes diferentes de centroides. Isso significa que o espa√ßo original √© aproximado por 9 vetores representativos.

Um √≠ndice PQ armazena, para cada *embedding* $\psi \in \Psi$, sua codifica√ß√£o $i_1, \ldots, i_m$, que requer $m \log_2 k$ bits de armazenamento [^32]. Durante o tempo de processamento da *query*, os *embeddings* do documento s√£o processados exaustivamente [^32]. No entanto, a computa√ß√£o da dist√¢ncia entre um *embedding* de *query* $\phi$ e um *embedding* de documento $\psi$ √© realizada usando a quantiza√ß√£o de produto do *embedding* pq(x) do documento [^32]:

$$
||\psi - \phi||^2 \approx ||pq(\psi) - \phi||^2 = \sum_{j=1}^m ||q_j(\psi_j) - \phi_j||^2
$$

> üí° **Exemplo Num√©rico:** Suponha que temos um vetor de *query* $\phi \in \mathbb{R}^8$ e queremos calcular a dist√¢ncia entre $\phi$ e o vetor quantizado $pq(\psi)$ do exemplo anterior. Dividimos $\phi$ em dois subvetores: $\phi = [\phi_1 | \phi_2]$, onde $\phi_1, \phi_2 \in \mathbb{R}^4$. A dist√¢ncia aproximada √© ent√£o:
>
> $||\psi - \phi||^2 = ||\mu_{1,2} - \phi_1||^2 + ||\mu_{2,1} - \phi_2||^2$.
>
>  Calculamos a dist√¢ncia entre o subvetor da *query* e o centroide correspondente para cada subespa√ßo e somamos as dist√¢ncias.  Se $||\mu_{1,2} - \phi_1||^2 = 2.5$ e $||\mu_{2,1} - \phi_2||^2 = 1.8$, ent√£o $||\psi - \phi||^2 = 2.5 + 1.8 = 4.3$.

Para implementar esta computa√ß√£o, $m$ tabelas de consulta s√£o computadas, uma por quantizador de subvetor: a j-√©sima tabela √© composta pelas dist√¢ncias quadradas entre o j-√©simo subvetor de $\phi$ e os centroides de $M_j$ [^33]. Essas tabelas podem ser usadas para calcular rapidamente as somas na equa√ß√£o (23) para cada *embedding* do documento [^33].

> üí° **Exemplo Num√©rico:** Construindo as tabelas de consulta: Para cada subvetor da *query* ($\phi_1$ e $\phi_2$), pr√©-calculamos as dist√¢ncias para todos os centroides em seus respectivos codebooks.
>
> | Centroide  | Dist√¢ncia Quadrada para $\phi_1$ ($||\mu_{j,i} - \phi_1||^2$) | Centroide  | Dist√¢ncia Quadrada para $\phi_2$ ($||\mu_{j,i} - \phi_2||^2$) |
> | ---------- | ------------------------------------------------------------- | ---------- | ------------------------------------------------------------- |
> | $\mu_{1,1}$ | 3.2                                                           | $\mu_{2,1}$ | 1.8                                                           |
> | $\mu_{1,2}$ | 2.5                                                           | $\mu_{2,2}$ | 2.7                                                           |
> | $\mu_{1,3}$ | 4.1                                                           | $\mu_{2,3}$ | 3.5                                                           |
>
> Durante a busca, ao encontrar o par de √≠ndices $(i_1, i_2) = (2, 1)$ para o vetor de documento quantizado, simplesmente consultamos as tabelas para obter as dist√¢ncias pr√©-calculadas (2.5 e 1.8) e som√°-las para obter a dist√¢ncia aproximada entre o documento e a *query*.

**Observa√ß√£o:** A escolha do n√∫mero de subvetores $m$ e o tamanho do codebook $k$ para cada subvetor s√£o par√¢metros cruciais que afetam o desempenho do PQ. Aumentar $m$ geralmente leva a uma melhor precis√£o, mas tamb√©m aumenta a complexidade computacional. Similarmente, aumentar $k$ melhora a precis√£o √† custa de maior espa√ßo de armazenamento para os codebooks.

**Teorema 1** (Decomposi√ß√£o da Dist√¢ncia Euclidiana): A dist√¢ncia Euclidiana ao quadrado entre dois vetores pode ser decomposta como a soma das dist√¢ncias Euclidianas ao quadrado entre seus respectivos subvetores, desde que os subvetores sejam uma parti√ß√£o ortogonal do espa√ßo vetorial original.

*Prova*: Seja $\psi, \phi \in \mathbb{R}^l$ dois vetores divididos em $m$ subvetores: $\psi = [\psi_1 | \psi_2 | \ldots | \psi_m]$ e $\phi = [\phi_1 | \phi_2 | \ldots | \phi_m]$. Ent√£o,

$||\psi - \phi||^2 = \sum_{i=1}^l (\psi_i - \phi_i)^2 = \sum_{j=1}^m \sum_{i \in I_j} (\psi_i - \phi_i)^2 = \sum_{j=1}^m ||\psi_j - \phi_j||^2$, onde $I_j$ √© o conjunto de √≠ndices correspondentes ao subvetor $\psi_j$.

Este teorema justifica a aproxima√ß√£o utilizada na equa√ß√£o (23) e demonstra a import√¢ncia da decomposi√ß√£o do vetor original em subvetores.

### Vantagens do Product Quantization
1.  **Complexidade Sublinear:** Permite a pesquisa de vizinhos mais pr√≥ximos aproximados (ANN) com complexidade de tempo sublinear, equilibrando a precis√£o e a velocidade de pesquisa [^30, 31].
2.  **Redu√ß√£o de Requisitos de Armazenamento:** Ao quantizar vetores, reduz o espa√ßo de armazenamento necess√°rio para manter os *embeddings* de alta dimens√£o, tornando-o escal√°vel para conjuntos de dados extensos [^32].
3.  **Melhora da Efici√™ncia da Busca:** A *product quantization* pode acelerar significativamente a busca por vizinhos mais pr√≥ximos, pr√©-calculando as dist√¢ncias entre os subvetores quantizados, reduzindo o n√∫mero de c√°lculos de dist√¢ncia necess√°rios no momento da busca [^33].

**Teorema 1.1** (Trade-off entre Precis√£o e Armazenamento): Para um dado espa√ßo vetorial e um erro de quantiza√ß√£o m√°ximo aceit√°vel $\epsilon$, existe um compromisso entre o n√∫mero de subvetores $m$ e o tamanho do codebook $k$ tal que o espa√ßo de armazenamento total √© minimizado enquanto o erro de quantiza√ß√£o permanece abaixo de $\epsilon$.

*Prova (Esbo√ßo)*: O erro de quantiza√ß√£o diminui √† medida que $k$ aumenta, para um $m$ fixo. Por outro lado, para um $k$ fixo, diminuir $m$ pode aumentar o erro de quantiza√ß√£o. Atingir o $\epsilon$ requer ajustar $m$ e $k$. O espa√ßo de armazenamento total √© proporcional a $m \times k$. A otimiza√ß√£o desse produto, sujeito √† restri√ß√£o de erro, fornecer√° o compromisso ideal. Encontrar a solu√ß√£o anal√≠tica pode ser complexo, mas algoritmos de busca podem ser aplicados para encontrar valores aproximados ideais de $m$ e $k$.

### Conclus√£o
Product Quantization oferece uma estrat√©gia eficaz para lidar com conjuntos de dados de alta dimens√£o em problemas de *nearest neighbor search*. Ao decompor vetores em subvetores e quantiz√°-los independentemente, o PQ atinge uma excelente compensa√ß√£o entre precis√£o e efici√™ncia computacional. Ele supera as limita√ß√µes dos m√©todos baseados em *k-means* clustering, tornando-se uma ferramenta valiosa na √°rea de *information retrieval* [^32, 33].

### Refer√™ncias
[^32]: H. J√©gou, M. Douze, and C. Schmid. 2011. Product Quantization for Nearest Neighbor Search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(1): 117‚Äì128.
[^33]: A. Mallia, J. Mackenzie, T. Suel, and N. Tonellotto. 2022. Faster Learned Sparse Retrieval with Guided Traversal. In Proc. SIGIR, p. 5.
<!-- END -->