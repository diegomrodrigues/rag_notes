## 4.4.1 Aprofundamento no √çndice Product Quantization (PQ) e IVFPQ

Como vimos na Se√ß√£o 4.4 [^32], a **quantiza√ß√£o vetorial** oferece uma alternativa √† divis√£o aleat√≥ria do espa√ßo de entrada, como no **Locality Sensitive Hashing (LSH)**, permitindo que a distribui√ß√£o dos dados guie a parti√ß√£o do espa√ßo. O **Product Quantization (PQ)** √© uma t√©cnica avan√ßada de quantiza√ß√£o vetorial que decomp√µe o espa√ßo vetorial em subespa√ßos menores para reduzir a complexidade computacional e de armazenamento [^32]. Esta se√ß√£o explora os detalhes da constru√ß√£o e utiliza√ß√£o de √≠ndices PQ e suas varia√ß√µes, em particular o **Inverted File with Product Quantization (IVFPQ)**, no contexto do *Neural Information Retrieval*.

### √çndice Product Quantization (PQ)

No √≠ndice PQ, para cada embedding $\psi \in \Psi$, armazena-se a codifica√ß√£o $(i_1, ..., i_m)$, onde cada $i_j$ representa o √≠ndice do centr√≥ide do $j$-√©simo subvetor de $\psi$. A exig√™ncia de armazenamento √© de $m \log k$ bits [^32], onde $m$ √© o n√∫mero de subvetores e $k$ √© o n√∫mero de centr√≥ides em cada subespa√ßo.

> üí° **Exemplo Num√©rico:** Suponha que temos um embedding $\psi$ e dividimos ele em $m=4$ subvetores. Em cada subvetor, usamos $k=256$ centr√≥ides (o que requer $\log_2(256) = 8$ bits para representar cada √≠ndice). O requisito total de armazenamento para esse embedding seria $4 \times 8 = 32$ bits.
>
> üí° **Exemplo Pr√°tico:** Se tivermos 1 milh√£o de embeddings, o espa√ßo total necess√°rio para armazenar os c√≥digos PQ seria $1,000,000 \times 32 \text{ bits} = 32,000,000 \text{ bits} = 4 \text{ MB}$.  Isso demonstra a efici√™ncia de armazenamento do PQ, especialmente para vetores de alta dimens√£o.

A dist√¢ncia entre um embedding de consulta $\phi$ e um embedding de documento $\psi$ √© aproximada usando a quantiza√ß√£o de produto:

$$
||\psi - \phi||^2 \approx ||pq(\psi) - \phi||^2 = \sum_{j=1}^{m} ||q_j(\psi_j) - \phi_j||^2
$$

onde $pq(\psi)$ √© a representa√ß√£o quantizada de produto de $\psi$, $q_j(\psi_j)$ √© o centr√≥ide atribu√≠do ao $j$-√©simo subvetor de $\psi$, e $\phi_j$ √© o $j$-√©simo subvetor de $\phi$ [^32]. Para implementar esta computa√ß√£o de dist√¢ncia, s√£o computadas $m$ tabelas de lookup, uma por subvetor quantizador. A $j$-√©sima tabela √© composta pelas dist√¢ncias quadradas entre o $j$-√©simo subvetor de $\phi$ e os centr√≥ides de $M_j$. Essas tabelas s√£o ent√£o usadas para rapidamente computar as somas na Equa√ß√£o (23) para cada embedding de documento [^33].

> üí° **Exemplo Num√©rico:** Suponha que temos um embedding de consulta $\phi$ e dividimos ele em $m=2$ subvetores, $\phi_1$ e $\phi_2$. Para o primeiro subvetor, calculamos as dist√¢ncias quadradas at√© os $k=4$ centr√≥ides mais pr√≥ximos: $||q_1(\psi_1) - \phi_1||^2 = [0.1, 0.2, 0.3, 0.4]$. Similarmente, para o segundo subvetor, calculamos: $||q_2(\psi_2) - \phi_2||^2 = [0.5, 0.6, 0.7, 0.8]$. Para encontrar a dist√¢ncia aproximada entre $\phi$ e $\psi$, somamos os menores valores em cada lista. Neste caso, $0.1 + 0.5 = 0.6$. Este valor aproximado √© ent√£o usado para ranquear os documentos.

Para melhor elucidar o processo de quantiza√ß√£o, podemos expressar o res√≠duo da quantiza√ß√£o de produto da seguinte forma:

**Proposi√ß√£o 1** O res√≠duo da quantiza√ß√£o de produto √© dado por:

$$
r(\psi, \phi) = ||\psi - \phi||^2 - ||pq(\psi) - \phi||^2 =  ||\psi - \phi||^2 - \sum_{j=1}^{m} ||q_j(\psi_j) - \phi_j||^2
$$

*Prova:* A proposi√ß√£o segue diretamente da defini√ß√£o da aproxima√ß√£o da dist√¢ncia no Product Quantization.  O termo $r(\psi, \phi)$ representa a diferen√ßa entre a dist√¢ncia real e a dist√¢ncia aproximada ap√≥s a quantiza√ß√£o. $\blacksquare$

Al√©m disso, a escolha do n√∫mero de subvetores $m$ impacta diretamente na precis√£o da aproxima√ß√£o.  Um valor maior de $m$ implica subespa√ßos menores, potencialmente permitindo uma melhor representa√ß√£o dos vetores originais, mas aumentando a complexidade computacional das tabelas de *lookup*.

> üí° **Exemplo Comparativo:**
>
> | N√∫mero de Subvetores (m) | Precis√£o da Aproxima√ß√£o | Tamanho das Tabelas de Lookup | Custo Computacional |
> | ------------------------- | ----------------------- | ---------------------------- | ------------------- |
> | 2                         | Baixa                   | Pequeno                      | Baixo               |
> | 8                         | M√©dia                   | M√©dio                        | M√©dio               |
> | 32                        | Alta                    | Grande                       | Alto                |
>
> Um valor maior de $m$ leva a uma aproxima√ß√£o mais precisa da dist√¢ncia real, mas tamb√©m aumenta o custo computacional devido ao maior tamanho das tabelas de *lookup*.

### √çndice Inverted File com Product Quantization (IVFPQ)

Como mencionado na Se√ß√£o 4.4 [^32], a busca *Approximate Nearest Neighbor (ANN)* em um √≠ndice PQ ainda pode ser exaustiva. Para evitar isso, o √≠ndice **IVFPQ** combina arquivos invertidos com quantiza√ß√£o de produto. Uma quantiza√ß√£o inicial particiona o conjunto de dados de entrada em listas invertidas, para um acesso r√°pido a pequenas por√ß√µes dos dados de entrada. Em uma dada lista invertida, a diferen√ßa entre cada dado de entrada e o centr√≥ide da lista (isto √©, o res√≠duo de entrada) √© codificada com um quantizador de produto [^33]. Assim fazendo, a busca ANN exaustiva pode ser realizada em um n√∫mero limitado das parti√ß√µes computadas pelo quantizador grosseiro.

**Em resumo, o processo IVFPQ consiste em duas etapas principais:**

1.  **Quantiza√ß√£o Grosseira:** O espa√ßo vetorial √© particionado em $k'$ clusters usando um algoritmo de clustering como o *k-means*. Cada cluster √© representado por seu centr√≥ide, e os vetores s√£o atribu√≠dos ao cluster mais pr√≥ximo.

2.  **Quantiza√ß√£o de Produto no Res√≠duo:** Para cada cluster, calcula-se o vetor res√≠duo subtraindo o centr√≥ide do cluster de cada vetor no cluster. Em seguida, aplica-se a quantiza√ß√£o de produto aos vetores res√≠duo, dividindo cada vetor em $m$ subvetores e quantizando cada subvetor separadamente [^32].

O IVFPQ equilibra a necessidade de precis√£o com a efici√™ncia computacional, permitindo a busca em grandes conjuntos de dados com um custo aceit√°vel de aproxima√ß√£o. A escolha do n√∫mero de clusters $k'$ e o n√∫mero de subvetores $m$ s√£o par√¢metros cruciais que afetam o desempenho do √≠ndice. O IVFPQ apresenta um *trade-off* entre a precis√£o e o *recall* e o custo computacional [^32].

> üí° **Exemplo Num√©rico:** Imagine que temos um conjunto de dados de 10.000 vetores. Na etapa de quantiza√ß√£o grosseira, escolhemos $k'=100$ clusters. Isso significa que cada cluster conter√°, em m√©dia, 100 vetores. Ao inv√©s de procurar nos 10.000 vetores, restringimos a busca aos vetores dentro de um ou alguns clusters mais pr√≥ximos da query, digamos 2 clusters, reduzindo a busca para aproximadamente 200 vetores. A quantiza√ß√£o de produto √© ent√£o aplicada a esses 200 vetores para refinar ainda mais a busca.

Para formalizar a etapa de quantiza√ß√£o grosseira, considere a fun√ß√£o $c(\psi)$ que mapeia um vetor $\psi$ para o √≠ndice do cluster mais pr√≥ximo.  Assim, a quantiza√ß√£o grosseira pode ser expressa como $\hat{\psi} = C_{c(\psi)}$, onde $C_{c(\psi)}$ √© o centr√≥ide do cluster $c(\psi)$.

**Proposi√ß√£o 2** O res√≠duo da quantiza√ß√£o grosseira √© definido como:

$$
e(\psi) = \psi - \hat{\psi} = \psi - C_{c(\psi)}
$$

Este res√≠duo $e(\psi)$ √© ent√£o submetido √† quantiza√ß√£o de produto na segunda etapa do IVFPQ.

*Prova:* A proposi√ß√£o segue diretamente da defini√ß√£o de res√≠duo, que √© a diferen√ßa entre o vetor original e sua representa√ß√£o quantizada pelo quantizador grosseiro. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que um vetor $\psi = [1.2, 3.4, 5.6, 7.8]$ e o centr√≥ide do cluster ao qual ele pertence √© $C_{c(\psi)} = [1.0, 3.0, 5.0, 7.0]$. O res√≠duo da quantiza√ß√£o grosseira seria $e(\psi) = [1.2-1.0, 3.4-3.0, 5.6-5.0, 7.8-7.0] = [0.2, 0.4, 0.6, 0.8]$. Este vetor res√≠duo √© ent√£o usado na etapa de quantiza√ß√£o de produto.

### Vantagens do IVFPQ
*   **Redu√ß√£o do Custo Computacional:** Ao restringir a busca a um subconjunto dos clusters, o IVFPQ diminui significativamente o n√∫mero de c√°lculos de dist√¢ncia necess√°rios, comparado a uma busca exaustiva [^32].
*   **Escalabilidade:** A capacidade de particionar o espa√ßo de busca torna o IVFPQ adequado para indexar grandes conjuntos de dados, facilitando a busca eficiente em larga escala.
*   **Compensa√ß√£o entre Precis√£o e Efici√™ncia:** A escolha do n√∫mero de clusters e o n√∫mero de bits usados na quantiza√ß√£o de produto permite ajustar o equil√≠brio entre a precis√£o dos resultados da busca e a velocidade da busca [^33].

### Desafios
*   **Ajuste de Par√¢metros:** Selecionar os valores √≥timos para o n√∫mero de clusters na quantiza√ß√£o inicial e o n√∫mero de subvetores na quantiza√ß√£o de produto pode ser desafiador e requer experimenta√ß√£o.
*   **Custo de Armazenamento:** Embora a quantiza√ß√£o de produto reduza o custo de armazenamento comparado a armazenar os vetores originais, o √≠ndice ainda requer uma quantidade consider√°vel de espa√ßo, especialmente para alta dimensionalidade [^32].
*   **Sensibilidade √† Distribui√ß√£o de Dados:** O desempenho do IVFPQ pode ser afetado pela distribui√ß√£o dos dados, e o desempenho ideal √© alcan√ßado quando os dados s√£o distribu√≠dos uniformemente dentro de cada cluster.

Adicionalmente, a escolha do algoritmo de clustering na etapa de quantiza√ß√£o grosseira pode impactar significativamente o desempenho do IVFPQ.  Enquanto o *k-means* √© frequentemente utilizado devido √† sua simplicidade e efici√™ncia, outros algoritmos como o *agglomerative clustering* ou *spectral clustering* podem ser mais adequados dependendo da estrutura dos dados.

A arquitetura de sistemas de recupera√ß√£o de informa√ß√£o que empregam IVFPQ pode ser melhor compreendida visualmente. A figura a seguir ilustra esse processo.

![Ranking pipeline architecture for multiple representation systems using learned embeddings and ANN search.](./../images/image3.png)

**Proposi√ß√£o 3** A complexidade da busca em IVFPQ √© dada por $O(n_{s} \cdot m \cdot k)$, onde $n_s$ √© o n√∫mero de vetores em um subconjunto selecionado de clusters, $m$ √© o n√∫mero de subvetores na quantiza√ß√£o de produto, e $k$ √© o n√∫mero de centr√≥ides em cada subespa√ßo.

*Prova:* A complexidade da busca depende do n√∫mero de vetores ($n_s$) que precisam ser comparados na etapa de quantiza√ß√£o de produto. Para cada vetor, s√£o realizadas $m$ compara√ß√µes (uma por subvetor), e cada compara√ß√£o envolve uma busca em uma tabela de lookup de tamanho $k$. Portanto, a complexidade total √© $O(n_{s} \cdot m \cdot k)$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se $n_s = 200$, $m = 4$, e $k = 256$, a complexidade da busca seria $O(200 \cdot 4 \cdot 256) = O(204,800)$. Isso mostra como os par√¢metros afetam diretamente a complexidade computacional. Reduzir $n_s$ selecionando menos clusters na etapa de quantiza√ß√£o grosseira, ou diminuir $m$ e $k$, pode diminuir significativamente o tempo de busca, mas potencialmente √† custa da precis√£o.

$\blacksquare$
<!-- END -->