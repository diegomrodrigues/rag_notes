## Graph-based Approaches for Efficient Similarity Search in Neural Information Retrieval

### Introdu√ß√£o

No campo da Neural Information Retrieval (NIR), a busca eficiente de documentos relevantes √© um desafio cr√≠tico, especialmente em grandes cole√ß√µes de dados. Como discutido anteriormente [^28], uma etapa fundamental em muitos sistemas de NIR √© a busca de vizinhos mais pr√≥ximos (Nearest Neighbors - NN) nos espa√ßos de embeddings densos. Este cap√≠tulo se aprofunda nas abordagens baseadas em grafos, que oferecem solu√ß√µes promissoras para a busca aproximada de NN (Approximate Nearest Neighbor - ANN). As abordagens baseadas em grafos exploram a estrutura inerente dos dados para acelerar a busca, oferecendo um bom compromisso entre precis√£o e velocidade. Especificamente, esta se√ß√£o aborda a estrutura de dados k-Nearest Neighbors (kNN) graph e seus m√©todos de constru√ß√£o e busca.

### k-NN Graphs para Busca de Similaridade

A busca de documentos relevantes em NIR muitas vezes se resume a encontrar os vetores de embeddings mais pr√≥ximos a um vetor de *query* em um espa√ßo de alta dimens√£o. Uma forma eficiente de organizar os dados para essa tarefa √© utilizando um grafo kNN [^33].

**Conceito Fundamental: k-NN Graph**

Um grafo kNN  $G = (V, E)$ √© uma estrutura de dados onde:

*   $V$ √© o conjunto de v√©rtices (n√≥s), representando cada ponto de dados (e.g., documento) no *dataset* $\Psi$. Cada n√≥ $v \in V$ corresponde a um vetor de entrada $\psi \in \Psi$.
*   $E$ √© o conjunto de arestas, onde uma aresta existe entre dois n√≥s se um for um dos *k* vizinhos mais pr√≥ximos do outro. A proximidade √© definida por uma m√©trica de dist√¢ncia, como a dist√¢ncia euclidiana ou similaridade de cossenos.

De acordo com o contexto [^33], armazenar as dist√¢ncias entre vetores em um *dataset* pode ser eficientemente realizado em uma estrutura de dados baseada em grafos, um *kNN graph*.

#### Constru√ß√£o do Grafo

A constru√ß√£o de um grafo kNN envolve as seguintes etapas:

1.  **C√°lculo da Similaridade:** Calcular a similaridade (ou dist√¢ncia) entre todos os pares de pontos de dados no *dataset*. Essa √© a etapa mais custosa, com complexidade $O(n^2)$, onde *n* √© o n√∫mero de pontos de dados.

2.  **Sele√ß√£o de Vizinhos:** Para cada ponto de dados, selecionar seus *k* vizinhos mais pr√≥ximos com base nas m√©tricas de similaridade calculadas.

3.  **Constru√ß√£o das Arestas:** Criar arestas no grafo conectando cada n√≥ aos seus *k* vizinhos mais pr√≥ximos.

> üí° **Exemplo Num√©rico:**
> Suponha que temos um dataset com 5 documentos, representados pelos seguintes vetores de embeddings:
>
> *   $D_1 = [1, 0, 0]$
> *   $D_2 = [0, 1, 0]$
> *   $D_3 = [0, 0, 1]$
> *   $D_4 = [1, 1, 0]$
> *   $D_5 = [0, 1, 1]$
>
> Queremos construir um grafo kNN com $k=2$ usando a dist√¢ncia euclidiana.
>
> $\text{Step 1: Calculate Euclidean distances between all pairs of documents.}$
>
> $\text{Dist}(D_1, D_2) = \sqrt{(1-0)^2 + (0-1)^2 + (0-0)^2} = \sqrt{2} \approx 1.41$
> $\text{Dist}(D_1, D_3) = \sqrt{(1-0)^2 + (0-0)^2 + (0-1)^2} = \sqrt{2} \approx 1.41$
> $\text{Dist}(D_1, D_4) = \sqrt{(1-1)^2 + (0-1)^2 + (0-0)^2} = 1$
> $\text{Dist}(D_1, D_5) = \sqrt{(1-0)^2 + (0-1)^2 + (0-1)^2} = \sqrt{3} \approx 1.73$
> $\text{Dist}(D_2, D_3) = \sqrt{(0-0)^2 + (1-0)^2 + (0-1)^2} = \sqrt{2} \approx 1.41$
> $\text{Dist}(D_2, D_4) = \sqrt{(0-1)^2 + (1-1)^2 + (0-0)^2} = 1$
> $\text{Dist}(D_2, D_5) = \sqrt{(0-0)^2 + (1-1)^2 + (0-1)^2} = 1$
> $\text{Dist}(D_3, D_4) = \sqrt{(0-1)^2 + (0-1)^2 + (1-0)^2} = \sqrt{3} \approx 1.73$
> $\text{Dist}(D_3, D_5) = \sqrt{(0-0)^2 + (0-1)^2 + (1-1)^2} = 1$
> $\text{Dist}(D_4, D_5) = \sqrt{(1-0)^2 + (1-1)^2 + (0-1)^2} = \sqrt{2} \approx 1.41$
>
> $\text{Step 2: Select the 2 nearest neighbors for each document.}$
>
> *   $D_1$'s nearest neighbors: $D_4$ and $D_2$ (or $D_3$ - tie)
> *   $D_2$'s nearest neighbors: $D_4$ and $D_5$
> *   $D_3$'s nearest neighbors: $D_5$ and $D_1$ (or $D_2$ - tie)
> *   $D_4$'s nearest neighbors: $D_1$ and $D_2$
> *   $D_5$'s nearest neighbors: $D_2$ and $D_3$
>
> $\text{Step 3: Build the kNN graph.}$
>
> The graph would have nodes $D_1, D_2, D_3, D_4, D_5$, and edges connecting each node to its 2 nearest neighbors. For example, $D_1$ would have edges to $D_4$ and $D_2$.

Devido √† complexidade computacional de $O(n^2)$ do c√°lculo exato do grafo kNN, v√°rias t√©cnicas de aproxima√ß√£o foram desenvolvidas [^33].  Uma t√©cnica comum para aproximar a constru√ß√£o do grafo kNN √© usar estruturas de dados como √°rvores KD-Tree ou Ball-Tree para acelerar a busca pelos vizinhos mais pr√≥ximos.

#### Busca Aproximada no Grafo kNN

A busca pelo vizinho mais pr√≥ximo aproximado (ANN) em um grafo kNN envolve a explora√ß√£o do grafo para encontrar os n√≥s mais pr√≥ximos ao vetor de *query*. A busca √© realizada usando um algoritmo de busca heur√≠stica gulosa (*greedy heuristic search*) [^33]:

1.  **Ponto de Entrada:** Come√ßar a busca a partir de um ou mais pontos de entrada no grafo. A escolha do ponto de entrada pode afetar a qualidade da busca.

2.  **Explora√ß√£o Gulosa:** Em cada itera√ß√£o, visitar um n√≥ e examinar seus vizinhos. Selecionar o vizinho que √© mais pr√≥ximo ao vetor de *query* e mover para aquele n√≥.

3.  **Termina√ß√£o:** A busca termina quando n√£o h√° melhoria na dist√¢ncia at√© o vetor de *query*, ou seja, quando o n√≥ atual √© mais pr√≥ximo do que qualquer um de seus vizinhos n√£o visitados.

**Desafios da Busca em Grafos kNN:**

*   **√ìtimos Locais:** A busca gulosa pode ficar presa em √≥timos locais, onde o algoritmo encontra um n√≥ pr√≥ximo ao vetor de *query*, mas n√£o o vizinho mais pr√≥ximo verdadeiro.

*   **Longos Caminhos:** Em grafos grandes, o caminho entre dois n√≥s pode ser longo, tornando a busca ineficiente.

Para quantificar o desempenho da busca em grafos kNN, podemos definir algumas m√©tricas importantes.

**Defini√ß√£o:** *Recall@R* √© a propor√ß√£o de *queries* para as quais pelo menos um dos *R* resultados retornados est√° entre os vizinhos mais pr√≥ximos verdadeiros.

**Defini√ß√£o:** *Precis√£o@R* √© a propor√ß√£o de vizinhos mais pr√≥ximos verdadeiros entre os *R* resultados retornados.

> üí° **Exemplo Num√©rico:**
>
> Suponha que executamos 100 queries em um grafo kNN. Para cada query, recuperamos os top-5 documentos (R=5).
>
> *   Em 80 das 100 queries, pelo menos um dos 5 documentos recuperados estava entre os vizinhos mais pr√≥ximos verdadeiros.
> *   No total, recuperamos 500 documentos (100 queries * 5 documentos/query). Desses 500 documentos, 300 eram vizinhos mais pr√≥ximos verdadeiros.
>
> $\text{Recall@5} = \frac{80}{100} = 0.8$
> $\text{Precis√£o@5} = \frac{300}{500} = 0.6$
>
> Isso significa que, em 80% das queries, o grafo kNN conseguiu encontrar pelo menos um vizinho pr√≥ximo verdadeiro nos top-5 resultados. No entanto, apenas 60% dos documentos recuperados eram realmente vizinhos pr√≥ximos verdadeiros.

### Navigable Small World (NSW) Graphs

Para mitigar as limita√ß√µes dos grafos kNN b√°sicos, foi proposto o grafo Navigable Small World (NSW) [^33]. Os grafos NSW introduzem conex√µes de longo alcance para reduzir o n√∫mero de passos necess√°rios para navegar pelo grafo.

**Caracter√≠sticas dos Grafos NSW:**

*   **Conex√µes de Curto e Longo Alcance:** Al√©m das conex√µes de vizinhos mais pr√≥ximos, grafos NSW incluem arestas conectando n√≥s aleatoriamente selecionados, permitindo que a busca "pule" atrav√©s do grafo.

*   **Hierarquia:** Grafos NSW podem ser organizados hierarquicamente, com camadas de grafos onde cada camada sucessiva tem menos n√≥s e conex√µes de longo alcance mais longas.

**Algoritmo de Busca em Grafos NSW:**

1.  **Ponto de Entrada na Camada Superior:** Come√ßar a busca na camada superior do grafo NSW.

2.  **Busca Gulosa em Cada Camada:** Em cada camada, realizar uma busca gulosa para encontrar o n√≥ mais pr√≥ximo ao vetor de *query*.

3.  **Transi√ß√£o para a Pr√≥xima Camada:** Descer para a pr√≥xima camada, come√ßando a busca a partir do n√≥ encontrado na camada anterior.

4.  **Termina√ß√£o:** Repetir os passos 2 e 3 at√© atingir a camada mais baixa, onde a busca final √© realizada.

Um aspecto crucial na constru√ß√£o de grafos NSW √© a sele√ß√£o adequada dos n√≥s para as conex√µes de longo alcance. Uma estrat√©gia comum √© selecionar n√≥s aleatoriamente, mas outras abordagens podem levar a melhores resultados.

**Proposi√ß√£o 1:** A sele√ß√£o de n√≥s para conex√µes de longo alcance baseada na distribui√ß√£o da densidade dos dados pode melhorar a navegabilidade do grafo NSW.

*Estrat√©gia de Prova:* N√≥s com baixa densidade de vizinhos podem atuar como "pontes" entre diferentes regi√µes do espa√ßo de dados. Conectar n√≥s de alta densidade a n√≥s de baixa densidade pode facilitar a fuga de √≥timos locais durante a busca.

### Hierarchical Navigable Small World (HNSW)

O Hierarchical Navigable Small World (HNSW) [^33] √© uma extens√£o do NSW que organiza os dados em m√∫ltiplas camadas hier√°rquicas, melhorando ainda mais a efici√™ncia da busca.

**Estrutura do HNSW:**

*   **Camadas Hier√°rquicas:** Os dados s√£o organizados em m√∫ltiplas camadas, onde a camada inferior cont√©m todos os pontos de dados e as camadas superiores cont√™m um subconjunto dos pontos de dados.

*   **Conex√µes Exponenciais:** O n√∫mero de n√≥s nas camadas diminui exponencialmente √† medida que se sobe na hierarquia.

*   **Busca em M√∫ltiplas Camadas:** A busca come√ßa na camada superior e desce pelas camadas, refinando a busca em cada n√≠vel [^33].



![Ranking pipeline architecture for multiple representation systems using learned embeddings and ANN search.](./../images/image3.png)

**Vantagens do HNSW:**

*   **Alta Precis√£o e Velocidade:** O HNSW oferece um excelente compromisso entre precis√£o e velocidade para a busca de ANN [^33].
*   **Escalabilidade:** O HNSW pode ser dimensionado para grandes *datasets* mantendo um bom desempenho.

A constru√ß√£o do HNSW envolve um processo de inser√ß√£o hier√°rquica. Durante a inser√ß√£o, um novo ponto √© inserido em v√°rias camadas, e seus vizinhos mais pr√≥ximos s√£o determinados em cada camada.

**Lema 1:** A escolha do n√∫mero m√°ximo de conex√µes (*maxconn*) por n√≥ em cada camada do HNSW afeta o desempenho da busca e o custo de constru√ß√£o.

*Estrat√©gia de Prova:* Um *maxconn* maior aumenta a conectividade do grafo, potencialmente melhorando a precis√£o da busca, mas tamb√©m aumentando o tempo de constru√ß√£o e o uso de mem√≥ria. Um *maxconn* menor reduz o custo de constru√ß√£o, mas pode diminuir a precis√£o da busca e aumentar o tempo de busca.

> üí° **Exemplo Num√©rico:**
>
> Consideremos um dataset com 1 milh√£o de documentos. Comparamos o desempenho do HNSW com diferentes valores de *maxconn* (16 e 32) para a camada 0 (a camada mais baixa) e medimos a precis√£o@10 e o tempo de busca.
>
> | maxconn (Camada 0) | Precis√£o@10 | Tempo de Busca (ms) | Tempo de Constru√ß√£o (min) | Uso de Mem√≥ria (GB) |
> |----------------------|-------------|----------------------|--------------------------|----------------------|
> | 16                   | 0.85        | 5                    | 20                       | 8                    |
> | 32                   | 0.92        | 7                    | 35                       | 12                   |
>
> Podemos observar que aumentar o *maxconn* de 16 para 32 resulta em uma melhoria na precis√£o@10 (de 0.85 para 0.92), mas tamb√©m aumenta o tempo de busca (de 5ms para 7ms), o tempo de constru√ß√£o (de 20 minutos para 35 minutos) e o uso de mem√≥ria (de 8GB para 12GB). A escolha ideal de *maxconn* depender√° do compromisso desejado entre precis√£o, velocidade e recursos computacionais.

Para otimizar a constru√ß√£o do HNSW, pode-se considerar a paraleliza√ß√£o do processo de inser√ß√£o.

**Teorema 1:** O processo de constru√ß√£o do grafo HNSW pode ser paralelizado para reduzir o tempo de constru√ß√£o sem afetar significativamente a qualidade da busca.

*Estrat√©gia de Prova:* Dividir o *dataset* em subconjuntos e construir grafos HNSW independentes para cada subconjunto. Em seguida, combinar os grafos em uma estrutura hier√°rquica unificada. A paraleliza√ß√£o reduz o tempo de constru√ß√£o, enquanto a combina√ß√£o dos grafos garante a qualidade da busca.

**Teorema 1.1:** A paraleliza√ß√£o da constru√ß√£o do HNSW com sincroniza√ß√£o peri√≥dica entre os subgrafos constru√≠dos pode melhorar a precis√£o da busca em compara√ß√£o com a constru√ß√£o paralela totalmente independente.

*Estrat√©gia de Prova:* A sincroniza√ß√£o peri√≥dica permite que os subgrafos compartilhem informa√ß√µes sobre a estrutura geral dos dados, reduzindo a probabilidade de constru√ß√£o de subgrafos isolados e melhorando a navegabilidade do grafo combinado.

### Conclus√£o

As abordagens baseadas em grafos, como os grafos kNN, NSW e HNSW, fornecem mecanismos eficientes para a busca de similaridade em sistemas de Neural Information Retrieval. Embora o grafo kNN exato tenha um alto custo computacional para ser constru√≠do, variantes aproximadas, como os grafos NSW e HNSW, oferecem um excelente equil√≠brio entre precis√£o e velocidade. O HNSW em particular se destaca por sua escalabilidade e alto desempenho, tornando-o uma escolha popular para aplicativos de NIR que envolvem grandes conjuntos de dados. As otimiza√ß√µes na constru√ß√£o e busca desses grafos, como a paraleliza√ß√£o e a sele√ß√£o cuidadosa de par√¢metros, podem melhorar ainda mais o desempenho desses m√©todos.

### Refer√™ncias

[^28]: Se√ß√£o 4, Retrieval Architectures and Vector Search
[^33]: Se√ß√£o 4.5, Graph Approaches
<!-- END -->