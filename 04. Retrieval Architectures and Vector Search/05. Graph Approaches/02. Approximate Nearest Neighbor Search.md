## Greedy Heuristic Search on kNN Graphs for Approximate Nearest Neighbor Search

### Introdu√ß√£o
Este cap√≠tulo explora as arquiteturas de *Retrieval* e a busca vetorial em sistemas de recupera√ß√£o de informa√ß√£o neural, com foco nas estrat√©gias para realizar buscas eficientes em grandes conjuntos de dados de *embeddings* [^28]. Ap√≥s introduzir o problema formal de busca do vizinho mais pr√≥ximo (*Nearest Neighbor Search - NN*) e do produto interno m√°ximo (*Maximum Inner Product Search - MIP*), abordaremos as t√©cnicas de *Locality Sensitive Hashing (LSH)* [^31] e *Vector Quantisation (VQ)* [^32]. Em continuidade, este cap√≠tulo se aprofundar√° nas abordagens baseadas em grafos, com √™nfase na busca heur√≠stica *greedy* em grafos kNN para realizar a busca aproximada do vizinho mais pr√≥ximo (*Approximate Nearest Neighbor Search - ANN*).

### Busca Heur√≠stica Greedy em Grafos kNN
Como vimos anteriormente, a busca exata em grafos kNN pode ser computacionalmente cara, especialmente para grandes conjuntos de dados [^33]. Para mitigar este problema, uma busca heur√≠stica *greedy* √© empregada para encontrar um vizinho mais pr√≥ximo aproximado para um elemento de consulta $\phi$ usando um grafo kNN [^33].

A busca heur√≠stica *greedy* opera da seguinte forma:
1. **Inicializa√ß√£o:** A busca come√ßa a partir de um n√≥ de entrada predefinido $v_{entry}$ no grafo. Esse n√≥ de entrada √© escolhido como um ponto de partida para a busca.
2. **Itera√ß√£o:** Em cada passo, o algoritmo examina os vizinhos n√£o visitados do n√≥ atual $v$. Dentre esses vizinhos, o algoritmo seleciona o n√≥ $v'$ que √© o mais pr√≥ximo de $\phi$ de acordo com alguma m√©trica de dist√¢ncia. Ou seja,

$$v' = \underset{u \in \text{Vizinhos n√£o visitados de } v}{\text{argmin}} \; \text{dist}(\phi, u)$$

3. **Atualiza√ß√£o:** O algoritmo move-se ent√£o para $v'$, tornando-o o novo n√≥ atual e marcando $v$ como visitado.
4. **Termina√ß√£o:** Este processo continua at√© que n√£o haja mais nenhuma melhoria no candidato NN atual. Mais formalmente, a busca termina quando

$$\text{dist}(\phi, v) \leq \text{dist}(\phi, u) \quad \forall \; u \in \text{Vizinhos n√£o visitados de } v$$

Na pr√°tica, v√°rios n√≥s de entrada s√£o usados ‚Äã‚Äãjuntamente com um or√ßamento de busca para evitar √≥timos locais [^33].

**Observa√ß√£o:** A escolha da m√©trica de dist√¢ncia $\text{dist}(\phi, u)$ √© crucial para o desempenho da busca. M√©tricas comuns incluem a dist√¢ncia euclidiana, a dist√¢ncia do cosseno e a dist√¢ncia de Mahalanobis. A escolha da m√©trica deve ser alinhada com a natureza dos dados e os objetivos da aplica√ß√£o.

> üí° **Exemplo Num√©rico:** Considere um espa√ßo vetorial 2D com os seguintes pontos: $\phi = [1, 1]$, $v = [2, 2]$, $u_1 = [1.5, 1.5]$, $u_2 = [3, 3]$. Usando a dist√¢ncia Euclidiana:
>
> $\text{dist}(\phi, v) = \sqrt{(2-1)^2 + (2-1)^2} = \sqrt{2} \approx 1.41$
> $\text{dist}(\phi, u_1) = \sqrt{(1.5-1)^2 + (1.5-1)^2} = \sqrt{0.5} \approx 0.71$
> $\text{dist}(\phi, u_2) = \sqrt{(3-1)^2 + (3-1)^2} = \sqrt{8} \approx 2.83$
>
> Se $v$ √© o n√≥ atual, e $u_1$ e $u_2$ s√£o os vizinhos n√£o visitados, ent√£o $v' = u_1$ pois $\text{dist}(\phi, u_1)$ √© a menor dist√¢ncia.

**Proposi√ß√£o 1:** A busca *greedy* em um grafo kNN converge para um m√≠nimo local da fun√ß√£o de dist√¢ncia entre o ponto de consulta $\phi$ e os n√≥s do grafo.

*Prova:* A cada itera√ß√£o, a dist√¢ncia entre o n√≥ atual e o ponto de consulta diminui. O algoritmo termina quando nenhum vizinho n√£o visitado est√° mais pr√≥ximo do ponto de consulta do que o n√≥ atual, o que significa que o n√≥ atual √© um m√≠nimo local. $\blacksquare$

### Vantagens e Desvantagens
**Vantagens:**
- A busca heur√≠stica *greedy* √© relativamente simples de implementar.
- Pode fornecer resultados de NN razo√°veis ‚Äã‚Äãem um tempo consideravelmente menor em compara√ß√£o com a busca exata.

**Desvantagens:**
- A qualidade dos resultados de NN depende fortemente da escolha do n√≥ de entrada e da estrutura do grafo kNN. Se o n√≥ de entrada estiver longe do NN real ou se o grafo tiver regi√µes mal conectadas, a busca *greedy* pode ficar presa em √≥timos locais e n√£o conseguir encontrar o NN real.
- Para um grande n√∫mero de n√≥s, a busca heur√≠stica no grafo kNN se torna ineficiente devido aos longos caminhos potencialmente necess√°rios para conectar dois n√≥s [^33].

> üí° **Exemplo Num√©rico:** Considere um grafo kNN com 1000 n√≥s. Se o n√≥ de entrada inicial est√° localizado em um extremo do grafo e o vizinho mais pr√≥ximo real est√° no extremo oposto, a busca *greedy* pode levar um n√∫mero significativo de itera√ß√µes para alcan√ßar o vizinho mais pr√≥ximo, especialmente se o grafo n√£o estiver bem conectado.

Para mitigar a depend√™ncia do n√≥ de entrada, podemos considerar m√∫ltiplas entradas aleat√≥rias.

**Teorema 1:** Utilizar m√∫ltiplos n√≥s de entrada aleat√≥rios na busca *greedy* em um grafo kNN aumenta a probabilidade de encontrar o vizinho mais pr√≥ximo real.

*Prova (Esbo√ßo):* A busca *greedy* converge para um m√≠nimo local. Se iniciarmos a busca a partir de diferentes pontos, exploramos diferentes caminhos no grafo. A probabilidade de um desses caminhos levar ao vizinho mais pr√≥ximo real √© maior do que a probabilidade de um √∫nico caminho levar ao vizinho mais pr√≥ximo real, assumindo que os m√≠nimos locais est√£o razoavelmente distribu√≠dos no grafo.

> üí° **Exemplo Num√©rico:** Suponha que temos um grafo kNN e um ponto de consulta. Iniciamos a busca *greedy* com 3 n√≥s de entrada diferentes. A busca a partir do primeiro n√≥ de entrada converge para um n√≥ que est√° a uma dist√¢ncia de 0.8 do ponto de consulta. A busca a partir do segundo n√≥ de entrada converge para um n√≥ que est√° a uma dist√¢ncia de 0.6 do ponto de consulta. A busca a partir do terceiro n√≥ de entrada converge para um n√≥ que est√° a uma dist√¢ncia de 0.4 do ponto de consulta. Nesse caso, escolher√≠amos o resultado da terceira busca, pois ele est√° mais pr√≥ximo do ponto de consulta.

### Grafos NSW (Navigable Small World)
Para mitigar a inefici√™ncia da busca heur√≠stica *greedy* em grafos kNN devido aos longos caminhos entre os n√≥s, podemos enriquecer o grafo kNN com conex√µes de longo alcance geradas aleatoriamente, i.e., arestas conectando dois n√≥s selecionados aleatoriamente [^33]. Este tipo de grafo kNN √© conhecido como um grafo *navigable small world (NSW)* [^33].

A heur√≠stica de busca *greedy* √© teoricamente e empiricamente eficiente para o grafo NSW [^33].

**Lema 1:** Em um grafo NSW, a dist√¢ncia esperada entre dois n√≥s quaisquer √© $O(\log N)$, onde $N$ √© o n√∫mero de n√≥s no grafo.

*Prova (Esbo√ßo):* As conex√µes de longo alcance permitem que a busca "pule" atrav√©s do grafo, reduzindo o n√∫mero de passos necess√°rios para chegar ao vizinho mais pr√≥ximo. A estrutura *small world* garante que esses saltos sejam eficazes na redu√ß√£o da dist√¢ncia.

**Teorema 1.1:** A complexidade da busca *greedy* em um grafo NSW √© $O(\log N)$ no caso m√©dio, onde $N$ √© o n√∫mero de n√≥s no grafo.

*Prova (Esbo√ßo):* Combinando a heur√≠stica *greedy* com as propriedades de caminho curto do grafo NSW (Lema 1), o n√∫mero esperado de passos na busca *greedy* se torna logar√≠tmico em rela√ß√£o ao n√∫mero de n√≥s. A busca *greedy* usa $O(1)$ para determinar o pr√≥ximo n√≥ a ser visitado, e a dist√¢ncia esperada √© $O(\log N)$.

### √çndice HNSW (Hierarchical NSW)
Um √≠ndice *Hierarchical NSW (HNSW)* armazena os dados de entrada em m√∫ltiplos grafos NSW [^33]. O grafo da camada inferior cont√©m um n√≥ para cada elemento de entrada, enquanto o n√∫mero de n√≥s nas outras camadas diminui exponencialmente a cada camada [^33]. O procedimento de busca para vetores NN aproximados come√ßa com o grafo da camada superior. Em cada camada, a busca heur√≠stica *greedy* encontra o n√≥ mais pr√≥ximo. Em seguida, a pr√≥xima camada √© pesquisada, come√ßando com o n√≥ correspondente ao n√≥ mais pr√≥ximo identificado na camada anterior [^33].

> üí° **Exemplo Num√©rico:** Imagine um √≠ndice HNSW com 3 camadas. A camada inferior tem 1000 n√≥s, a camada intermedi√°ria tem 100 n√≥s e a camada superior tem 10 n√≥s. A busca come√ßa na camada superior. Ap√≥s alguns passos *greedy*, o algoritmo identifica o n√≥ mais pr√≥ximo na camada superior. Em seguida, ele desce para a camada intermedi√°ria, come√ßando a busca a partir do n√≥ correspondente ao n√≥ mais pr√≥ximo identificado na camada superior. Este processo √© repetido at√© que a camada inferior seja alcan√ßada, onde o vizinho mais pr√≥ximo aproximado √© encontrado.

**Teorema 2:** A complexidade da busca em um √≠ndice HNSW com $L$ camadas √© $O(L \log N)$, onde $N$ √© o n√∫mero de n√≥s na camada mais baixa.

*Prova (Esbo√ßo):* A busca *greedy* √© realizada em cada uma das $L$ camadas. Como cada camada √© um grafo NSW, a complexidade da busca em cada camada √© $O(\log N_i)$, onde $N_i$ √© o n√∫mero de n√≥s na camada $i$. Como o n√∫mero de n√≥s diminui exponencialmente a cada camada, a complexidade total √© dominada pela camada inferior, resultando em $O(L \log N)$.

**Observa√ß√£o:** Na pr√°tica, o fator constante em $O(L \log N)$ √© pequeno, tornando o HNSW uma estrutura de √≠ndice muito eficiente para a busca ANN.

> üí° **Exemplo Num√©rico:** Compara√ß√£o da complexidade da busca:
>
> | M√©todo          | Complexidade da Busca |
> |-----------------|-----------------------|
> | Busca Exaustiva | $O(N)$                  |
> | k-d Tree        | $O(\log N)$ (melhor caso)|
> | HNSW            | $O(L \log N)$            |
>
> Onde:
> *   N √© o n√∫mero de vetores.
> *   L √© o n√∫mero de camadas no HNSW.
>
> Para N = 1.000.000 e L = 5, a busca exaustiva requer 1.000.000 opera√ß√µes, enquanto o HNSW requer aproximadamente 5 * log(1.000.000) ‚âà 30 opera√ß√µes (base 2). Isto demonstra a vantagem significativa em termos de efici√™ncia do HNSW.

### Conclus√£o
Neste cap√≠tulo, exploramos a busca heur√≠stica *greedy* em grafos kNN como um m√©todo para busca aproximada do vizinho mais pr√≥ximo. Embora esta t√©cnica ofere√ßa melhorias de efici√™ncia em rela√ß√£o √† busca exata, ela introduz *trade-offs* na precis√£o. Vimos tamb√©m o *Navigable Small World (NSW)* como uma alternativa e o *Hierarchical NSW (HNSW)*, que constr√≥i uma hierarquia de grafos NSW para acelerar ainda mais o processo de busca. Essas estruturas permitem que o algoritmo de busca navegue pelo grafo em diferentes n√≠veis de granularidade, melhorando a efici√™ncia e a escalabilidade.

### Refer√™ncias
[^28]: Veja a Se√ß√£o 4 do documento fonte.
[^31]: Veja a Se√ß√£o 4.3 do documento fonte.
[^32]: Veja a Se√ß√£o 4.4 do documento fonte.
[^33]: Veja a Se√ß√£o 4.5 do documento fonte.
<!-- END -->