## 4.5. Graph Approaches: Navigable Small World Graphs for Efficient Approximate Nearest Neighbor Search

### Introdu√ß√£o
Como vimos anteriormente, a busca por vizinhos mais pr√≥ximos (NN) √© fundamental para muitas aplica√ß√µes em *information retrieval* (IR) [^29, ^30, ^31]. No entanto, a busca exata se torna invi√°vel em conjuntos de dados de alta dimensionalidade, devido √† maldi√ß√£o da dimensionalidade. Para mitigar esse problema, t√©cnicas de busca aproximada de NN (ANN) s√£o frequentemente empregadas [^30]. Uma dessas t√©cnicas, focada em estruturas de dados baseadas em grafos, √© o que exploraremos em profundidade nesta se√ß√£o. Especificamente, vamos nos concentrar em como superar a inefici√™ncia da busca heur√≠stica gulosa em grafos kNN com um grande n√∫mero de n√≥s, enriquecendo-os com arestas de longo alcance geradas aleatoriamente, formando assim um grafo de mundo pequeno naveg√°vel (NSW) [^33]. Al√©m disso, discutiremos como um √≠ndice NSW hier√°rquico (HNSW) armazena os dados de entrada em m√∫ltiplos grafos NSW, habilitando um procedimento de busca eficiente [^33].

Para complementar esta introdu√ß√£o, podemos formalizar o conceito de busca ANN.

**Defini√ß√£o 1 (Busca Aproximada do Vizinho Mais Pr√≥ximo (ANN))** Dado um conjunto de dados $\Psi$ em um espa√ßo m√©trico $(\mathcal{X}, d)$, um inteiro $k \geq 1$ e um par√¢metro de aproxima√ß√£o $\epsilon > 0$, o problema da busca ANN consiste em, dado um ponto de consulta $q \in \mathcal{X}$, retornar um conjunto $A \subseteq \Psi$ de $k$ pontos tais que para todo $y \in A$ e todo $x \in \Psi \setminus A$, temos que $d(q, y) \leq (1 + \epsilon)d(q, x)$.

> üí° **Exemplo Num√©rico:** Suponha que temos um conjunto de dados $\Psi$ com 1000 pontos, $k = 5$ (queremos os 5 vizinhos mais pr√≥ximos), e $\epsilon = 0.1$. Para um ponto de consulta $q$, encontramos um conjunto $A$ de 5 pontos. Se a dist√¢ncia do ponto mais distante em $A$ para $q$ √© 2.0, ent√£o para qualquer ponto $x$ fora de $A$, a dist√¢ncia de $x$ para $q$ deve ser maior que $\frac{2.0}{1 + 0.1} \approx 1.82$. Isso significa que os pontos em $A$ est√£o dentro de um fator de $(1 + \epsilon)$ da dist√¢ncia real dos 5 vizinhos mais pr√≥ximos.

<!-- END_INSERT -->

### Navigable Small World (NSW) Graphs
Conforme descrito anteriormente [^33], as dist√¢ncias entre vetores em um *dataset* podem ser armazenadas eficientemente em uma estrutura de dados baseada em grafos chamada **grafo kNN** (k-Nearest Neighbors). Em um grafo kNN $G = (V, E)$, cada dado de entrada $\psi \in \Psi$ √© representado como um n√≥ $v \in V$, e, para seus $k$ vizinhos mais pr√≥ximos, uma aresta correspondente √© adicionada em $E$.

Um desafio surge quando o n√∫mero de n√≥s no grafo kNN √© grande, pois a busca heur√≠stica gulosa por um vizinho mais pr√≥ximo aproximado para um elemento $q$ se torna ineficiente [^33]. Esse m√©todo envolve visitar o grafo um n√≥ por vez, encontrando continuamente o n√≥ mais pr√≥ximo de $q$ entre os n√≥s vizinhos n√£o visitados. Para melhorar a efici√™ncia da busca, √© poss√≠vel enriquecer o grafo kNN com arestas de longo alcance geradas aleatoriamente, formando um **grafo de mundo pequeno naveg√°vel (NSW)**.

> Em vez de armazenar apenas arestas de curto alcance (ou seja, arestas conectando dois n√≥s pr√≥ximos), o grafo kNN pode ser enriquecido com arestas de longo alcance geradas aleatoriamente (ou seja, arestas conectando dois n√≥s selecionados aleatoriamente) [^33].

Essa modifica√ß√£o, conforme demonstrado na literatura [^33], permite que a busca heur√≠stica gulosa seja teoricamente e empiricamente eficiente [^33], auxiliando no processo de encontrar caminhos entre n√≥s mais distantes.

> üí° **Exemplo Num√©rico:** Considere um grafo kNN com 1000 n√≥s. Durante a constru√ß√£o do NSW, para cada n√≥, conectamos aos seus 10 vizinhos mais pr√≥ximos (arestas de curto alcance). Al√©m disso, adicionamos 2 arestas de longo alcance a n√≥s escolhidos aleatoriamente. Estas arestas de longo alcance ajudam a pular n√≥s distantes durante a busca, acelerando o processo. Sem as arestas de longo alcance, a busca poderia levar em m√©dia 50 passos para encontrar um vizinho aproximado; com elas, a m√©dia cai para 20 passos.

Para formalizar a constru√ß√£o de um grafo NSW, podemos apresentar o seguinte algoritmo.

**Algoritmo 1 (Constru√ß√£o de um Grafo NSW)**

1.  Inicialize um grafo vazio $G = (V, E)$.
2.  Para cada ponto de dados $\psi_i \in \Psi$:
    *   Adicione um n√≥ $v_i$ a $V$ correspondente a $\psi_i$.
3.  Para cada n√≥ $v_i \in V$:
    *   Encontre os $k$ vizinhos mais pr√≥ximos de $\psi_i$ em $\Psi$ usando uma m√©trica de dist√¢ncia $d(\cdot, \cdot)$.
    *   Adicione arestas entre $v_i$ e os n√≥s correspondentes aos seus $k$ vizinhos mais pr√≥ximos.
    *   Selecione aleatoriamente um subconjunto de n√≥s $V_{long}$ de $V$ (excluindo os $k$ vizinhos mais pr√≥ximos). O tamanho de $V_{long}$ pode ser um par√¢metro ajust√°vel.
    *   Adicione arestas de longo alcance entre $v_i$ e os n√≥s em $V_{long}$.

**Observa√ß√£o:** A escolha dos n√≥s para as arestas de longo alcance pode ser feita de diferentes maneiras, como sele√ß√£o uniforme aleat√≥ria ou usando uma distribui√ß√£o de probabilidade que favore√ßa n√≥s mais distantes.

> üí° **Exemplo Num√©rico:** Suponha que para um n√≥ $v_i$, seus 3 vizinhos mais pr√≥ximos s√£o $v_1, v_2, v_3$. Al√©m disso, selecionamos aleatoriamente dois outros n√≥s, $v_{50}$ e $v_{920}$, para serem conectados por arestas de longo alcance. Assim, $v_i$ ter√° arestas para $v_1, v_2, v_3, v_{50}, v_{920}$. Durante a busca, se a busca gulosa chegar a $v_i$ e $v_{920}$ estiver mais pr√≥ximo do ponto de consulta do que $v_1, v_2, v_3$, a busca poder√° pular para a vizinhan√ßa de $v_{920}$ em vez de explorar os vizinhos pr√≥ximos de $v_i$.

<!-- END_INSERT -->

### Hierarchical Navigable Small World (HNSW) Index
Para otimizar ainda mais a busca ANN, √© poss√≠vel empregar um √≠ndice **Hierarchical NSW (HNSW)**.

> Um √≠ndice HNSW armazena os dados de entrada em m√∫ltiplos grafos NSW [^33].

Em um √≠ndice HNSW, o grafo da camada inferior cont√©m um n√≥ para cada elemento de entrada, enquanto o n√∫mero de n√≥s nas camadas superiores diminui exponencialmente [^33].

O processo de busca por vetores NN aproximados come√ßa com o grafo na camada superior [^33]. Em cada camada, a busca heur√≠stica gulosa busca o n√≥ mais pr√≥ximo. Ent√£o, a pr√≥xima camada √© pesquisada, come√ßando do n√≥ que corresponde ao n√≥ mais pr√≥ximo identificado na camada anterior [^33]. Este processo continua at√© que a camada inferior seja alcan√ßada, na qual os $k$ n√≥s mais pr√≥ximos s√£o retornados [^33].

> üí° **Exemplo Num√©rico:** Imagine um HNSW com 3 camadas. A camada 2 (superior) tem 10 n√≥s, a camada 1 tem 100 n√≥s e a camada 0 (inferior) tem 1000 n√≥s. A busca come√ßa na camada 2, encontra o n√≥ mais pr√≥ximo (digamos, n√≥ 5). Ent√£o, na camada 1, come√ßa a busca pelos vizinhos do n√≥ 5 (da camada 2), encontrando o n√≥ 42. Finalmente, na camada 0, busca pelos vizinhos do n√≥ 42 (da camada 1), encontrando os 3 vizinhos mais pr√≥ximos (os n√≥s 123, 456, 789). O processo hier√°rquico reduz drasticamente o n√∫mero de compara√ß√µes necess√°rias.

Podemos detalhar o processo de constru√ß√£o do √≠ndice HNSW.

**Algoritmo 2 (Constru√ß√£o do √çndice HNSW)**

1.  Inicialize uma lista de grafos NSW vazios $L = [G_0, G_1, ..., G_L]$, onde $G_0$ √© a camada inferior.
2.  Insira todos os pontos de dados $\psi_i \in \Psi$ na camada inferior $G_0$.
3.  Para cada camada $l = 1$ at√© $L$:
    *   Selecione aleatoriamente um subconjunto de n√≥s de $G_{l-1}$ para promover para a camada $G_l$. O n√∫mero de n√≥s promovidos geralmente diminui exponencialmente com o aumento da camada.
    *   Construa um grafo NSW $G_l$ usando apenas os n√≥s promovidos para esta camada.
    *   Conecte cada n√≥ promovido em $G_l$ a seus $k$ vizinhos mais pr√≥ximos em $G_l$, assim como a seus vizinhos na camada inferior $G_{l-1}$.

> üí° **Exemplo Num√©rico:** Suponha que temos 1000 pontos de dados. Na camada 0 (inferior), todos os 1000 pontos s√£o inseridos. Para construir a camada 1, selecionamos aleatoriamente 100 pontos da camada 0. Para construir a camada 2, selecionamos aleatoriamente 10 pontos da camada 1. Cada n√≥ na camada superior √© conectado a seus vizinhos mais pr√≥ximos naquela camada, bem como a um ou mais n√≥s na camada inferior. Por exemplo, um n√≥ na camada 2 √© conectado aos seus 5 vizinhos mais pr√≥ximos na camada 2, e a 2 n√≥s correspondentes na camada 1.

<!-- END_INSERT -->

O √≠ndice HNSW √© uma estrutura de dados poderosa que permite buscas ANN eficientes, balanceando precis√£o e velocidade de busca, tornando-se uma escolha popular em aplica√ß√µes de *information retrieval* e *machine learning*.

Para analisar a complexidade da busca em HNSW, podemos considerar o seguinte:

**Proposi√ß√£o 1 (Complexidade da Busca em HNSW)** A complexidade da busca em um √≠ndice HNSW √© aproximadamente $O(\log n)$ no n√∫mero de pontos de dados $n$, assumindo que a dimensionalidade dos dados √© moderada e o grafo NSW em cada camada possui boas propriedades de mundo pequeno.

**Estrat√©gia da Prova:** A busca come√ßa na camada superior, que cont√©m um n√∫mero significativamente menor de n√≥s do que a camada inferior. A busca gulosa em cada camada reduz o espa√ßo de busca exponencialmente √† medida que se desce pelas camadas. Portanto, o n√∫mero total de n√≥s visitados √© logar√≠tmico no n√∫mero de pontos de dados.

> üí° **Exemplo Num√©rico:** Se tivermos 1 milh√£o de pontos de dados ($n = 10^6$), a complexidade de busca seria aproximadamente proporcional a $\log(10^6) \approx 6$. Isso significa que, em m√©dia, o n√∫mero de opera√ß√µes necess√°rias para encontrar o vizinho mais pr√≥ximo cresce muito lentamente √† medida que o tamanho do conjunto de dados aumenta. Comparando com uma busca linear que teria uma complexidade de $O(n)$, onde cada um dos 1 milh√£o de pontos seria comparado, a vantagem de HNSW se torna evidente.

<!-- END_INSERT -->

### Conclus√£o

Os grafos de mundo pequeno naveg√°veis (NSW) e seus √≠ndices hier√°rquicos (HNSW) fornecem uma maneira eficaz de realizar a busca aproximada do vizinho mais pr√≥ximo (ANN) em espa√ßos de alta dimens√£o. Enriquecer grafos kNN com arestas de longo alcance geradas aleatoriamente facilita a navega√ß√£o eficiente entre n√≥s distantes, mitigando a inefici√™ncia da busca gulosa em grafos kNN tradicionais [^33]. Ao organizar hierarquicamente os dados de entrada em v√°rias camadas de grafos NSW, os √≠ndices HNSW permitem um procedimento de busca escal√°vel e preciso, tornando-os inestim√°veis para aplica√ß√µes de *information retrieval* e outras tarefas com grandes conjuntos de dados de alta dimens√£o [^33].

### Refer√™ncias
[^29]: Salton, G., Wong, A., & Yang, C. S. (1975). A Vector Space Model for Automatic Indexing. *Communications of the ACM, 18*(11), 613-620.
[^30]: Indyk, P., & Motwani, R. (1998). Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality. In *Proceedings of the thirtieth annual ACM symposium on Theory of computing* (pp. 604-613).
[^31]: Bachrach, Y., Finkelstein, Y., Gilad-Bachrach, R., Katzir, L., Koenigstein, N., Nice, N., & Paquet, U. (2014). Speeding up the Xbox recommender system using a Euclidean transformation for inner-product spaces. In *Proceedings of the 8th ACM Conference on Recommender Systems* (pp. 257-264).
[^32]: Malkov, Y. A., Ponomarenko, A., Logvinov, A., & Krylov, V. (2013). Approximate nearest neighbor algorithm based on navigable small world graphs. *Information Systems, 45*, 61-68.
[^33]: Malkov, Y. A., & Yashunin, D. A. (2020). Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs. *IEEE Transactions on Pattern Analysis and Machine Intelligence, 42*(4), 824-836.
<!-- END -->