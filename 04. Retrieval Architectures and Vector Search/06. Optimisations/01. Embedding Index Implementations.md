## Otimiza√ß√µes em Arquiteturas de Recupera√ß√£o e Busca Vetorial: Implementa√ß√µes de √çndices de Embedding

### Introdu√ß√£o
No cap√≠tulo anterior, exploramos diversas arquiteturas de *Retrieval* e busca vetorial, focando em algoritmos e estruturas de dados que viabilizam a identifica√ß√£o eficiente de vizinhos mais pr√≥ximos (NN) e a maximiza√ß√£o do produto interno (MIP) em espa√ßos vetoriais de alta dimens√£o [^28]. Este cap√≠tulo aprofunda-se nas otimiza√ß√µes pr√°ticas e implementa√ß√µes de √≠ndices de *embedding*, detalhando as caracter√≠sticas e os *trade-offs* de diferentes abordagens, como √≠ndices planos (flat), *Locality Sensitive Hashing* (LSH), arquivos invertidos (IVF), *Product Quantization* (PQ), IVFPQ e *Hierarchical Navigable Small World* (HNSW) [^34].

### Implementa√ß√µes de √çndices de Embedding
Conforme discutido anteriormente, a performance de sistemas de *Retrieval* baseados em *embeddings* densos depende crucialmente da efici√™ncia com que os documentos relevantes s√£o identificados [^22]. As implementa√ß√µes de √≠ndices de *embedding* visam acelerar essa busca, permitindo que os *backends* de *Retrieval* processem um grande volume de dados em tempo h√°bil.

#### √çndices Planos (Flat)
Os √≠ndices planos s√£o a forma mais direta de indexa√ß√£o, armazenando os *embeddings* dos documentos explicitamente em um *array* ou lista [^30]. A busca por similaridade envolve o c√°lculo da dist√¢ncia ou produto interno entre o *embedding* da *query* e todos os *embeddings* no √≠ndice.

*   **Vantagens:** Simplicidade de implementa√ß√£o e garantia de resultados exatos.
*   **Desvantagens:** Complexidade computacional $O(nl)$, onde *n* √© o n√∫mero de documentos e *l* √© a dimens√£o dos *embeddings*, torna-se proibitiva para grandes conjuntos de dados [^30].
*   **Casos de Uso:** Adequado para conjuntos de dados pequenos, onde a precis√£o √© primordial e a lat√™ncia n√£o √© cr√≠tica [^34].

> üí° **Exemplo Num√©rico:**
>
> Considere um conjunto de dados com apenas 5 documentos, e cada documento √© representado por um *embedding* de dimens√£o 100. Queremos encontrar o documento mais similar a uma *query* espec√≠fica.
>
> 1.  Armazenamos os 5 *embeddings* em um *array*.
> 2.  Para cada documento, calculamos o produto interno (ou similaridade do cosseno) entre o *embedding* da *query* e o *embedding* do documento.
> 3.  Selecionamos o documento com o maior produto interno.
>
> Este processo garante que encontraremos o vizinho mais pr√≥ximo exato, mas o custo computacional √© proporcional a $5 \times 100 = 500$ opera√ß√µes (aproximadamente), o que pode se tornar invi√°vel para conjuntos de dados maiores.
>
> | Documento | Produto Interno |
> | --------- | --------------- |
> | Doc 1     | 0.85            |
> | Doc 2     | 0.92            |
> | Doc 3     | 0.78            |
> | Doc 4     | 0.88            |
> | Doc 5     | 0.95            |
>
> Neste exemplo, o Doc 5 seria retornado como o mais similar √† *query*.

Para complementar a discuss√£o sobre √≠ndices planos, √© √∫til analisar o custo computacional de opera√ß√µes b√°sicas.

**Proposi√ß√£o 1** O custo de adicionar um novo embedding a um √≠ndice plano √© $O(1)$, assumindo que h√° espa√ßo alocado dispon√≠vel no array ou lista. O custo de deletar um embedding √© $O(1)$ se a ordem dos embeddings n√£o precisar ser preservada (podemos simplesmente substituir o embedding a ser deletado pelo √∫ltimo embedding da lista e reduzir o tamanho da lista em 1), e $O(n)$ se a ordem precisar ser preservada (requerendo o deslocamento de todos os embeddings subsequentes).

*Proof.* Adicionar um novo embedding ao final de um array ou lista tem custo constante. A dele√ß√£o sem preservar a ordem tamb√©m tem custo constante. Preservar a ordem requer percorrer o array e deslocar os elementos, resultando em um custo linear.

<!-- END_ADDITION -->

#### Locality Sensitive Hashing (LSH)
LSH √© uma fam√≠lia de algoritmos de *hashing* que visa agrupar *embeddings* similares em "baldes" (buckets) de *hash* [^31]. O princ√≠pio subjacente √© que, ap√≥s uma "proje√ß√£o" por meio de uma fun√ß√£o de *hash*, *embeddings* pr√≥ximos permanecem pr√≥ximos, enquanto *embeddings* distantes t√™m baixa probabilidade de colidir no mesmo balde.

*   **Vantagens:** Busca sublinear com complexidade $O(n^c)$, onde $c < 1$, oferecendo um *trade-off* entre precis√£o e velocidade [^31].
*   **Desvantagens:** Requer um ajuste cuidadoso dos par√¢metros de *hash* para otimizar o balan√ßo entre a probabilidade de colis√£o para vizinhos pr√≥ximos (alta) e a probabilidade de colis√£o para itens distantes (baixa). O consumo de mem√≥ria pode ser elevado, especialmente para garantir resultados precisos [^31].
*   **Casos de Uso:** √ötil quando a lat√™ncia √© uma preocupa√ß√£o maior que a precis√£o, e existe a possibilidade de otimizar as fun√ß√µes de *hash* para o dom√≠nio espec√≠fico [^34].

> üí° **Exemplo Num√©rico:**
>
> Imagine que temos 10.000 documentos com *embeddings* de dimens√£o 128. Utilizamos LSH com 100 tabelas de *hash* e cada tabela tem 1000 baldes.
>
> 1.  Para uma dada *query*, calculamos o *hash* para cada uma das 100 tabelas.
> 2.  Em cada tabela, recuperamos os documentos que est√£o no mesmo balde de *hash* que a *query*.
> 3.  Unimos os resultados das 100 tabelas (removendo duplicatas).
> 4.  Calculamos a similaridade (produto interno ou cosseno) entre a *query* e os documentos resultantes.
> 5.  Retornamos os *k* documentos mais similares.
>
> Sem LSH, ter√≠amos que comparar a *query* com os 10.000 documentos. Com LSH, esperamos comparar a *query* com um n√∫mero muito menor de documentos, dependendo da distribui√ß√£o dos dados e da qualidade das fun√ß√µes de *hash*. Por exemplo, se em m√©dia 10 documentos colidem no mesmo balde em cada tabela, comparar√≠amos a *query* com aproximadamente $100 \times 10 = 1000$ documentos (ap√≥s remover duplicatas), o que representa uma redu√ß√£o significativa no custo computacional. No entanto, essa redu√ß√£o pode vir √† custa da precis√£o, j√° que alguns dos vizinhos mais pr√≥ximos verdadeiros podem n√£o ter sido inclu√≠dos nos baldes recuperados.

Um aspecto importante do LSH √© a escolha da fam√≠lia de fun√ß√µes de hash. Uma fam√≠lia popular √© o E2LSH (Exact Euclidean LSH), adequado para espa√ßos Euclidianos.

**Teorema 2** (E2LSH) Para qualquer $c > 1$, existe uma fam√≠lia de fun√ß√µes de hash $H$ para o espa√ßo Euclidiando $R^d$ tal que para quaisquer pontos $p, q \in R^d$:

*   Se $||p - q|| \leq r$, ent√£o $P_H[h(p) = h(q)] \geq P_1$
*   Se $||p - q|| \geq cr$, ent√£o $P_H[h(p) = h(q)] \leq P_2$

onde $P_1 > P_2$ e $P_H$ denota a probabilidade sobre a escolha aleat√≥ria de $h$ de $H$.

Este teorema formaliza a ideia de que pontos pr√≥ximos t√™m maior probabilidade de colidir sob uma fun√ß√£o de hash escolhida aleatoriamente da fam√≠lia $H$. A efici√™ncia do LSH depende criticamente da escolha de $c$, $r$, $P_1$, e $P_2$, que s√£o par√¢metros a serem otimizados.

<!-- END_ADDITION -->

#### Arquivos Invertidos (IVF)
IVF √© uma t√©cnica que combina indexa√ß√£o invertida com quantiza√ß√£o vetorial [^32]. O espa√ßo de *embedding* √© particionado em *k* *clusters* usando algoritmos como *k-means*, e os documentos s√£o atribu√≠dos ao *cluster* cujo centr√≥ide √© o mais pr√≥ximo.

*   **Vantagens:** Permite restringir a busca a um subconjunto dos *clusters* mais relevantes para a *query*, reduzindo drasticamente o tempo de busca.
*   **Desvantagens:** A qualidade da busca depende da escolha dos centr√≥ides e do n√∫mero de parti√ß√µes *k*. Requer a manuten√ß√£o de um √≠ndice adicional para mapear *embeddings* para *clusters* [^32].
*   **Casos de Uso:** Boa op√ß√£o quando a estrutura do espa√ßo de *embedding* permite uma clusteriza√ß√£o eficaz, e o objetivo √© reduzir o tempo de busca sem incorrer em perdas de precis√£o significativas [^34].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos 1 milh√£o de documentos e decidimos usar IVF com *k* = 1000 *clusters*. Isso significa que primeiro executamos o algoritmo *k-means* para agrupar os documentos em 1000 *clusters*.
>
> 1.  Para cada documento, calculamos a dist√¢ncia entre seu *embedding* e os 1000 centr√≥ides.
> 2.  Atribu√≠mos o documento ao *cluster* cujo centr√≥ide √© o mais pr√≥ximo.
> 3.  Agora, para buscar documentos similares a uma *query*, calculamos a dist√¢ncia entre o *embedding* da *query* e os 1000 centr√≥ides.
> 4.  Selecionamos os *n* *clusters* mais pr√≥ximos da *query* (por exemplo, os 10 *clusters* mais pr√≥ximos).
> 5.  Comparamos a *query* apenas com os documentos pertencentes a esses 10 *clusters*.
>
> Se cada *cluster* contiver em m√©dia 1000 documentos, em vez de comparar a *query* com 1 milh√£o de documentos, comparamos apenas com $10 \times 1000 = 10000$ documentos, reduzindo significativamente o tempo de busca. A escolha de *n* (n√∫mero de *clusters* a serem considerados) afeta o *trade-off* entre precis√£o e velocidade.

Para melhorar a efici√™ncia da aloca√ß√£o de vetores aos clusters no IVF, podemos considerar a utiliza√ß√£o de estruturas de dados auxiliares.

**Lema 3** A atribui√ß√£o de um vetor a um cluster pode ser acelerada utilizando estruturas de dados como k-d trees ou ball trees para indexar os centr√≥ides dos clusters. Com essas estruturas, o custo da atribui√ß√£o se torna $O(log k)$ por vetor, onde k √© o n√∫mero de clusters, em vez de $O(k)$ com uma busca linear.

*Proof.* k-d trees e ball trees s√£o estruturas de dados espaciais que permitem a busca do vizinho mais pr√≥ximo em tempo logar√≠tmico sob certas condi√ß√µes. Ao indexar os centr√≥ides dos clusters com essas estruturas, a busca pelo centr√≥ide mais pr√≥ximo para um dado vetor se torna uma opera√ß√£o de busca do vizinho mais pr√≥ximo, resultando na complexidade de tempo mencionada.

<!-- END_ADDITION -->

#### Product Quantization (PQ) e IVFPQ
PQ decomp√µe o espa√ßo de *embedding* em m√∫ltiplos subespa√ßos de menor dimens√£o, quantizando cada subvetor independentemente [^32]. IVFPQ combina IVF com PQ, aplicando PQ aos res√≠duos ap√≥s a atribui√ß√£o do *cluster* [^33].

*   **Vantagens:** PQ reduz drasticamente os requisitos de armazenamento, enquanto IVFPQ combina essa economia de espa√ßo com a velocidade da busca baseada em *clusters*.
*   **Desvantagens:** A quantiza√ß√£o introduz erros, o que pode afetar a precis√£o da busca. A complexidade de implementa√ß√£o √© maior comparada com √≠ndices planos ou LSH [^34].
*   **Casos de Uso:** Ideal para cen√°rios onde a mem√≥ria √© limitada e a busca exata √© dispens√°vel, permitindo escalar para conjuntos de dados muito grandes com uma perda controlada de precis√£o [^34].

> üí° **Exemplo Num√©rico:**
>
> Considere *embeddings* de dimens√£o 128. Com PQ, podemos dividir cada *embedding* em 8 subvetores de dimens√£o 16. Para cada subvetor, realizamos *k-means* para criar um codebook de, digamos, 256 centr√≥ides.
>
> 1.  Cada subvetor √© ent√£o substitu√≠do pelo ID do centr√≥ide mais pr√≥ximo em seu codebook. Assim, cada *embedding* original de 128 dimens√µes √© agora representado por 8 IDs, cada um ocupando 8 bits (j√° que $2^8 = 256$). Isso reduz drasticamente o espa√ßo de armazenamento.
> 2.  Durante a busca, a *query* tamb√©m √© dividida em subvetores, e cada subvetor √© quantizado usando os mesmos codebooks.
> 3.  As dist√¢ncias entre a *query* quantizada e os *embeddings* quantizados s√£o ent√£o aproximadas usando tabelas pr√©-computadas de dist√¢ncias entre os centr√≥ides.
>
> No IVFPQ, primeiro usamos IVF para restringir a busca a um subconjunto de *clusters*, e ent√£o aplicamos PQ aos res√≠duos (a diferen√ßa entre o *embedding* original e o centr√≥ide do *cluster*). Isso pode melhorar a precis√£o em compara√ß√£o com o PQ puro.

√â importante notar que a escolha do n√∫mero de subespa√ßos e o n√∫mero de centr√≥ides por subespa√ßo em PQ afeta diretamente o desempenho e a precis√£o.

**Teorema 4** Seja *m* o n√∫mero de subespa√ßos em PQ, e *k'* o n√∫mero de centr√≥ides em cada subespa√ßo. A mem√≥ria total requerida para armazenar os centr√≥ides √© $m \cdot k' \cdot l/m$, onde *l* √© a dimens√£o original do embedding. A precis√£o da quantiza√ß√£o aumenta com o aumento de *k'*, mas o custo computacional da quantiza√ß√£o tamb√©m aumenta.

*Proof.* A mem√≥ria total requerida √© simplesmente o n√∫mero de subespa√ßos multiplicado pelo n√∫mero de centr√≥ides por subespa√ßo multiplicado pela dimens√£o de cada centr√≥ide (que √© *l/m*). A rela√ß√£o entre *k'* e precis√£o decorre do fato de que com mais centr√≥ides, a representa√ß√£o quantizada pode aproximar melhor o vetor original. O aumento do custo computacional com *k'* se deve ao fato de que a atribui√ß√£o de um subvetor a um centr√≥ide requer a busca pelo vizinho mais pr√≥ximo entre os *k'* centr√≥ides.

<!-- END_ADDITION -->

#### Hierarchical Navigable Small World (HNSW)
HNSW organiza os *embeddings* em uma estrutura hier√°rquica de grafos, onde cada camada representa uma aproxima√ß√£o mais grosseira do conjunto de dados [^33]. A busca come√ßa na camada superior e, a cada n√≠vel, o algoritmo refina a busca para os vizinhos mais pr√≥ximos.

*   **Vantagens:** Oferece um excelente equil√≠brio entre velocidade e precis√£o, sendo capaz de lidar com grandes conjuntos de dados com baixa lat√™ncia. A estrutura hier√°rquica permite buscas eficientes em espa√ßos de alta dimensionalidade [^33].
*   **Desvantagens:** A constru√ß√£o do √≠ndice pode ser computacionalmente intensiva. Requer um ajuste fino dos par√¢metros para otimizar o desempenho em diferentes conjuntos de dados [^34].
*   **Casos de Uso:** Tornou-se uma escolha popular para aplica√ß√µes de *Retrieval* denso, onde a combina√ß√£o de alta velocidade e boa precis√£o √© essencial [^34].

> üí° **Exemplo Num√©rico:**
>
> Imagine construir um √≠ndice HNSW para 1 milh√£o de documentos.
>
> 1.  Come√ßamos com uma camada superior que cont√©m apenas alguns n√≥s (os "vizinhos mais diversos").
> 2.  Cada n√≥ na camada superior tem conex√µes com n√≥s na camada inferior, que representam uma aproxima√ß√£o mais refinada dos dados.
> 3.  Continuamos adicionando camadas at√© chegar √† camada inferior, que cont√©m todos os 1 milh√£o de documentos.
> 4.  Para buscar um vizinho pr√≥ximo de uma *query*:
>     *   Come√ßamos na camada superior e encontramos o n√≥ mais pr√≥ximo da *query*.
>     *   Movemos para a camada inferior, restringindo a busca aos vizinhos do n√≥ selecionado na camada superior.
>     *   Repetimos o processo at√© chegar √† camada inferior, onde encontramos o vizinho mais pr√≥ximo final.
>
> A estrutura hier√°rquica permite que o algoritmo ignore grande parte do conjunto de dados durante a busca, resultando em uma busca muito mais r√°pida em compara√ß√£o com uma busca exaustiva.

Um dos par√¢metros cruciais no HNSW √© o n√∫mero de conex√µes por n√≥ em cada camada. Este par√¢metro afeta tanto a velocidade de constru√ß√£o do √≠ndice quanto a precis√£o da busca.

**Proposi√ß√£o 5** Aumentar o n√∫mero de conex√µes por n√≥ em HNSW geralmente leva a uma maior precis√£o da busca, mas tamb√©m aumenta o tempo de constru√ß√£o do √≠ndice e o consumo de mem√≥ria.

*Proof.* Mais conex√µes permitem que o algoritmo explore um espa√ßo maior durante a busca, aumentando a probabilidade de encontrar os vizinhos mais pr√≥ximos verdadeiros. No entanto, manter e percorrer mais conex√µes aumenta o custo computacional tanto na constru√ß√£o quanto na busca. O aumento no consumo de mem√≥ria √© direto, pois mais conex√µes precisam ser armazenadas para cada n√≥.

<!-- END_ADDITION -->

### Otimiza√ß√µes Adicionais
Al√©m da escolha do tipo de √≠ndice, v√°rias otimiza√ß√µes podem ser aplicadas para melhorar ainda mais o desempenho dos sistemas de busca vetorial [^34]:

*   **Redu√ß√£o da Dimensionalidade:** T√©cnicas como PCA (Principal Component Analysis) ou auto-encoders podem ser usadas para reduzir a dimensionalidade dos *embeddings* antes da indexa√ß√£o, diminuindo os requisitos de armazenamento e o tempo de busca.
*   **Quantiza√ß√£o:** A quantiza√ß√£o escalar ou vetorial pode ser usada para comprimir os *embeddings*, reduzindo o uso de mem√≥ria.
*   **Pruning de Query Embeddings:** Em sistemas com m√∫ltiplas representa√ß√µes de *query*, como ColBERT, a remo√ß√£o de *embeddings* de *query* menos relevantes pode reduzir a lat√™ncia sem impactar significativamente a precis√£o.
*   **Sele√ß√£o de Documentos:** Limitar o n√∫mero de documentos processados na segunda etapa de um *pipeline* de re-rankeamento pode melhorar a efici√™ncia sem comprometer a qualidade dos resultados.

> üí° **Exemplo Num√©rico:**
>
> Suponha que estamos usando PCA para reduzir a dimensionalidade de *embeddings* de 768 para 128 dimens√µes. Isso significa que treinamos um modelo PCA no conjunto de dados de *embeddings* e, em seguida, projetamos todos os *embeddings* originais no espa√ßo de 128 dimens√µes.
>
> Isso tem v√°rias vantagens:
>
> 1.  Reduz o espa√ßo de armazenamento em um fator de 6 (768/128).
> 2.  Acelera a busca, pois as opera√ß√µes de dist√¢ncia s√£o realizadas em vetores menores.
>
> No entanto, a redu√ß√£o da dimensionalidade tamb√©m pode levar a uma perda de informa√ß√£o, o que pode afetar a precis√£o da busca. √â importante avaliar o *trade-off* entre velocidade e precis√£o ao escolher o n√∫mero de dimens√µes a serem mantidas.

Al√©m das otimiza√ß√µes mencionadas, a paraleliza√ß√£o da busca e da constru√ß√£o do √≠ndice √© uma t√©cnica poderosa.

**Teorema 6** A busca em √≠ndices de embedding pode ser paralelizada de forma eficaz, tanto em CPUs quanto em GPUs. Em particular, algoritmos como IVF e PQ s√£o altamente paraleliz√°veis, pois a atribui√ß√£o de vetores a clusters e a quantiza√ß√£o de subvetores podem ser feitas independentemente para diferentes vetores.

*Proof.* A independ√™ncia das opera√ß√µes permite que sejam distribu√≠das entre m√∫ltiplos n√∫cleos de CPU ou unidades de processamento de GPU, resultando em uma redu√ß√£o significativa no tempo total de busca. Frameworks como FAISS exploram extensivamente essa paraleliza√ß√£o.

<!-- END_ADDITION -->
 A seguir, apresentamos algumas figuras que ilustram arquiteturas de sistemas de recupera√ß√£o de informa√ß√£o, para complementar as explica√ß√µes.

A figura 7 ilustra a arquitetura de um *pipeline* de re-rankeamento.

![Re-ranking pipeline architecture for interaction-focused neural IR systems.](./../images/image1.png)

A figura 8 mostra a arquitetura de recupera√ß√£o densa.

![Dense retrieval architecture using representation-focused neural networks.](./../images/image2.png)

Por fim, a figura 9 apresenta a arquitetura de *pipeline* de ranking para sistemas de m√∫ltiplas representa√ß√µes.

![Ranking pipeline architecture for multiple representation systems using learned embeddings and ANN search.](./../images/image3.png)

### Conclus√£o

A escolha do √≠ndice de *embedding* e das otimiza√ß√µes apropriadas √© um processo complexo, que depende das caracter√≠sticas do conjunto de dados, dos requisitos de lat√™ncia e precis√£o, e dos recursos computacionais dispon√≠veis [^34]. √çndices planos oferecem precis√£o m√°xima, mas n√£o escalam bem para grandes conjuntos de dados. LSH, IVF, PQ e HNSW representam *trade-offs* entre velocidade, precis√£o e uso de mem√≥ria. T√©cnicas de otimiza√ß√£o adicionais podem ser aplicadas para refinar ainda mais o desempenho dos sistemas de busca vetorial.

A crescente sofistica√ß√£o das arquiteturas de *Retrieval* e a disponibilidade de *frameworks* como FAISS [^34] e motores de busca como Lucene e Vespa [^34] facilitam a experimenta√ß√£o e a implementa√ß√£o de solu√ß√µes de busca vetorial eficientes e escal√°veis. A compreens√£o detalhada dos diferentes tipos de √≠ndices e das otimiza√ß√µes dispon√≠veis √© fundamental para projetar sistemas de *Retrieval* que atendam √†s necessidades espec√≠ficas de cada aplica√ß√£o.

### Refer√™ncias

[^22]: Urbanek, J., Fan, A., Karamcheti, S., Jain, S., Humeau, S., Dinan, E., ... & Weston, J. (2019). Learning to speak and act in a fantasy text adventure game. In *Proc. EMNLP-IJCNLP*, pp. 673‚Äì683.
[^28]: Dai, Z., Xiong, C., Callan, J., and Liu, Z. 2018. Convolutional Neural Networks for Soft-Matching N-Grams in Ad-Hoc Search. In *Proc. WSDM*, p. 126‚Äì134.
[^30]: Bachrach, Y., Finkelstein, Y., Gilad-Bachrach, R., Katzir, L., Koenigstein, N., Nice, N., and Paquet, U. 2014. Speeding up the Xbox Recommender System Using a Euclidean Transformation for Inner-Product Spaces. In *Proc. RecSys*, p. 257‚Äì264.
[^31]: Indyk, P. and Motwani, R. 1998. Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality. In *Proc. STOC*, —Ä. 604‚Äì613.
[^32]: J√©gou, H., Douze, M., and Schmid, C. 2011. Product Quantization for Nearest Neighbor Search. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 33(1): 117‚Äì128.
[^33]: Malkov, Y. and Yashunin, D. A. 2020. Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs. *IEEE Transactions on Pattern Analysis & Machine Intelligence*, 42(04): 824‚Äì836.
[^34]: Johnson, J., Douze, M., and J√©gou, H. 2021. Billion-Scale Similarity Search with GPUs. *IEEE Transactions on Big Data*, 7(03): 535‚Äì547.
<!-- END -->