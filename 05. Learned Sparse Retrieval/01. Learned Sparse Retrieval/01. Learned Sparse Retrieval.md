## Learned Sparse Retrieval: Bridging Neural Networks and Inverted Indexes

### Introdu√ß√£o
Como introduzido, o **Learned Sparse Retrieval (LSR)** visa integrar os avan√ßos de efetividade das redes neurais nos *inverted indexes*, aproveitando algoritmos de processamento de *query* eficientes [^35]. Essa abordagem preenche a lacuna entre os sistemas de Information Retrieval (IR) tradicionais, baseados em representa√ß√µes *sparse*, e os sistemas de IR neurais modernos que utilizam representa√ß√µes *dense*, oferecendo um equil√≠brio entre a efetividade da busca e a efici√™ncia do processamento de *query* [^35]. Os sistemas de IR tradicionais dependem de representa√ß√µes *sparse*, *inverted indexes* e fun√ß√µes de pontua√ß√£o de relev√¢ncia baseadas em l√©xico, como o BM25, que oferecem uma troca entre efetividade e efici√™ncia, adequada para a busca na web em escala industrial [^35]. Os sistemas de IR neurais, inversamente, dependem de representa√ß√µes *dense* de *queries* e documentos, proporcionando efetividade de busca superior, mas ao custo de tempos de processamento de *query* mais lentos [^35].

Este cap√≠tulo se aprofundar√° nos m√©todos de Learned Sparse Retrieval, explorando como as arquiteturas *transformer* s√£o empregadas em diversos cen√°rios para obter o melhor dos dois mundos: a efetividade sem√¢ntica das redes neurais e a efici√™ncia computacional dos *inverted indexes*.

### Conceitos Fundamentais
Learned Sparse Retrieval (LSR) emerge como uma √°rea de pesquisa promissora, buscando preencher a lacuna entre os modelos de recupera√ß√£o lexical tradicionais e a expressividade dos modelos neurais [^35]. Enquanto os modelos lexicais, como o BM25 [^5, 9], dependem de *inverted indexes* e da frequ√™ncia de termos para determinar a relev√¢ncia, eles frequentemente falham em capturar as nuances sem√¢nticas e o contexto da linguagem. Em contrapartida, os modelos neurais, particularmente aqueles baseados em arquiteturas *transformer* como BERT [^15], s√£o capazes de aprender representa√ß√µes ricas e contextuais de *queries* e documentos, superando os modelos lexicais em termos de efetividade de recupera√ß√£o [^17]. No entanto, essa melhoria de efetividade tem um custo computacional significativo, tornando impratic√°vel a aplica√ß√£o direta de modelos neurais para classificar grandes cole√ß√µes de documentos.

O LSR tenta combinar o melhor dos dois mundos, aprendendo representa√ß√µes *sparse* que podem ser indexadas de forma eficiente usando *inverted indexes*, ao mesmo tempo em que incorporam o conhecimento sem√¢ntico aprendido de modelos neurais [^35]. Isso geralmente envolve o uso de modelos neurais para *document expansion*, *impact score learning* ou *sparse representation learning*, que s√£o ent√£o integrados aos pipelines de recupera√ß√£o tradicionais.

> üí° **Exemplo Num√©rico: TF-IDF e BM25**
>
> Para ilustrar a diferen√ßa entre modelos lexicais e neurais, considere os seguintes documentos e *query*:
>
> *   Documento 1: "O gato est√° no tapete."
> *   Documento 2: "O cachorro est√° no tapete."
> *   Query: "Animal no tapete"
>
> Uma abordagem tradicional como TF-IDF ou BM25 pode dar pontua√ß√µes similares a ambos os documentos, j√° que ambos compartilham as palavras "no" e "tapete". No entanto, um modelo neural poderia reconhecer que "gato" e "cachorro" s√£o ambos "animais" e, portanto, atribuir uma pontua√ß√£o mais alta para ambos os documentos em rela√ß√£o √† *query*, demonstrando a capacidade de capturar a sem√¢ntica.
>
> Vamos calcular o TF-IDF para um termo no Documento 1: "gato".
>
> Suponha que temos um corpus de 1000 documentos. O termo "gato" aparece em 10 documentos. No Documento 1, o termo "gato" aparece 1 vez, e o documento tem um total de 5 termos.
>
> $\text{TF}(gato, Doc1) = \frac{\text{Frequ√™ncia de 'gato' em Doc1}}{\text{Total de termos em Doc1}} = \frac{1}{5} = 0.2$
>
> $\text{IDF}(gato) = \log\left(\frac{\text{Total de documentos}}{\text{Documentos com 'gato'}}\right) = \log\left(\frac{1000}{10}\right) = \log(100) \approx 2$
>
> $\text{TF-IDF}(gato, Doc1) = \text{TF}(gato, Doc1) \times \text{IDF}(gato) = 0.2 \times 2 = 0.4$
>
> Agora, vamos calcular a pontua√ß√£o BM25 simplificada para a *query* "Animal no tapete" e o Documento 1, considerando apenas o termo "gato" como representativo da sem√¢ntica "Animal". Assumindo $k_1 = 1.2$ e $b = 0.75$, e um tamanho m√©dio de documento de 5 termos.
>
> $\text{BM25}(q, Doc1) = \sum_{t \in q} IDF(t) \cdot \frac{TF(t, Doc1) \cdot (k_1 + 1)}{TF(t, Doc1) + k_1 \cdot (1 - b + b \cdot \frac{\text{Tamanho do Doc1}}{\text{Tamanho m√©dio dos documentos}})}$
>
> $\text{BM25}(q, Doc1) \approx IDF(gato) \cdot \frac{TF(gato, Doc1) \cdot (k_1 + 1)}{TF(gato, Doc1) + k_1 \cdot (1 - b + b \cdot \frac{\text{5}}{\text{5}})}$
>
> $\text{BM25}(q, Doc1) \approx 2 \cdot \frac{0.2 \cdot (1.2 + 1)}{0.2 + 1.2 \cdot (1 - 0.75 + 0.75)} = 2 \cdot \frac{0.44}{0.2 + 1.2 \cdot 1} = 2 \cdot \frac{0.44}{1.4} \approx 0.628$
>
> Este exemplo simplificado mostra como o BM25 d√° uma pontua√ß√£o baseada na frequ√™ncia dos termos, e um modelo neural poderia capturar a rela√ß√£o sem√¢ntica entre "animal" e "gato/cachorro", potencialmente atribuindo pontua√ß√µes diferentes.

Para complementar essa discuss√£o, podemos definir formalmente o problema de Learned Sparse Retrieval.

**Defini√ß√£o 1 (Learned Sparse Retrieval)** Dado um corpus de documentos $\mathcal{D} = \{d_1, d_2, ..., d_N\}$ e uma *query* $q$, o objetivo do Learned Sparse Retrieval √© aprender uma fun√ß√£o $f(d, q)$ que retorna um score de relev√¢ncia entre o documento $d$ e a *query* $q$, onde a representa√ß√£o de $d$ √© *sparse* e index√°vel atrav√©s de *inverted indexes*. Formalmente, $f(d, q) = score(sparse(d), q)$, onde $sparse(d)$ representa a representa√ß√£o *sparse* do documento $d$, e $score$ √© uma fun√ß√£o de pontua√ß√£o (e.g., similaridade do cosseno, BM25) aplicada sobre a representa√ß√£o *sparse* do documento e a *query*.

Conforme mencionado na introdu√ß√£o [^35], as arquiteturas *transformer* s√£o usadas em diferentes cen√°rios em *learned sparse retrieval*:
*   ***Document expansion learning:*** modelos *sequence-to-sequence* s√£o usados para modificar o conte√∫do real dos documentos, impulsionando as estat√≠sticas dos termos importantes e gerando novos termos para serem inclu√≠dos em um documento [^35].
*   ***Impact score learning:*** os *output embeddings* de documentos fornecidos como entrada para modelos *encoder-only* s√£o posteriormente transformados com redes neurais para gerar um √∫nico valor real, usado para estimar a contribui√ß√£o m√©dia de relev√¢ncia do termo no documento [^35].
*   ***Sparse representation learning:*** os *output embeddings* de documentos fornecidos como entrada para modelos *encoder-only* s√£o projetados com uma matriz aprendida no vocabul√°rio da cole√ß√£o, a fim de estimar os termos relevantes em um documento [^35].

√â importante notar que a escolha da arquitetura *transformer* e a estrat√©gia de treinamento desempenham um papel crucial no desempenho do LSR. Por exemplo, o uso de t√©cnicas de *knowledge distillation* pode transferir o conhecimento de modelos *dense* maiores para modelos *sparse* menores, melhorando a efetividade sem comprometer a efici√™ncia. Al√©m disso, a sele√ß√£o de uma fun√ß√£o de *loss* apropriada √© fundamental para otimizar o processo de aprendizado.

> üí° **Exemplo Num√©rico: Knowledge Distillation**
>
> Imagine que temos um modelo *dense* (professor) que produz embeddings de alta qualidade, mas √© computacionalmente caro. Queremos treinar um modelo *sparse* (aluno) que seja mais eficiente, mantendo o m√°ximo poss√≠vel da efetividade do professor.
>
> O modelo professor, ap√≥s ser alimentado com o "Documento 1: O gato est√° no tapete", gera o seguinte *embedding* *dense*:
>
> $E_{professor} = [0.1, 0.2, 0.8, 0.3, 0.5, 0.9, 0.2, 0.4]$
>
> O objetivo do *knowledge distillation* √© fazer com que o modelo aluno (sparse) aprenda a produzir uma representa√ß√£o *sparse* que se aproxime da distribui√ß√£o de probabilidade impl√≠cita na representa√ß√£o *dense* do professor. Por exemplo, for√ßar o modelo aluno a dar alta import√¢ncia aos termos que o modelo professor considera importantes.
>
> Suponha que o modelo aluno produza a seguinte representa√ß√£o *sparse* inicial:
>
> $S_{aluno} = \{gato: 0.1, tapete: 0.05 \}$
>
> Durante o treinamento, a fun√ß√£o de *loss* (e.g., *KL divergence*) ir√° penalizar as diferen√ßas entre a distribui√ß√£o de probabilidade criada a partir de $E_{professor}$ e $S_{aluno}$, for√ßando o modelo aluno a aumentar o peso dos termos relevantes (e.g., "gato", "tapete") e possivelmente adicionar outros termos importantes que o modelo professor considera relevantes, resultando em uma representa√ß√£o *sparse* mais informada.
>
> Por exemplo, ap√≥s algumas itera√ß√µes de treinamento, $S_{aluno}$ poderia evoluir para:
>
> $S_{aluno} = \{gato: 0.7, tapete: 0.6 \}$
>
> Demonstrando que o modelo aluno aprendeu a aumentar a import√¢ncia dos termos relevantes, imitando o comportamento do modelo professor.

**Proposi√ß√£o 1 (Import√¢ncia da Fun√ß√£o de Loss)** A escolha da fun√ß√£o de *loss* no treinamento de modelos LSR impacta diretamente a qualidade das representa√ß√µes *sparse* aprendidas. Uma fun√ß√£o de *loss* adequada deve promover a gera√ß√£o de representa√ß√µes que capturem a sem√¢ntica relevante e que sejam discriminativas o suficiente para distinguir entre documentos relevantes e irrelevantes para uma dada *query*.

*Prova (esbo√ßo)*: A fun√ß√£o de *loss* define o objetivo de otimiza√ß√£o durante o treinamento do modelo. Se a fun√ß√£o de *loss* n√£o refletir adequadamente o objetivo de aprendizado (i.e., gerar representa√ß√µes *sparse* que capturem a sem√¢ntica e discriminem documentos relevantes), o modelo pode convergir para uma solu√ß√£o sub√≥tima. Por exemplo, uma fun√ß√£o de *loss* baseada apenas na similaridade lexical pode n√£o capturar as nuances sem√¢nticas, enquanto uma fun√ß√£o de *loss* que n√£o penaliza a densidade da representa√ß√£o pode resultar em representa√ß√µes *sparse* ineficientes.

Nas se√ß√µes 5.1, 5.2 e 5.3 [^35], o documento explora as principais abordagens existentes nesses cen√°rios, respectivamente.

> üí° **Exemplo Num√©rico: Contrastive Learning**
>
> No contexto de Learned Sparse Retrieval, *contrastive learning* pode ser usado para treinar modelos a gerar representa√ß√µes *sparse* que s√£o semelhantes para *queries* e documentos relevantes, e diferentes para *queries* e documentos irrelevantes.
>
> Considere a *query* "Qual a capital da Fran√ßa?" e o documento relevante "Paris √© a capital da Fran√ßa." Criamos um par positivo (query, documento relevante) e um par negativo (query, documento irrelevante). Por exemplo, o documento irrelevante poderia ser "O clima em Londres √© chuvoso".
>
> *   Par Positivo: (*query*, *documento relevante*)
> *   Par Negativo: (*query*, *documento irrelevante*)
>
> O objetivo √© treinar o modelo LSR para maximizar a similaridade entre as representa√ß√µes *sparse* da *query* e do documento relevante (par positivo) e minimizar a similaridade entre as representa√ß√µes *sparse* da *query* e do documento irrelevante (par negativo).
>
> Suponha que as representa√ß√µes *sparse* iniciais sejam:
>
> *   $S_{query} = \{capital: 0.2, fran√ßa: 0.3\}$
> *   $S_{doc\_rel} = \{paris: 0.4, capital: 0.3, fran√ßa: 0.5\}$
> *   $S_{doc\_irr} = \{clima: 0.2, londres: 0.4\}$
>
> Uma fun√ß√£o de *loss* comum para *contrastive learning* √© a *hinge loss* ou *triplet loss*. A ideia √© definir uma margem e penalizar o modelo se a dist√¢ncia entre a *query* e o documento irrelevante for menor que a dist√¢ncia entre a *query* e o documento relevante, mais a margem.
>
> Durante o treinamento, o modelo ajustar√° os pesos dos termos nas representa√ß√µes *sparse* para satisfazer a fun√ß√£o de *loss*. Por exemplo, pode aumentar os pesos dos termos compartilhados entre a *query* e o documento relevante ("capital", "fran√ßa") e diminuir os pesos dos termos espec√≠ficos do documento irrelevante ("clima", "londres").
>
> Ap√≥s o treinamento, as representa√ß√µes *sparse* podem ser:
>
> *   $S_{query} = \{capital: 0.7, fran√ßa: 0.8\}$
> *   $S_{doc\_rel} = \{paris: 0.6, capital: 0.7, fran√ßa: 0.8\}$
> *   $S_{doc\_irr} = \{clima: 0.1, londres: 0.1\}$
>
> A similaridade entre $S_{query}$ e $S_{doc\_rel}$ ser√° maior que a similaridade entre $S_{query}$ e $S_{doc\_irr}$, demonstrando que o modelo aprendeu a associar a *query* com o documento relevante e dissoci√°-la do documento irrelevante.

Uma das abordagens mencionadas para *document expansion learning* √© o modelo DocT5Query. A imagem abaixo ilustra um exemplo dessa abordagem.

![Example of DocT5Query model generating related queries for document expansion.](./../images/image1.png)

### Conclus√£o

Learned Sparse Retrieval representa uma fronteira ativa na pesquisa de Information Retrieval, buscando combinar a for√ßa da representa√ß√£o sem√¢ntica das redes neurais com a efici√™ncia dos inverted indexes. As tr√™s abordagens principais exploradas (document expansion learning, impact score learning e sparse representation learning) oferecem diferentes caminhos para alcan√ßar esse objetivo, cada um com suas pr√≥prias vantagens e desafios. Ao aproveitar as arquiteturas *transformer* e outras t√©cnicas de aprendizado profundo, o LSR tem o potencial de melhorar significativamente a efetividade e a efici√™ncia dos sistemas de recupera√ß√£o de informa√ß√µes, abrindo caminho para uma nova gera√ß√£o de mecanismos de busca inteligentes.

Uma poss√≠vel dire√ß√£o futura para a pesquisa em LSR √© a explora√ß√£o de t√©cnicas de *quantization* e *pruning* para reduzir ainda mais o tamanho dos *inverted indexes* e acelerar o processamento de *query*. Al√©m disso, a investiga√ß√£o de m√©todos de aprendizado *online* e *transfer learning* pode permitir a adapta√ß√£o cont√≠nua dos modelos LSR a novos dados e dom√≠nios, melhorando sua robustez e generaliza√ß√£o.

### Refer√™ncias
[^5]: B√ºttcher, C., Clarke, C., and Cormack, G. V. 2010. Information Retrieval: Implementing and Evaluating Search Engines. The MIT Press.
[^9]: Cambazoglu, B. B. and Baeza-Yates, R. A. 2015. Scalability Challenges in Web Search Engines. Morgan & Claypool Publishers.
[^15]: Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. NAACL, pp. 4171‚Äì4186.
[^17]: Nogueira, R. and Cho, K. 2019. Passage Re-ranking with BERT. arXiv 1901.04085.
[^35]: Nicola Tonellotto. Lecture Notes on Neural Information Retrieval.
<!-- END -->