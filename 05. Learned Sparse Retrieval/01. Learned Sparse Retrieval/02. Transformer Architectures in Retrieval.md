## Transformer Architectures in Learned Sparse Retrieval

### Introdu√ß√£o
Como discutido anteriormente, os sistemas tradicionais de Information Retrieval (IR) se baseiam em representa√ß√µes esparsas, √≠ndices invertidos e fun√ß√µes de pontua√ß√£o l√©xicas, como o BM25 [^35]. Embora estes m√©todos apresentem um bom *trade-off* entre efic√°cia e efici√™ncia, os recentes avan√ßos em *neural Information Retrieval* (IR) t√™m demonstrado melhorias not√°veis na efic√°cia da pesquisa, utilizando representa√ß√µes densas de *queries* e documentos, embora com um custo computacional maior [^35]. Para conciliar estas vantagens, a √°rea de *Learned Sparse Retrieval* (LSR) surgiu com o objetivo de incorporar os benef√≠cios das redes neurais nos tradicionais √≠ndices invertidos [^35]. Este cap√≠tulo foca-se no papel fundamental que as arquiteturas *Transformer* desempenham no *Learned Sparse Retrieval* [^35].

### Arquiteturas Transformer no Learned Sparse Retrieval
As arquiteturas *Transformer* s√£o empregadas em diversos cen√°rios dentro do *Learned Sparse Retrieval* para otimizar o processo de recupera√ß√£o de informa√ß√£o [^35]. Em geral, o objetivo destas t√©cnicas √© melhorar a precis√£o e a efici√™ncia na identifica√ß√£o de documentos relevantes em resposta √†s *queries* dos usu√°rios, utilizando modelos treinados para modificar o conte√∫do dos documentos, estimar a relev√¢ncia dos termos e projetar os *embeddings* dos documentos num vocabul√°rio da cole√ß√£o [^35].

1.  **Modifica√ß√£o do Conte√∫do do Documento (Document Expansion Learning):**
    Modelos *sequence-to-sequence* s√£o utilizados para alterar o conte√∫do real dos documentos, refor√ßando as estat√≠sticas dos termos importantes e gerando novos termos a serem inclu√≠dos no documento [^35]. Esta t√©cnica visa resolver o problema de *vocabulary mismatch* [^36], onde as *queries* utilizam termos semanticamente similares, mas lexicalmente distintos dos utilizados nos documentos relevantes.

    > üí° **Exemplo Num√©rico:**
    > Suponha que temos um documento sobre "carros el√©tricos". Uma *query* como "ve√≠culos movidos a bateria" pode n√£o corresponder diretamente devido ao *vocabulary mismatch*. Atrav√©s do *document expansion*, o modelo pode adicionar termos como "ve√≠culo el√©trico", "carro a bateria", e "EV" ao documento.
    >
    > | Termo Original | Termos Adicionados |
    > |-----------------|----------------------|
    > | carros          | ve√≠culos             |
    > | el√©tricos      | bateria, EV          |

2.  **Estimativa da Relev√¢ncia do Termo (Impact Score Learning):**
    Os *embeddings* de sa√≠da dos documentos, fornecidos como *input* para modelos *encoder-only*, s√£o transformados por redes neurais para gerar um √∫nico valor real, que √© usado para estimar a contribui√ß√£o m√©dia de relev√¢ncia do termo no documento [^35]. Este valor, quantizado e armazenado nos *postings*, serve como um *proxy* para a relev√¢ncia do termo, permitindo uma avalia√ß√£o mais precisa da import√¢ncia de cada termo no contexto do documento.

    > üí° **Exemplo Num√©rico:**
    > Considere um documento contendo os termos "Transformer", "arquitetura", e "aten√ß√£o". Um modelo *encoder-only* pode gerar *embeddings* para cada termo. Uma rede neural ent√£o processa esses *embeddings* para produzir *impact scores*. Por exemplo:
    >
    > | Termo         | Embedding (Simplificado) | Impact Score |
    > |---------------|--------------------------|--------------|
    > | Transformer   | [0.8, 0.2, 0.5]           | 0.9          |
    > | arquitetura  | [0.1, 0.9, 0.3]           | 0.7          |
    > | aten√ß√£o       | [0.6, 0.4, 0.7]           | 0.8          |
    >
    > O *impact score* quantificado e armazenado no √≠ndice invertido reflete a relev√¢ncia de cada termo. Um *impact score* mais alto indica maior relev√¢ncia.

3.  **Proje√ß√£o de *Embeddings* no Vocabul√°rio da Cole√ß√£o (Sparse Representation Learning):**
    Os *embeddings* de sa√≠da dos documentos, fornecidos como *input* para modelos *encoder-only*, s√£o projetados com uma matriz aprendida no vocabul√°rio da cole√ß√£o, a fim de estimar os termos relevantes em um documento [^35]. Em ess√™ncia, esta t√©cnica aprende uma representa√ß√£o esparsa dos documentos, onde cada dimens√£o corresponde a um termo no vocabul√°rio, e o valor representa a import√¢ncia desse termo para o documento.

    > üí° **Exemplo Num√©rico:**
    > Suponha que o vocabul√°rio da cole√ß√£o contenha os termos: {"carro", "el√©trico", "bateria", "motor", "combust√£o"}. Um documento sobre "carros el√©tricos" teria um *embedding* projetado nesse vocabul√°rio. Ap√≥s a proje√ß√£o, ter√≠amos um vetor esparso:
    >
    > | Termo      | Valor |
    > |------------|-------|
    > | carro      | 0.8   |
    > | el√©trico   | 0.9   |
    > | bateria    | 0.7   |
    > | motor      | 0.2   |
    > | combust√£o  | 0.0   |
    >
    > Este vetor esparso indica que os termos "carro", "el√©trico", e "bateria" s√£o mais relevantes para este documento do que "motor" ou "combust√£o". Este vetor √© usado no √≠ndice invertido.

Para complementar a discuss√£o sobre as diferentes abordagens de utiliza√ß√£o de Transformers em LSR, podemos introduzir a no√ß√£o de *query expansion*, que, embora n√£o focada diretamente na modifica√ß√£o do documento, interage de forma complementar com as t√©cnicas apresentadas.

**Teorema 1** (Rela√ß√£o entre Document Expansion e Query Expansion): *Document expansion* e *query expansion* s√£o t√©cnicas complementares para mitigar o *vocabulary mismatch* no *Learned Sparse Retrieval*. Enquanto *document expansion* enriquece o conte√∫do dos documentos com termos relevantes adicionais, *query expansion* expande a *query* original do usu√°rio com sin√¥nimos, hiper√¥nimos ou termos relacionados, aumentando as chances de correspond√™ncia com documentos relevantes.

#### T√©cnicas Espec√≠ficas e Modelos
Para detalhar os m√©todos espec√≠ficos, podemos citar:

*   **Doc2Query e DocT5Query:** Estas abordagens utilizam arquiteturas *Transformer* para gerar novas *queries* para as quais um documento espec√≠fico ser√° relevante [^36]. Dado um conjunto de dados de pares *query*-documento relevantes, estes modelos s√£o treinados para prever *queries* relevantes para um determinado documento. Estas *queries* s√£o ent√£o adicionadas ao documento, expandindo o seu conte√∫do e aumentando a probabilidade de correspond√™ncia com *queries* futuras.

    > üí° **Exemplo Num√©rico:**
    > Um documento sobre "benef√≠cios do ch√° verde" pode gerar *queries* como "ch√° verde faz bem para sa√∫de?", "quais as vantagens do ch√° verde?", "ch√° verde emagrece?". Estas *queries* adicionadas ao documento melhoram sua capacidade de corresponder a pesquisas relacionadas.





![Example of DocT5Query model generating related queries for document expansion.](./../images/image1.png)

*   **TILDEv2:** Esta t√©cnica explora o modelo BERT para computar o *embedding* \[CLS] de um documento, projetando-o linearmente sobre todo o vocabul√°rio BERT [^36]. Desta forma, calcula uma distribui√ß√£o de probabilidade sobre o vocabul√°rio e adiciona ao documento um certo n√∫mero de novos termos correspondentes aos de maior probabilidade.

    > üí° **Exemplo Num√©rico:**
    > Ap√≥s calcular o *embedding* \[CLS] para um documento sobre "computa√ß√£o qu√¢ntica", a proje√ß√£o no vocabul√°rio pode resultar em altas probabilidades para os termos "quantum", "algoritmos", "bits", "superposi√ß√£o". Esses termos seriam adicionados ao documento.

*   **DeepCT:** Este sistema projeta as representa√ß√µes contextuais de palavras do BERT em novas frequ√™ncias de termos nos documentos, para uso com fun√ß√µes de *ranking* cl√°ssicas como o BM25 [^38]. Para cada termo $w_i \in V$ num dado documento, o DeepCT estima a sua import√¢ncia espec√≠fica de contexto $z_i \in \mathbb{R}$, que √© escalada e arredondada para um valor inteiro semelhante √† frequ√™ncia *tf*, que pode ser armazenado num √≠ndice invertido [^38].

    > üí° **Exemplo Num√©rico:**
    > Suponha que a palavra "ma√ß√£" apare√ßa em um documento. DeepCT pode determinar que em um contexto espec√≠fico, "ma√ß√£" se refere √† fruta, atribuindo-lhe um *impact score* de 3. Em outro documento, "ma√ß√£" pode se referir √† empresa Apple, recebendo um *impact score* de 5. Esses valores s√£o usados no BM25.

*   **DeepImpact:** Esta abordagem computa diretamente um *impact score* para cada termo √∫nico num documento, sem recorrer a fun√ß√µes de *ranking* cl√°ssicas [^38]. Estima o impacto espec√≠fico de contexto $z_i \in \mathbb{R}$ alimentando o modelo *encoder-only* com os *sub-word tokens* do documento.

    > üí° **Exemplo Num√©rico:**
    > Em um documento sobre "redes neurais convolucionais", o termo "convolucionais" pode receber um alto *impact score* de 0.8, enquanto o termo "redes" pode receber um *impact score* de 0.6. Esses valores s√£o usados diretamente para *ranking*.

*   **SPLADE:** Este modelo computa *per-token masked language heads* usando o BERT, filtra e soma estes vetores de tamanho de vocabul√°rio num √∫nico vetor $\gamma(d) \in \mathbb{R}^{|V|}$ representando todo o documento [^40].

    $$
    \gamma(d) = \sum_{i=1}^{|d|} \log(1 + ReLU(X_i))
    $$

    Onde a fun√ß√£o logaritmo e ReLU na Eq. (28) s√£o computadas *element-wise*; o logaritmo impede que alguns termos com grandes valores dominem, e a fun√ß√£o ReLU lida com os componentes negativos de $\gamma(d)$.

    > üí° **Exemplo Num√©rico:**
    > Suponha um documento com os *tokens* "gato", "preto", "dormindo". Ap√≥s passar pelo BERT e aplicar a Eq. (28), podemos obter o seguinte vetor $\gamma(d)$:
    >
    > | Termo      | Valor |
    > |------------|-------|
    > | gato       | 2.1   |
    > | preto      | 1.8   |
    > | dormindo   | 1.5   |
    > | cachorro   | 0.0   |
    > | correndo   | 0.0   |
    >
    > A esparsidade √© evidente, com "cachorro" e "correndo" tendo valores 0, indicando que n√£o s√£o relevantes para este documento.

Ap√≥s apresentar o modelo SPLADE, uma extens√£o natural √© considerar suas propriedades de esparsidade e como elas podem ser quantificadas e controladas.

**Teorema 1.1** (Controle da Esparsidade em SPLADE): A esparsidade da representa√ß√£o $\gamma(d)$ em SPLADE pode ser controlada ajustando-se o limiar da fun√ß√£o ReLU ou introduzindo uma penalidade de regulariza√ß√£o L1 durante o treinamento. Um limiar mais alto na ReLU resulta em mais valores zero em $\gamma(d)$, aumentando a esparsidade. Similarmente, uma penalidade L1 incentiva o modelo a gerar representa√ß√µes mais esparsas.

*Proof Strategy.* A prova baseia-se na an√°lise da fun√ß√£o ReLU e da penalidade L1. A fun√ß√£o ReLU, definida como $ReLU(x) = max(0, x)$, zera todos os valores negativos. Aumentar o limiar significa que valores menores precisam ser atingidos para serem diferentes de zero, aumentando a esparsidade. A penalidade L1, adicionada √† fun√ß√£o de perda durante o treinamento, penaliza a magnitude dos pesos, for√ßando muitos pesos a serem zero e, consequentemente, aumentando a esparsidade da representa√ß√£o $\gamma(d)$.

### Conclus√£o
Em resumo, as arquiteturas *Transformer* t√™m demonstrado ser ferramentas poderosas no contexto do *Learned Sparse Retrieval*, oferecendo mecanismos eficazes para modificar o conte√∫do dos documentos, estimar a relev√¢ncia dos termos e projetar os *embeddings* dos documentos no vocabul√°rio da cole√ß√£o [^35]. Estas t√©cnicas permitem melhorar significativamente a efic√°cia e a efici√™ncia dos sistemas de *Information Retrieval*, combinando as vantagens das representa√ß√µes esparsas com o poder expressivo das redes neurais profundas. A cont√≠nua investiga√ß√£o nesta √°rea promete avan√ßos ainda maiores na capacidade de encontrar informa√ß√£o relevante de forma precisa e eficiente.

### Refer√™ncias
[^35]: N. Tonellotto, *Lecture Notes on Neural Information Retrieval*, 2022.
[^36]: L. Zhao, *Modeling and solving term mismatch for full-text retrieval*. PhD thesis, Carnegie Mellon University.
[^38]: Z. Dai and J. Callan. 2020. Context-aware document term weighting for ad-hoc search. In Proc. WWW, pp. 1897‚Äì1907.
[^40]: T. Formal, B. Piwowarski, and S. Clinchant. 2021. SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking. In Proc. SIGIR, p. 2288‚Äì2292.
[^36]: R. Nogueira, W. Yang, J. Lin, and K. Cho. 2019b. Document expansion by query prediction. Preprint: arXiv:1904.08375.
<!-- END -->