## 5.3 Sparse Representation Learning: Enhancing Neural IR through Vocabulary-Based Techniques

### Introdu√ß√£o

Em continuidade aos m√©todos de *Learned Sparse Retrieval (LSR)*, esta se√ß√£o explora a t√©cnica de *sparse representation learning* [^35], que representa uma abordagem integrada para aprimorar a efic√°cia e a efici√™ncia dos sistemas de *Information Retrieval (IR)*. Diferentemente das estrat√©gias que aprendem independentemente a expandir documentos e, em seguida, aprendem a pontuar o impacto dos termos nos documentos expandidos, o *sparse representation learning* visa aprender ambos simultaneamente. Este m√©todo se baseia na proje√ß√£o dos *output embeddings* de modelos *encoder-only* para o vocabul√°rio de entrada, computando, para cada termo de entrada no documento, um modelo de linguagem, ou seja, uma distribui√ß√£o de probabilidade sobre todo o vocabul√°rio.

### Abordagem Central do Sparse Representation Learning

No seu n√∫cleo, o *sparse representation learning* projeta os *output embeddings* de um modelo *encoder-only* para o vocabul√°rio de entrada [^35]. Isso permite computar, para cada termo de entrada no documento, um modelo de linguagem, ou seja, uma distribui√ß√£o de probabilidade sobre todo o vocabul√°rio. Estes modelos de linguagem baseados em termos capturam as correla√ß√µes sem√¢nticas entre o termo de entrada e todos os outros termos na cole√ß√£o. Eles podem ser usados para:

1.  Expandir o texto de entrada com termos altamente correlacionados [^35].
2.  Comprimir o texto de entrada removendo termos com baixas probabilidades em rela√ß√£o aos outros termos [^35].

> üí° **Exemplo Num√©rico:** Imagine que o modelo identifica que, no contexto de um documento sobre "gatos", a palavra "felino" tem uma alta probabilidade. A expans√£o adicionaria "felino" ao documento para capturar melhor o t√≥pico. Por outro lado, se a palavra "irrelevante" tem uma probabilidade muito baixa, ela seria removida.

Modelos *encoder-only* como o BERT j√° computam modelos de linguagem baseados em termos como parte do seu treinamento como modelos de linguagem mascarados [^40]. Formalmente, dado um documento $d$, juntamente com os *output embeddings* $\Psi_{[CLS]}, \Psi_1, \dots, \Psi_{|d|}$, um modelo *encoder-only* tamb√©m retorna os *masked language heads* $X_1, \dots, X_{|d|}$, um para cada token no documento. Aqui, $X_i \in \mathbb{R}^{|V|}$ para $i = 1, \dots, |d|$ √© uma estimativa da import√¢ncia de cada palavra no vocabul√°rio impl√≠cita pelo *i-th token* no documento $d$ [^40].

Para complementar a compreens√£o dos *masked language heads*, podemos formalizar a no√ß√£o de probabilidade associada a cada palavra no vocabul√°rio.

**Defini√ß√£o 1.** Dado o *i-th token* em um documento $d$, a probabilidade $P(w|d_i)$ de uma palavra $w \in V$ ser relevante, condicionada ao *i-th token*, √© dada por:

$$
P(w|d_i) = \frac{\exp(X_i[w])}{\sum_{w' \in V} \exp(X_i[w'])}
$$

onde $X_i[w]$ denota o valor do elemento correspondente √† palavra $w$ no vetor $X_i$.

> üí° **Exemplo Num√©rico:** Suponha que para o token "gato" em um documento, o *masked language head* produza os seguintes valores (n√£o normalizados) para algumas palavras do vocabul√°rio:
>
> | Palavra    | Valor (X_i[w]) |
> |------------|-----------------|
> | gato       | 5.2             |
> | felino     | 4.8             |
> | cachorro   | 1.1             |
> | irrelevante | -2.5            |
>
> Primeiro, calculamos o denominador, a soma dos exponenciais de todos os valores do vocabul√°rio. Para simplificar, vamos considerar apenas as 4 palavras acima.
>
> $\sum_{w' \in V} \exp(X_i[w']) = \exp(5.2) + \exp(4.8) + \exp(1.1) + \exp(-2.5) \approx 181.27 + 121.51 + 3.00 + 0.08 = 305.86$
>
> Agora, podemos calcular $P(w|d_i)$ para cada palavra:
>
> $P(\text{gato}|d_i) = \frac{\exp(5.2)}{305.86} \approx \frac{181.27}{305.86} \approx 0.59$
>
> $P(\text{felino}|d_i) = \frac{\exp(4.8)}{305.86} \approx \frac{121.51}{305.86} \approx 0.40$
>
> $P(\text{cachorro}|d_i) = \frac{\exp(1.1)}{305.86} \approx \frac{3.00}{305.86} \approx 0.01$
>
> $P(\text{irrelevante}|d_i) = \frac{\exp(-2.5)}{305.86} \approx \frac{0.08}{305.86} \approx 0.0003$
>
> Observe como as palavras "gato" e "felino" t√™m probabilidades significativamente maiores do que "cachorro" e "irrelevante", refletindo sua relev√¢ncia no contexto do token "gato". Este √© um exemplo simplificado, mas ilustra como a probabilidade √© calculada.

Esta defini√ß√£o transforma o vetor $X_i$ em uma distribui√ß√£o de probabilidade sobre o vocabul√°rio, permitindo uma interpreta√ß√£o mais direta da import√¢ncia relativa de cada termo.

### Sistemas Pioneiros e Metodologias

Os sistemas EPIC [MacAvaney et al. 2020a] e SparTerm [Bai et al. 2020] foram os primeiros a focar na expans√£o baseada em vocabul√°rio e na estimativa de import√¢ncia, e inspiraram o sistema SPLADE [Formal et al. 2021], no qual focaremos [^40].

Para um dado documento $d \in D$, o SPLADE computa seus *masked language heads* por token $X_1, \dots, X_{|d|}$ usando BERT, filtra e soma esses vetores de tamanho de vocabul√°rio em um √∫nico vetor $\gamma(d) \in \mathbb{R}^{|V|}$ representando todo o documento [^40].

$$
\gamma(d) = \sum_{i=1}^{|d|} \log(1 + ReLU(X_i))
$$

onde as fun√ß√µes de logaritmo e ReLU na equa√ß√£o s√£o computadas elemento a elemento [^41]; o logaritmo impede que alguns termos com valores grandes dominem, e a fun√ß√£o ReLU lida com os componentes negativos de $\gamma(d)$ [^41].

> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, suponha que o documento $d$ contenha tr√™s tokens: "adoro", "meu", e "gato". Para simplificar, vamos considerar que os *masked language heads* $X_i$ para esses tokens j√° foram calculados (como no exemplo anterior) e que estamos focando apenas na palavra "gato" para ilustrar como $\gamma(d)$ √© computado. Assuma os seguintes valores (ap√≥s aplicar ReLU):
>
> | Palavra    | X_1 (adoro) | X_2 (meu) | X_3 (gato) |
> |------------|-------------|-----------|------------|
> | gato       | 0.1         | 0.2       | 5.2        |
> | felino     | 0.0         | 0.1       | 4.8        |
> | cachorro   | 0.0         | 0.0       | 1.1        |
> | irrelevante | 0.0         | 0.0       | 0.0        |
>
> Agora aplicamos $\log(1 + X_i)$ elemento a elemento:
>
> | Palavra    | log(1 + X_1) | log(1 + X_2) | log(1 + X_3) |
> |------------|--------------|--------------|--------------|
> | gato       | 0.095        | 0.182        | 1.825        |
> | felino     | 0.0          | 0.095        | 1.758        |
> | cachorro   | 0.0          | 0.0          | 0.742        |
> | irrelevante | 0.0          | 0.0          | 0.0          |
>
> Finalmente, somamos as colunas para obter $\gamma(d)$:
>
> | Palavra    | Œ≥(d)          |
> |------------|---------------|
> | gato       | 0.095 + 0.182 + 1.825 = 2.102 |
> | felino     | 0.0 + 0.095 + 1.758 = 1.853   |
> | cachorro   | 0.0 + 0.0 + 0.742 = 0.742     |
> | irrelevante | 0.0 + 0.0 + 0.0 = 0.0       |
>
> Portanto, $\gamma(d)$ representa a import√¢ncia agregada de cada palavra no documento ap√≥s a aplica√ß√£o do logaritmo e da ReLU. A palavra "gato" ainda tem a maior import√¢ncia, seguida por "felino", o que faz sentido dado o conte√∫do do documento.

A representa√ß√£o do documento $\gamma$ potencialmente cont√©m todos os termos no vocabul√°rio, mesmo que as fun√ß√µes logar√≠tmicas e ReLU possam zerar alguns de seus componentes [^41].

Para aprender a "esparsificar" as representa√ß√µes do documento, Formal et al. [2021] utilizam o regularizador FLOPS $L_{FLOPS}$ [Paria et al. 2020] [^41]. Como parte da fun√ß√£o de perda SPLADE usada durante o treinamento, a perda FLOPS √© computada como a soma, em todos os termos no vocabul√°rio, da probabilidade quadrada $p_t^2$ de que um termo $w$ tenha um peso diferente de zero em um documento [^41]. Minimizar a perda FLOPS coincide com minimizar os pesos diferentes de zero em um documento, ou seja, maximizar o n√∫mero de pesos zero em um documento [^41].

A opera√ß√£o quadrada ajuda a reduzir pesos de termos altos mais do que pesos de termos baixos. A probabilidade de que um termo $w \in V$ tenha um peso diferente de zero em um documento $d$ √© proporcional ao peso m√©dio daquele termo $\gamma_t(d)$ estimado em toda a cole√ß√£o [^41]. Para tornar a computa√ß√£o vi√°vel, a m√©dia √© computada em um lote $b$ de documentos durante o treinamento, considerado como uma amostra representativa de toda a cole√ß√£o:

$$
L_{FLOPS} = \sum_{t \in V} p_t^2 = \sum_{t \in V} \left(\frac{1}{|b|}\sum_{d \in b}\gamma_t(d)\right)^2
$$

> üí° **Exemplo Num√©rico:** Considere um lote $b$ de 2 documentos. Queremos calcular a perda $L_{FLOPS}$. Suponha que para a palavra "gato", os valores de $\gamma_t(d)$ nos dois documentos s√£o 2.102 e 1.853, respectivamente (calculados como no exemplo anterior). Ent√£o:
>
> $\frac{1}{|b|}\sum_{d \in b}\gamma_t(d) = \frac{2.102 + 1.853}{2} = \frac{3.955}{2} = 1.9775$
>
> Portanto, $p_{\text{gato}} = 1.9775$. A contribui√ß√£o da palavra "gato" para a perda FLOPS √© ent√£o:
>
> $p_{\text{gato}}^2 = (1.9775)^2 \approx 3.91$
>
> Este c√°lculo seria repetido para cada palavra no vocabul√°rio, e a soma de todos os $p_t^2$ resultaria na perda FLOPS para aquele lote de documentos. Minimizar essa perda durante o treinamento for√ßa o modelo a reduzir a magnitude dos pesos, promovendo a esparsidade.

Al√©m da perda FLOPS, podemos considerar a adi√ß√£o de uma perda que incentive a cobertura do vocabul√°rio, garantindo que o modelo n√£o se concentre excessivamente em um subconjunto limitado de termos.

**Teorema 1.** A adi√ß√£o de uma perda de entropia ao vetor $\gamma(d)$ promove uma distribui√ß√£o mais uniforme dos pesos dos termos, incentivando a cobertura do vocabul√°rio e potencialmente melhorando a generaliza√ß√£o do modelo.

*Prova.* A entropia de $\gamma(d)$ normalizado √© dada por:

$$
H(\gamma(d)) = - \sum_{t \in V} \hat{\gamma}_t(d) \log(\hat{\gamma}_t(d))
$$

onde $\hat{\gamma}_t(d) = \frac{\gamma_t(d)}{\sum_{t' \in V} \gamma_{t'}(d)}$ √© a probabilidade normalizada do termo $t$ no documento $d$. Maximizar $H(\gamma(d))$ for√ßa a distribui√ß√£o de probabilidade a ser mais uniforme, o que implica que o modelo atribui pesos n√£o negligenci√°veis a um conjunto maior de termos no vocabul√°rio. Combinada com a perda $L_{FLOPS}$, essa abordagem equilibra a esparsidade com a cobertura, potencialmente melhorando o desempenho do modelo.

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, onde $\gamma(d)$ para o documento tinha os valores 2.102 ("gato"), 1.853 ("felino"), 0.742 ("cachorro") e 0.0 ("irrelevante"), primeiro normalizamos esses valores para criar $\hat{\gamma}(d)$:
>
> $\sum_{t' \in V} \gamma_{t'}(d) = 2.102 + 1.853 + 0.742 + 0.0 = 4.697$
>
> $\hat{\gamma}_{\text{gato}}(d) = \frac{2.102}{4.697} \approx 0.447$
>
> $\hat{\gamma}_{\text{felino}}(d) = \frac{1.853}{4.697} \approx 0.395$
>
> $\hat{\gamma}_{\text{cachorro}}(d) = \frac{0.742}{4.697} \approx 0.158$
>
> $\hat{\gamma}_{\text{irrelevante}}(d) = \frac{0.0}{4.697} = 0.0$
>
> Agora calculamos a entropia:
>
> $H(\gamma(d)) = - (0.447 \log(0.447) + 0.395 \log(0.395) + 0.158 \log(0.158) + 0.0 \log(0.0))$
>
> $H(\gamma(d)) \approx - (0.447 \cdot (-0.349) + 0.395 \cdot (-0.403) + 0.158 \cdot (-0.801) + 0.0) \approx 0.156 + 0.159 + 0.127 = 0.442$
>
> Maximizar essa entropia durante o treinamento incentivaria o modelo a atribuir pesos maiores a termos menos frequentes, promovendo uma cobertura mais ampla do vocabul√°rio.

### Expans√£o e Compress√£o de Queries

SPLADE n√£o limita a expans√£o apenas a documentos [^41]. De fato, a Eq. 28 pode ser aplicada a uma *query* $q$ tamb√©m, para computar o vetor $\gamma(q) \in \mathbb{R}^{|V|}$ correspondente [^41]. No entanto, esta expans√£o de query deve ser realizada no tempo de processamento da query; para reduzir a lat√™ncia, a query expandida deve ser muito mais esparsa do que um documento. Para refor√ßar este comportamento diferente, Formal et al. [2021] adotam dois regularizadores FLOPS distintos para documentos e queries, ambos como na Eq. 29 [^41].

Para quantificar a esparsidade das representa√ß√µes de documentos e queries, podemos definir uma m√©trica que capture a propor√ß√£o de termos com pesos significativos.

**Defini√ß√£o 2.** A esparsidade $S(x)$ de um vetor $x \in \mathbb{R}^{|V|}$ √© definida como:

$$
S(x) = \frac{\|\mathbb{I}(|x_i| > \theta)\|_{0}}{|V|}
$$

onde $\mathbb{I}$ √© a fun√ß√£o indicadora, $x_i$ √© o *i-√©simo* elemento de $x$, $\theta$ √© um limiar (threshold) predefinido, e $\|\cdot\|_{0}$ denota a norma $L_0$, que conta o n√∫mero de elementos n√£o nulos. Um valor de $S(x)$ pr√≥ximo de 0 indica alta esparsidade, enquanto um valor pr√≥ximo de 1 indica baixa esparsidade.

> üí° **Exemplo Num√©rico:** Considere um vocabul√°rio de tamanho $|V| = 10000$. Temos dois vetores, um representando um documento ($d$) e outro representando uma query ($q$). Ap√≥s aplicar SPLADE, os vetores $\gamma(d)$ e $\gamma(q)$ t√™m as seguintes caracter√≠sticas:
>
> -   $\gamma(d)$: 500 termos com valores absolutos maiores que $\theta = 0.1$.
> -   $\gamma(q)$: 50 termos com valores absolutos maiores que $\theta = 0.1$.
>
> Calculamos a esparsidade para cada vetor:
>
> $S(d) = \frac{\|\mathbb{I}(|\gamma_i(d)| > 0.1)\|_{0}}{|V|} = \frac{500}{10000} = 0.05$
>
> $S(q) = \frac{\|\mathbb{I}(|\gamma_i(q)| > 0.1)\|_{0}}{|V|} = \frac{50}{10000} = 0.005$
>
> A esparsidade da query (0.005) √© menor que a esparsidade do documento (0.05), o que indica que a representa√ß√£o da query √© mais esparsa. Isso significa que a query tem menos termos considerados significativos, o que √© desej√°vel para reduzir a lat√™ncia durante a recupera√ß√£o.

Essa m√©trica permite comparar quantitativamente a esparsidade das representa√ß√µes de documentos e queries, oferecendo uma maneira de monitorar o efeito dos regularizadores FLOPS.

### Conclus√£o

O *sparse representation learning* emerge como uma estrat√©gia sofisticada para melhorar os sistemas de *Neural IR*. Ao integrar a expans√£o de documentos e o aprendizado de impacto de termos em um √∫nico processo de aprendizado, essa t√©cnica permite uma representa√ß√£o mais precisa e eficiente de documentos e queries [^35]. As funda√ß√µes te√≥ricas e experimentais para o *sparse representation learning* continuam a evoluir, prometendo ainda mais avan√ßos no futuro dos sistemas de *Information Retrieval* [^35].

### Refer√™ncias

[^35]: Se√ß√£o 5 Learned Sparse Retrieval
[^40]: Se√ß√£o 5.3 Sparse representation learning
[^41]: Se√ß√£o 5.3 Sparse representation learning
<!-- END -->