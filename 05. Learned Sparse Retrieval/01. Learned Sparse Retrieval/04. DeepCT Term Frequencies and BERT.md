### Introdu√ß√£o
Este cap√≠tulo aprofunda-se na √°rea de *learned sparse retrieval*, com foco espec√≠fico nas t√©cnicas de *impact score learning*. Como vimos anteriormente, os sistemas tradicionais de Information Retrieval (IR) dependem de representa√ß√µes esparsas, √≠ndices invertidos e fun√ß√µes de pontua√ß√£o baseadas em l√©xico, como o BM25 [^5]. No entanto, os modelos neurais abriram caminho para a utiliza√ß√£o de *embeddings* e para o aprendizado de representa√ß√µes mais sofisticadas. Nesta se√ß√£o, exploramos duas abordagens proeminentes que visam combinar o melhor dos dois mundos: o poder expressivo das redes neurais e a efici√™ncia dos √≠ndices invertidos. Especificamente, analisamos como os modelos DeepCT e DeepImpact empregam representa√ß√µes contextuais de palavras para aprender novas frequ√™ncias de termos em documentos, que s√£o ent√£o utilizadas com fun√ß√µes de *ranking* cl√°ssicas [^37]. Al√©m disso, podemos mencionar o uso de *knowledge graphs* para enriquecer as representa√ß√µes de termos e documentos, o que pode ser integrado a essas arquiteturas para melhorar ainda mais a precis√£o da recupera√ß√£o.

**Teorema 1** [Incorpora√ß√£o de *Knowledge Graphs*] A incorpora√ß√£o de informa√ß√µes de *knowledge graphs* nas representa√ß√µes de termos e documentos, juntamente com modelos como DeepCT e DeepImpact, pode levar a uma melhoria significativa na precis√£o da recupera√ß√£o, especialmente em dom√≠nios onde o conhecimento sem√¢ntico √© crucial.

*Estrat√©gia de Prova:* A prova envolveria demonstrar empiricamente que a inclus√£o de informa√ß√µes de *knowledge graphs* (por exemplo, usando *entity embeddings* ou rela√ß√µes extra√≠das de um grafo de conhecimento) como *features* adicionais nos modelos DeepCT e DeepImpact leva a um aumento nas m√©tricas de avalia√ß√£o de IR (como MAP, NDCG) em conjuntos de dados apropriados.

### DeepCT: Contextualiza√ß√£o para Refinamento de Frequ√™ncia de Termos
O DeepCT [Dai and Callan 2019b, 2020] [^38] representa um dos primeiros esfor√ßos para integrar representa√ß√µes contextuais de palavras aprendidas a partir de BERT para melhorar a estimativa da import√¢ncia de termos. A ideia central do DeepCT √© substituir ou aprimorar as frequ√™ncias de termos (TF) em documentos, que s√£o componentes cruciais para fun√ß√µes de *ranking* como o BM25. Em ess√™ncia, o DeepCT visa aprender uma representa√ß√£o contextualizada da import√¢ncia de cada termo em um documento, indo al√©m das contagens de ocorr√™ncias brutas [^38].

Para cada termo $w_i \in V$ (onde $V$ √© o vocabul√°rio) em um dado documento, o DeepCT estima sua import√¢ncia espec√≠fica ao contexto $z_i \in \mathbb{R}$. Este valor √© ent√£o escalado e arredondado para um valor inteiro semelhante √† frequ√™ncia, $tf_i$, que pode ser armazenado em um √≠ndice invertido. O processo √© formalizado da seguinte maneira: para cada documento $d \in D$, o DeepCT projeta as representa√ß√µes *l*-dimensionais $\psi_i$ para cada *token* BERT de entrada $w_i$ no documento (com $i = 1, \ldots, |d|$) em uma import√¢ncia escalar de termo usando a matriz aprendida $W \in \mathbb{R}^{1 \times l}$ [^38]:

$$
\begin{aligned}
[\psi_0, \psi_1, \ldots] &= \text{Encoder}(d) \\
z_i &= W\psi_i
\end{aligned}
$$

Onde *Encoder(d)* representa um modelo BERT que gera as representa√ß√µes contextuais. A arquitetura do DeepCT √© treinada com uma tarefa de regress√£o *per-token*, tentando prever a import√¢ncia dos termos [^38]. A import√¢ncia real do termo a ser prevista √© derivada do documento que cont√©m o termo ou de um conjunto de dados de pares de consulta-documento relevantes. Um termo que aparece em m√∫ltiplos documentos relevantes e em diferentes consultas tem uma import√¢ncia maior do que um termo que corresponde a um n√∫mero menor de documentos e/ou consultas.

Uma nuance importante √© como o DeepCT lida com *tokens* de subpalavras do BERT. O modelo usa a import√¢ncia do primeiro *token* de subpalavra para a palavra inteira [^38]. Al√©m disso, quando um termo ocorre v√°rias vezes em um documento, o DeepCT usa a import√¢ncia m√°xima entre as m√∫ltiplas ocorr√™ncias.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos o documento "O gato preto est√° sentado no tapete." e o modelo BERT tokeniza "gato" como "ga" e "##to". Digamos que ap√≥s passar pelo *Encoder(d)* e pela matriz *W*, obtemos os seguintes valores de import√¢ncia:
>
> *   psi("ga") = 0.8
> *   psi("##to") = 0.6
> *   psi("preto") = 0.9
>
> DeepCT usar√° 0.8 como a import√¢ncia do termo "gato" porque "ga" √© o primeiro subtoken. Se "gato" aparecer duas vezes no documento, e a segunda ocorr√™ncia tivesse psi("ga") = 0.7, DeepCT ainda usaria 0.8 (o m√°ximo).
>
> Isso demonstra como o DeepCT agrega subpalavras e m√∫ltiplas ocorr√™ncias para determinar a import√¢ncia final do termo.

**Lema 1** [Agrega√ß√£o de Subpalavras] Utilizar a import√¢ncia m√°xima entre os *tokens* de subpalavra para representar a import√¢ncia de uma palavra inteira √© uma heur√≠stica eficaz que captura a relev√¢ncia contextual do termo, mesmo que ela se manifeste em apenas uma das subpalavras.

*Estrat√©gia de Prova:* A validade desta heur√≠stica pode ser avaliada empiricamente comparando seu desempenho com outras estrat√©gias de agrega√ß√£o, como a m√©dia ou a soma das import√¢ncias das subpalavras.

### DeepImpact: Impacto Contextualizado Direto do Termo
O DeepImpact [Mallia et al. 2021] [^39] prop√µe uma abordagem diferente, computando diretamente uma pontua√ß√£o de impacto para cada termo √∫nico em um documento, sem recorrer a fun√ß√µes de *ranking* cl√°ssicas. Em vez disso, ele simplesmente soma os impactos dos termos de consulta que aparecem em um documento no momento do processamento da consulta para computar sua pontua√ß√£o de relev√¢ncia.

Para cada termo $w_i \in V$ em um dado documento $d \in D$, o DeepImpact estima seu impacto espec√≠fico ao contexto $z_i \in \mathbb{R}$. O DeepImpact alimenta o modelo *encoder-only* com os *tokens* de subpalavras do documento, produzindo um *embedding* para cada *token* de entrada. Uma camada de *gating* n√£o aprendida Mask remove os *embeddings* dos *tokens* de subpalavra que n√£o correspondem ao primeiro *token* de subpalavra da palavra inteira. Em seguida, o DeepImpact transforma as representa√ß√µes *l*-dimensionais restantes com duas redes *feed forward* com ativa√ß√µes ReLU [^39]:

$$
\begin{aligned}
[\psi_0, \psi_1, \ldots] &= \text{Encoder}(\text{DocT5Query}(d)) \\
[x_0, x_1, \ldots] &= \text{Mask}(\psi_0, \psi_1, \ldots) \\
y_i &= \text{ReLU}(W_1 x_i) \\
z_i &= \text{ReLU}(W_2 y_i)
\end{aligned}
$$
Onde $W_1 \in \mathbb{R}^{l \times l}$ e $W_2 \in \mathbb{R}^{1 \times l}$ s√£o matrizes de peso aprendidas.

![Example of DocT5Query model generating related queries for document expansion.](./../images/image1.png)

Os n√∫meros reais de sa√≠da $z_i$, com $i = 1, \ldots, |d|$, um por palavra inteira no documento de entrada, s√£o ent√£o quantizados linearmente em inteiros de 8 bits que podem ser armazenados em um √≠ndice invertido [^39]. Isso produz uma pontua√ß√£o de valor √∫nico para cada termo √∫nico no documento, representando seu impacto. Dada uma consulta *q*, a pontua√ß√£o do documento *d* √© simplesmente a soma dos impactos para a interse√ß√£o de termos em *q* e *d*. O DeepImpact √© treinado com triplos consulta, documento relevante e documento n√£o relevante e, para cada triplo, duas pontua√ß√µes para os dois documentos correspondentes s√£o computadas [^39]. O modelo √© otimizado via perda de entropia cruzada *pairwise* nas pontua√ß√µes dos documentos.

> üí° **Exemplo Num√©rico:**
>
> Suponha que, ap√≥s o `Encoder(DocT5Query(d))` e a aplica√ß√£o das camadas Mask, ReLU e *W*, temos os seguintes impactos para as palavras no documento "O gato preto est√° sentado":
>
> | Termo   | Impacto (zi) |
> | ------- | ------------ |
> | O       | 0.1          |
> | gato    | 1.5          |
> | preto   | 2.0          |
> | est√°    | 0.2          |
> | sentado | 0.8          |
>
> Agora, digamos que a consulta seja "gato preto". A pontua√ß√£o do documento para esta consulta seria a soma dos impactos dos termos "gato" e "preto", ou seja, 1.5 + 2.0 = 3.5. Este valor seria ent√£o usado para ranquear este documento em rela√ß√£o a outros.
>
> Note que os valores de impacto s√£o, na pr√°tica, quantizados para inteiros de 8 bits. Por exemplo, um impacto de 1.5 poderia ser quantizado para 2, dependendo do esquema de quantiza√ß√£o.

**Teorema 2** [Quantiza√ß√£o e Efici√™ncia] A quantiza√ß√£o linear dos impactos dos termos em inteiros de 8 bits no DeepImpact representa um compromisso eficaz entre a precis√£o da representa√ß√£o e a efici√™ncia do armazenamento e da computa√ß√£o.

*Estrat√©gia de Prova:* Este teorema pode ser suportado por experimentos que comparam o desempenho do DeepImpact com diferentes n√≠veis de quantiza√ß√£o (por exemplo, 16 bits, 4 bits) e com representa√ß√µes de ponto flutuante. A prova envolveria analisar a degrada√ß√£o da precis√£o da recupera√ß√£o em rela√ß√£o √† redu√ß√£o no tamanho do √≠ndice e ao aumento na velocidade de computa√ß√£o.

**Corol√°rio 2.1** A quantiza√ß√£o linear com diferentes granularidades pode ser adaptada dinamicamente com base na import√¢ncia do termo, melhorando o equil√≠brio entre precis√£o e efici√™ncia.

### Compara√ß√£o e Contraste
Ambos DeepCT e DeepImpact aproveitam modelos *transformer* para aprimorar a representa√ß√£o da import√¢ncia do termo, mas eles divergem em v√°rios aspectos cr√≠ticos:
*   **Objetivo**: DeepCT visa aprimorar ou substituir as frequ√™ncias de termos existentes para uso com fun√ß√µes de *ranking* cl√°ssicas como BM25, enquanto DeepImpact visa computar pontua√ß√µes de impacto diretamente que s√£o somadas para gerar a pontua√ß√£o do documento [^38, 39].
*   **Arquitetura**: DeepCT utiliza uma tarefa de regress√£o *per-token* para prever a import√¢ncia do termo, enquanto DeepImpact usa uma arquitetura *feed forward* para computar pontua√ß√µes de impacto e √© treinado com perda de entropia cruzada *pairwise* [^38, 39].
*   **Tokens de Subpalavras**: DeepCT atribui a import√¢ncia do primeiro *token* de subpalavra para toda a palavra, enquanto DeepImpact usa uma camada de *gating* n√£o aprendida para mascarar *embeddings* de *tokens* de subpalavras adicionais que n√£o correspondem ao primeiro sub*token* da palavra inteira [^38, 39].

> üí° **Exemplo Num√©rico:**
>
> Considere o seguinte cen√°rio para ilustrar as diferen√ßas na pontua√ß√£o:
>
> *   **Documento:** "O carro vermelho corre r√°pido."
> *   **Consulta:** "carro r√°pido"
>
> Suponha que o DeepCT, ap√≥s an√°lise contextual, modifique a frequ√™ncia dos termos da seguinte forma:
>
> | Termo      | TF Original | TF DeepCT |
> | ---------- | ------------- | ------------- |
> | O          | 1             | 0             |
> | carro      | 1             | 2             |
> | vermelho   | 1             | 0.5           |
> | corre      | 1             | 1             |
> | r√°pido     | 1             | 1.8           |
>
> O BM25 usaria esses valores TF aprimorados para calcular a relev√¢ncia.
>
> Por outro lado, o DeepImpact diretamente calcula um impacto, por exemplo:
>
> | Termo      | Impacto |
> | ---------- | ------- |
> | carro      | 3.0     |
> | r√°pido     | 2.5     |
>
> A pontua√ß√£o final do DeepImpact seria simplesmente 3.0 + 2.5 = 5.5.

**Proposi√ß√£o 1** [Complementaridade dos Modelos] A combina√ß√£o das abordagens DeepCT e DeepImpact, por exemplo, usando as frequ√™ncias de termos aprimoradas pelo DeepCT como entrada para o c√°lculo do impacto do DeepImpact, pode resultar em um sistema de IR ainda mais eficaz.

*Estrat√©gia de Prova:* A prova envolveria a implementa√ß√£o de um sistema h√≠brido que combina as duas abordagens e demonstrar empiricamente que ele supera tanto o DeepCT quanto o DeepImpact em m√©tricas de avalia√ß√£o de IR.

### Conclus√£o
O DeepCT e o DeepImpact representam abordagens inovadoras para integrar o poder das redes neurais na estrutura tradicional dos sistemas de *Information Retrieval* baseados em √≠ndice invertido [^38, 39]. Ao aprender a import√¢ncia do termo aprimorada contextualizada, esses modelos visam preencher a lacuna entre a precis√£o dos modelos neurais densos e a efici√™ncia das representa√ß√µes esparsas. As extens√µes propostas, como a incorpora√ß√£o de *knowledge graphs* e a combina√ß√£o das duas abordagens, t√™m o potencial de levar a avan√ßos ainda maiores na √°rea de *learned sparse retrieval*.
<!-- END -->