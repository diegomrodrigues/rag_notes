## 5.1.1 Abordagens Neurais para Expans√£o de Documentos: Superando o Desafio do Descompasso Vocabular

### Introdu√ß√£o
O problema do **descompasso vocabular** (*vocabulary mismatch problem*) em Information Retrieval (IR) surge quando as *queries* formuladas pelos usu√°rios empregam termos semanticamente similares, mas lexicalmente distintos daqueles presentes nos documentos relevantes [^36]. Em outras palavras, a maneira como um usu√°rio expressa sua necessidade de informa√ß√£o pode n√£o corresponder diretamente √† linguagem utilizada nos documentos que atendem a essa necessidade. As estrat√©gias tradicionais para mitigar esse problema concentravam-se na **expans√£o da *query***, incorporando termos relacionados para aumentar a probabilidade de correspond√™ncia com os documentos relevantes. Exemplos not√≥rios incluem a t√©cnica de *relevance feedback* [^Gey1994] e *pseudo relevance feedback* [^Lavrenko and Croft 2001]. No entanto, o avan√ßo das redes neurais e do Processamento de Linguagem Natural (NLP) abriu caminho para uma abordagem complementar: a **expans√£o do documento** (*document expansion*), que visa enriquecer o conte√∫do dos documentos com novos termos aprendidos, tornando-os mais robustos e resilientes √†s varia√ß√µes vocabulares nas *queries*.

**Proposi√ß√£o 1.** *A expans√£o do documento √© particularmente eficaz em cen√°rios onde a linguagem utilizada nos documentos √© mais t√©cnica ou formal, enquanto as queries dos usu√°rios tendem a ser mais coloquiais.*

*Prova.* Isso decorre do fato de que a expans√£o neural permite que o documento "aprenda" e incorpore termos mais comuns e informais que os usu√°rios podem empregar, sem alterar a representa√ß√£o original do documento. Al√©m disso, a expans√£o pode incluir sin√¥nimos e varia√ß√µes de termos t√©cnicos.

> üí° **Exemplo Num√©rico:** Considere um documento t√©cnico sobre "Otimiza√ß√£o de Redes Neurais Convolucionais". Um usu√°rio pode procurar por "melhorar CNN". A expans√£o do documento adicionaria termos como "aprimorar", "performance", "efici√™ncia", e "redes convolucionais", aumentando a chance de correspond√™ncia.

### Superando o Descompasso Vocabular atrav√©s da Expans√£o Neural de Documentos

A expans√£o neural de documentos se prop√µe a enriquecer o conte√∫do dos documentos, adicionando novos termos aprendidos, tornando-os mais resilientes √†s varia√ß√µes vocabulares nas *queries* [^36]. Duas abordagens seminais que demonstraram o potencial das arquiteturas *transformer* para essa finalidade s√£o o **Doc2Query** [^Nogueira et al. 2019b] e o **DocT5Query** [^Nogueira and Lin 2019]. Ambas as t√©cnicas compartilham o objetivo comum de gerar novas *queries* para as quais um documento espec√≠fico ser√° relevante.

Em ess√™ncia, Doc2Query e DocT5Query exploram a capacidade dos modelos *sequence-to-sequence* para sintetizar *queries* que capturam os aspectos essenciais do conte√∫do de um documento. Atrav√©s do treinamento em um *dataset* de pares *query*-documento relevantes, esses modelos aprendem a mapear um determinado documento para o espa√ßo das *queries* relevantes. Uma vez *fine-tuned*, o modelo pode ser usado para predizer novas *queries* usando uma t√©cnica de amostragem aleat√≥ria (*top k random sampling*) [^Fan et al. 2018a]. Essas *queries* preditas s√£o, ent√£o, anexadas ao documento original antes da indexa√ß√£o, efetivamente expandindo seu vocabul√°rio e aumentando suas chances de correspond√™ncia com *queries* futuras. A Figura 10 [^37] ilustra um exemplo desse processo.

![Example of DocT5Query model generating related queries for document expansion.](./../images/image1.png)

> üí° **Exemplo Num√©rico:** Um documento sobre "Doen√ßas Cardiovasculares" poderia gerar queries como "problemas de cora√ß√£o", "sa√∫de cardiovascular", "tratamento de doen√ßas card√≠acas" usando Doc2Query. Estas *queries* s√£o ent√£o anexadas ao documento original, tornando-o mais f√°cil de encontrar quando um usu√°rio pesquisa usando uma dessas varia√ß√µes.

Para complementar essas abordagens, podemos considerar estrat√©gias de *data augmentation* no treinamento dos modelos Doc2Query e DocT5Query.

**Teorema 1.** O desempenho dos modelos Doc2Query e DocT5Query pode ser aprimorado atrav√©s de t√©cnicas de *data augmentation* que introduzem varia√ß√µes nas *queries* de treinamento, simulando o descompasso vocabular.

*Prova.* A prova repousa no princ√≠pio de que modelos treinados com um conjunto de dados mais diverso e representativo da varia√ß√£o lingu√≠stica apresentar√£o melhor generaliza√ß√£o. T√©cnicas como substitui√ß√£o de sin√¥nimos, reformula√ß√£o de frases e adi√ß√£o de ru√≠do podem ser aplicadas √†s *queries* de treinamento para aumentar a robustez dos modelos em face do descompasso vocabular.

> üí° **Exemplo Num√©rico:** Uma *query* de treinamento "Como baixar o TensorFlow" pode ser aumentada para "Instalar TensorFlow", "Guia de instala√ß√£o TensorFlow", "TensorFlow download". O modelo aprende a associar estas diferentes *queries* ao mesmo documento, tornando-o mais robusto.

### TILDEv2 e SparTerm: Modelagem da Import√¢ncia dos Termos no Documento

Uma abordagem alternativa, personificada pelo **TILDEv2** [^Zhuang and Zuccon 2021b], abandona a gera√ß√£o direta de *queries*. Em vez disso, o TILDEv2 se concentra em computar a import√¢ncia de todos os termos no vocabul√°rio com rela√ß√£o a um determinado documento, selecionando os termos mais importantes para enriquecer o documento. O TILDEv2 emprega o modelo BERT [^Devlin et al. 2019] para gerar um *embedding* do documento, representado pelo *token* especial [CLS]. Esse *embedding* √©, ent√£o, linearmente projetado sobre todo o vocabul√°rio BERT, produzindo uma distribui√ß√£o de probabilidade sobre os termos. Os termos com as maiores probabilidades s√£o adicionados ao documento.

**Lema 2.** *A efic√°cia do TILDEv2 depende criticamente da qualidade do embedding gerado pelo modelo BERT.*

*Prova.* Dado que a distribui√ß√£o de probabilidade sobre os termos do vocabul√°rio √© diretamente derivada da proje√ß√£o linear do *embedding* do documento, a capacidade do modelo em capturar as nuances sem√¢nticas do documento √© fundamental. Um *embedding* de baixa qualidade ou que n√£o represente adequadamente o conte√∫do do documento resultar√° em uma distribui√ß√£o de probabilidade imprecisa e, consequentemente, na sele√ß√£o de termos irrelevantes para a expans√£o.

> üí° **Exemplo Num√©rico:** Suponha que o BERT embedding do documento "Impacto da Intelig√™ncia Artificial na Medicina" seja o vetor $v = [0.1, 0.2, -0.1, 0.3, \ldots]$. A proje√ß√£o linear deste vetor sobre o vocabul√°rio resulta em scores para cada termo. Digamos que os termos "IA", "sa√∫de", "diagn√≥stico", "tratamento" recebam os scores mais altos (e.g., 0.8, 0.7, 0.6, 0.5, respectivamente). Esses termos seriam ent√£o adicionados ao documento.

O **SparTerm** [^Bai et al. 2020] adota uma estrat√©gia similar, modelando a import√¢ncia dos termos no documento. No entanto, em vez de usar um √∫nico *embedding* [CLS], o SparTerm computa um modelo de linguagem para cada *token* de sa√≠da do BERT, incluindo o [CLS]. Esses modelos de linguagem individuais s√£o somados para criar uma distribui√ß√£o de import√¢ncia de termos sobre o vocabul√°rio para o documento. Finalmente, um mecanismo de *gating* aprendido seleciona um subconjunto esparso desses termos para compor o conte√∫do do documento expandido.

**Teorema 2.1.** A utiliza√ß√£o de m√∫ltiplos modelos de linguagem no SparTerm, em vez de um √∫nico *embedding* como no TILDEv2, permite uma representa√ß√£o mais rica e contextualizada da import√¢ncia dos termos no documento.

*Prova.* Ao considerar o contexto de cada *token* individualmente, o SparTerm √© capaz de capturar rela√ß√µes sem√¢nticas mais sutis e nuances contextuais que seriam perdidas em uma representa√ß√£o baseada em um √∫nico *embedding* global. A soma dos modelos de linguagem individuais permite ponderar a import√¢ncia de cada termo com base em sua relev√¢ncia em diferentes partes do documento, resultando em uma expans√£o mais precisa e relevante. Al√©m disso, o mecanismo de *gating* esparso contribui para reduzir a inclus√£o de termos ruidosos ou irrelevantes, refinando ainda mais o processo de expans√£o.

> üí° **Exemplo Num√©rico:** Considere a frase "Redes Neurais s√£o usadas para Vis√£o Computacional". SparTerm criaria embeddings para cada token ("Redes", "Neurais", "s√£o", "usadas", "para", "Vis√£o", "Computacional"). Cada embedding √© ent√£o usado para prever uma distribui√ß√£o de probabilidade sobre o vocabul√°rio. Por exemplo, o embedding de "Vis√£o" pode dar alta probabilidade para "reconhecimento", "imagem", "detec√ß√£o". O mecanismo de gating esparso ent√£o seleciona os termos mais relevantes de todas as distribui√ß√µes para adicionar ao documento.

<!-- END -->