## Document Expansion via Transformer-Based Query Generation

### Introdu√ß√£o
Como discutido anteriormente, a representa√ß√£o de textos √© um passo fundamental em sistemas de Information Retrieval (IR) [^8]. A t√©cnica de *document expansion* visa melhorar a representa√ß√£o de documentos, mitigando o problema do *vocabulary mismatch* [^36]. Este problema ocorre quando as consultas dos usu√°rios empregam termos semanticamente semelhantes, mas lexicalmente distintos dos termos presentes nos documentos relevantes. Em continuidade com as representa√ß√µes esparsas e densas apresentadas no cap√≠tulo anterior, exploraremos como a expans√£o de documentos, impulsionada por arquiteturas Transformer, enriquece o conte√∫do textual, focando especificamente na gera√ß√£o de novas *queries*.

> üí° **Exemplo Num√©rico:** Imagine um documento sobre "c√£es" e uma consulta sobre "cachorros". Sem document expansion, o sistema pode n√£o identificar a relev√¢ncia se a palavra "cachorro" n√£o estiver presente no documento. Document expansion visa adicionar termos como "cachorros", "caninos" e "pets" ao documento para melhorar a recupera√ß√£o.

### Gera√ß√£o de Queries Relevantes com Transformers

Uma abordagem inovadora para a expans√£o de documentos reside na utiliza√ß√£o de arquiteturas Transformer para gerar novas consultas (*queries*) para as quais um dado documento ser√° considerado relevante [^36]. Essa estrat√©gia √© exemplificada pelos modelos **Doc2Query** [Nogueira et al. 2019b] e **DocT5Query** [Nogueira e Lin 2019], que demonstram como os Transformers podem ser empregados para aumentar o conte√∫do informacional dos documentos atrav√©s da cria√ß√£o de *queries* suplementares.

**Doc2Query e DocT5Query** compartilham o objetivo de gerar *queries* sint√©ticas que capturem a ess√™ncia do conte√∫do de um documento. A metodologia geral envolve o treinamento de um modelo *sequence-to-sequence* para traduzir um documento em *queries* representativas. O treinamento √© realizado com base em um conjunto de dados que consiste em pares de *queries* e documentos relevantes.

**Abordagem Doc2Query:**

O modelo Doc2Query [Nogueira et al. 2019b] adota uma arquitetura Transformer *sequence-to-sequence* gen√©rica [Vaswani et al. 2017]. O processo de treinamento consiste em *fine-tuning* do modelo, utilizando pares de *queries* e documentos relevantes. Dado um documento, o modelo √© treinado para gerar *queries* que representem efetivamente o conte√∫do informacional do documento.

**Abordagem DocT5Query:**

O modelo DocT5Query [Nogueira e Lin 2019] utiliza o modelo T5 [Raffel et al. 2020], que tamb√©m √© baseado na arquitetura Transformer, como sua base. De forma an√°loga ao Doc2Query, o DocT5Query √© submetido a um processo de *fine-tuning* utilizando pares de *queries* e documentos relevantes. O modelo recebe o documento como entrada e gera a *query* correspondente.

**Processo de Enriquecimento do Documento:**

Ap√≥s o *fine-tuning*, ambos os modelos (Doc2Query e DocT5Query) s√£o utilizados para prever novas *queries* com base no conte√∫do dos documentos. O processo de previs√£o envolve o uso de t√©cnicas de amostragem, como o *top-k random sampling* [Fan et al. 2018a], para gerar um conjunto diversificado de *queries*. Essas *queries* geradas s√£o, ent√£o, anexadas ao documento original antes da indexa√ß√£o, enriquecendo, assim, sua representa√ß√£o [^36].

> üí° **Exemplo Num√©rico:** Suponha que um documento sobre "vacina√ß√£o infantil" gere as seguintes *queries* usando DocT5Query: "benef√≠cios da vacina√ß√£o infantil", "efeitos colaterais da vacina infantil", "calend√°rio de vacina√ß√£o para crian√ßas". Essas *queries* s√£o adicionadas ao documento, expandindo seu conte√∫do.

![Example of DocT5Query model generating related queries for document expansion.](./../images/image1.png)

A escolha do valor de *k* no *top-k random sampling* √© crucial. Um valor muito baixo pode resultar em pouca diversidade nas *queries* geradas, enquanto um valor muito alto pode introduzir ru√≠do e diminuir a precis√£o.

Para complementar a discuss√£o sobre o processo de amostragem, podemos introduzir um resultado que estabelece um limite superior para a probabilidade de selecionar uma *query* de baixa qualidade.

**Teorema 1**
Seja $Q$ o conjunto de todas as *queries* geradas pelo modelo ap√≥s o *fine-tuning*, e seja $Q_{ruim} \subset Q$ o subconjunto de *queries* consideradas de baixa qualidade. Seja $p$ a probabilidade de selecionar uma *query* de $Q_{ruim}$ usando o *top-k random sampling*. Ent√£o, $p \leq \frac{|Q_{ruim}|}{k}$, onde $|Q_{ruim}|$ √© a cardinalidade de $Q_{ruim}$.

*Prova:*
No *top-k random sampling*, selecionamos aleatoriamente uma *query* dentre as *k* melhores *queries* geradas. A probabilidade de selecionar uma *query* de baixa qualidade √© m√°xima quando todas as *queries* de $Q_{ruim}$ est√£o entre as *k* melhores. Neste caso, a probabilidade √© dada por $\frac{|Q_{ruim} \cap Q_{topk}|}{k}$, onde $Q_{topk}$ √© o conjunto das *k* melhores *queries*. Portanto, $p \leq \frac{|Q_{ruim}|}{k}$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se o modelo gera 100 *queries* ($|Q| = 100$) e 20 s√£o consideradas de baixa qualidade ($|Q_{ruim}| = 20$) e usarmos *top-k random sampling* com $k=10$, ent√£o $p \leq \frac{20}{10} = 2$.  Esta desigualdade n√£o √© √∫til pois $p$ √© uma probabilidade e deve ser menor ou igual a 1. Se usarmos $k=30$, ent√£o $p \leq \frac{20}{30} \approx 0.67$. Isso significa que h√° uma probabilidade m√°xima de 67% de selecionar uma *query* de baixa qualidade usando o *top-k random sampling* com $k=30$. Reduzir $|Q_{ruim}|$ por melhorar o modelo de gera√ß√£o de *queries* √© mais efetivo do que aumentar $k$.

Al√©m disso, √© importante considerar alternativas ao *top-k random sampling*. Uma op√ß√£o √© usar *temperature sampling*, que modula a distribui√ß√£o de probabilidade das *queries* geradas antes da amostragem.

**Lema 1**
Seja $P(q)$ a probabilidade de gerar a *query* $q$ pelo modelo Transformer. No *temperature sampling*, a probabilidade modificada $P_T(q)$ √© dada por:

$$P_T(q) = \frac{exp(\frac{log(P(q))}{T})}{\sum_{q' \in Q} exp(\frac{log(P(q'))}{T})}$$

onde $T$ √© a temperatura. Valores mais altos de $T$ tornam a distribui√ß√£o mais uniforme, aumentando a diversidade das *queries* amostradas, enquanto valores mais baixos tornam a distribui√ß√£o mais concentrada nas *queries* de maior probabilidade.

> üí° **Exemplo Num√©rico:** Suponha que um modelo gere duas *queries*, $q_1$ e $q_2$, com probabilidades $P(q_1) = 0.8$ e $P(q_2) = 0.2$. Se usarmos *temperature sampling* com $T=1$, as probabilidades modificadas ser√£o:
>
> $P_T(q_1) = \frac{exp(log(0.8)/1)}{exp(log(0.8)/1) + exp(log(0.2)/1)} = \frac{0.8}{0.8+0.2} = 0.8$
>
> $P_T(q_2) = \frac{exp(log(0.2)/1)}{exp(log(0.8)/1) + exp(log(0.2)/1)} = \frac{0.2}{0.8+0.2} = 0.2$
>
> Se aumentarmos a temperatura para $T=2$, as probabilidades se tornar√£o mais uniformes:
>
> $P_T(q_1) = \frac{exp(log(0.8)/2)}{exp(log(0.8)/2) + exp(log(0.2)/2)} \approx \frac{0.894}{0.894 + 0.447} \approx 0.667$
>
> $P_T(q_2) = \frac{exp(log(0.2)/2)}{exp(log(0.8)/2) + exp(log(0.2)/2)} \approx \frac{0.447}{0.894 + 0.447} \approx 0.333$
>
> Aumentar a temperatura de $T=1$ para $T=2$ aumentou a probabilidade de amostragem da *query* menos prov√°vel ($q_2$), tornando a amostragem mais diversa.

**Vantagens:**

*   **Mitiga√ß√£o do *vocabulary mismatch*:** As *queries* geradas podem incluir termos que n√£o est√£o presentes no documento original, mas que s√£o semanticamente relevantes, aumentando a probabilidade de correspond√™ncia com as *queries* dos usu√°rios.
*   **Melhora da recupera√ß√£o:** Ao expandir o documento com *queries* relevantes, o sistema de IR pode recuperar o documento para uma gama maior de *queries* dos usu√°rios.

**Desafios:**

*   **Qualidade das *queries* geradas:** A efic√°cia da expans√£o do documento depende da qualidade das *queries* geradas. *Queries* irrelevantes ou de baixa qualidade podem degradar o desempenho do sistema de IR.
*   **Custo computacional:** O *fine-tuning* e a gera√ß√£o de *queries* com modelos Transformer podem ser computacionalmente intensivos, especialmente para grandes cole√ß√µes de documentos.

Para mitigar o desafio da qualidade das *queries*, podemos introduzir um filtro baseado em similaridade sem√¢ntica.

**Proposi√ß√£o 1**
Seja $S(d, q)$ uma fun√ß√£o que mede a similaridade sem√¢ntica entre um documento $d$ e uma *query* $q$. Podemos definir um limiar $\theta$ tal que apenas as *queries* com $S(d, q) > \theta$ sejam adicionadas ao documento expandido. Isso ajuda a garantir que as *queries* adicionadas sejam realmente relevantes para o conte√∫do do documento.

> üí° **Exemplo Num√©rico:** Suponha que um documento $d$ sobre "energia solar" gere uma *query* $q_1$ "pain√©is fotovoltaicos" e outra *query* $q_2$ "receitas de bolo". Se a similaridade sem√¢ntica entre $d$ e $q_1$ for $S(d, q_1) = 0.8$ e entre $d$ e $q_2$ for $S(d, q_2) = 0.2$, e definirmos um limiar $\theta = 0.5$, apenas a *query* $q_1$ ser√° adicionada ao documento expandido, pois $S(d, q_1) > \theta$.

A escolha do limiar $\theta$ pode ser feita empiricamente, buscando um equil√≠brio entre a qualidade das *queries* adicionadas e a quantidade de *queries* descartadas.

> üí° **Exemplo Num√©rico:** Considere a seguinte tabela comparando diferentes valores de $\theta$ e seu impacto na precis√£o e revoca√ß√£o (recall) do sistema de IR:
>
> | Limiar ($\theta$) | Precis√£o | Revoca√ß√£o |
> |--------------------|----------|-----------|
> | 0.2                | 0.65     | 0.80      |
> | 0.5                | 0.75     | 0.70      |
> | 0.8                | 0.85     | 0.60      |
>
> Um valor de $\theta = 0.5$ parece oferecer um bom equil√≠brio entre precis√£o e revoca√ß√£o neste cen√°rio. Um valor mais alto de $\theta = 0.8$ aumenta a precis√£o, garantindo que apenas *queries* altamente relevantes sejam adicionadas, mas diminui a revoca√ß√£o, pois algumas *queries* relevantes podem ser descartadas.  O objetivo √© encontrar o valor de $\theta$ que maximize uma m√©trica como F1-score.

### Conclus√£o

A utiliza√ß√£o de arquiteturas Transformer para a gera√ß√£o de *queries* e a consequente expans√£o de documentos representa uma estrat√©gia promissora para melhorar o desempenho de sistemas de IR, mitigando o problema do *vocabulary mismatch* [^36]. Modelos como Doc2Query e DocT5Query demonstram a viabilidade dessa abordagem. A escolha cuidadosa dos par√¢metros de treinamento, bem como das t√©cnicas de amostragem, √© fundamental para garantir a qualidade das *queries* geradas e, consequentemente, a efic√°cia da expans√£o do documento. As futuras pesquisas podem explorar m√©todos para melhorar a qualidade das *queries* geradas, reduzir o custo computacional e adaptar essa t√©cnica para diferentes dom√≠nios e tipos de documentos.
### Refer√™ncias
[^8]: Section 1 provides a short depiction of the different representations for text adopted in IR, from the classical BOW encodings to learning-to-rank features to word embeddings.
[^36]: Document expansion techniques address the vocabulary mismatch problem [Zhao 2012]: queries can use terms semantically similar but lexically different from those used in the relevant documents.
[^36]: In learned sparse retrieval the transformer architectures are used in different scenarios: document expansion learning: sequence-to-sequence models are used to modify the actual content of documents, boosting the statistics of the important terms and generating new terms to be included in a document
[^36]: Instead of leveraging the encoder-decoder models for sentence generation and fine-tune them on document expansion, a different approach computes the importance of all terms in the vocabulary w.r.t. a given document and selects the most important new terms to enrich the document, leveraging an encoder-only architecture to compute the document embeddings.
<!-- END -->