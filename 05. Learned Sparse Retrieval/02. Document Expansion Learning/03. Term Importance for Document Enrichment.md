### Introdu√ß√£o
Expandindo sobre as t√©cnicas de **document expansion learning** introduzidas na Se√ß√£o 5.1 [^36], este cap√≠tulo foca em abordagens alternativas que evitam a gera√ß√£o de senten√ßas. Em vez disso, estas t√©cnicas calculam a *import√¢ncia de termos* no vocabul√°rio em rela√ß√£o a um documento espec√≠fico. O objetivo √© selecionar termos novos e importantes que enrique√ßam o documento, utilizando arquiteturas *encoder-only*. Esta abordagem difere fundamentalmente dos m√©todos que utilizam modelos sequence-to-sequence para gerar novas queries [^36], direcionando o foco para a identifica√ß√£o direta de termos relevantes dentro do vocabul√°rio existente.

Para complementar essa introdu√ß√£o, podemos considerar um cen√°rio onde a expans√£o do documento √© restrita a um subconjunto espec√≠fico do vocabul√°rio.

**Proposi√ß√£o 1** [Document Expansion com Vocabul√°rio Restrito]:
Dado um vocabul√°rio $V$ e um subconjunto $V' \subset V$, as t√©cnicas de document expansion podem ser adaptadas para selecionar termos apenas de $V'$. Isso √© particularmente √∫til em dom√≠nios especializados onde apenas certos termos s√£o relevantes. A adapta√ß√£o envolve restringir a matriz de proje√ß√£o $W$ ou as distribui√ß√µes de probabilidade $p_i$ para considerar apenas os termos em $V'$.

*Proof strategy.* A prova envolve demonstrar que as opera√ß√µes descritas para TILDEv2 e SparTerm podem ser modificadas para trabalhar com o vocabul√°rio restrito $V'$. Para TILDEv2, a matriz $W$ pode ser substitu√≠da por uma matriz $W' \in \mathbb{R}^{|V'| \times l}$. Para SparTerm, a distribui√ß√£o $p_{total}$ pode ser zerada para todos os termos que n√£o pertencem a $V'$.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um vocabul√°rio $V$ de tamanho 1000, e queremos restringir a expans√£o a um subconjunto $V'$ de tamanho 100, que representa termos espec√≠ficos de um dom√≠nio m√©dico.
>
> Para TILDEv2, se a dimens√£o do embedding [CLS] ($l$) √© 768, a matriz de proje√ß√£o original $W$ seria de dimens√£o 1000x768. Com a restri√ß√£o do vocabul√°rio, usar√≠amos uma nova matriz $W'$ de dimens√£o 100x768.  Isto reduz o n√∫mero de par√¢metros a serem considerados durante a expans√£o.
>
> Para SparTerm, se a distribui√ß√£o $p_{total}$ original cont√©m probabilidades para todos os 1000 termos em $V$, zerar√≠amos as probabilidades para os 900 termos que n√£o est√£o em $V'$.  Por exemplo, se $p_{total}[500] = 0.001$ e o termo 500 n√£o est√° em $V'$, ent√£o $p_{total}[500]$ seria definido como 0.
>
> Essa restri√ß√£o garante que apenas os termos relevantes do dom√≠nio m√©dico sejam considerados para a expans√£o do documento, aumentando a precis√£o e a relev√¢ncia dos resultados.

### Conceitos Fundamentais

#### TILDEv2: Document Language Model com BERT
**TILDEv2** [Zhuang and Zuccon 2021b] explora o modelo **BERT** para calcular o embedding de sa√≠da [CLS] de um documento [^36]. Este embedding √© ent√£o projetado sobre todo o vocabul√°rio BERT, computando uma distribui√ß√£o de probabilidade. Este processo efetivamente cria um *modelo de linguagem do documento*.

> *A proje√ß√£o do embedding [CLS] sobre o vocabul√°rio permite que o modelo capture as rela√ß√µes entre o documento e cada termo no vocabul√°rio.*

A opera√ß√£o pode ser formalizada da seguinte maneira:

1.  Dado um documento $d$, o embedding [CLS] √© computado:

    $$
    \phi_{[CLS]} = BERT(d)
    $$
2.  Uma matriz de proje√ß√£o $W \in \mathbb{R}^{|V| \times l}$ √© usada para projetar $\phi_{[CLS]}$ sobre o vocabul√°rio $V$, onde $|V|$ √© o tamanho do vocabul√°rio e $l$ √© a dimens√£o do embedding [CLS] [^16]:

    $$
    p = W \phi_{[CLS]}
    $$
3.  A distribui√ß√£o de probabilidade sobre o vocabul√°rio √© ent√£o obtida, e os novos termos com as maiores probabilidades s√£o adicionados ao documento.

Este m√©todo permite a incorpora√ß√£o de termos que s√£o semanticamente relacionados ao documento, mas que podem n√£o estar presentes textualmente [^36].

**Teorema 1** [Converg√™ncia da Distribui√ß√£o de Probabilidade TILDEv2]: Sob certas condi√ß√µes de regularidade na matriz de proje√ß√£o $W$ e na distribui√ß√£o dos documentos $d$, a distribui√ß√£o de probabilidade $p$ calculada por TILDEv2 converge para uma distribui√ß√£o estacion√°ria conforme o n√∫mero de documentos aumenta.

*Proof strategy.* A prova pode envolver mostrar que a sequ√™ncia de distribui√ß√µes $p$ forma uma cadeia de Markov, e que essa cadeia √© erg√≥dica sob as condi√ß√µes apropriadas sobre $W$ e a distribui√ß√£o de $d$.

> üí° **Exemplo Num√©rico:**
>
> Considere um documento simples: "gato dorme tapete".
>
> 1. **Embedding CLS:** Suponha que ap√≥s passar o documento pelo BERT, obtemos o embedding [CLS] como um vetor de dimens√£o 768:
>
>    $\phi_{[CLS]} = [0.1, -0.2, 0.3, ..., 0.05]$ (vetor de 768 dimens√µes)
>
> 2. **Proje√ß√£o:** Assumindo um vocabul√°rio $|V| = 30000$, a matriz de proje√ß√£o $W$ tem dimens√µes 30000x768.  A proje√ß√£o √© calculada como:
>
>    $p = W \phi_{[CLS]}$
>
>    Onde $p$ √© um vetor de 30000 dimens√µes, representando a probabilidade de cada termo do vocabul√°rio.
>
> 3. **Distribui√ß√£o de Probabilidade:** Ap√≥s calcular $p$, aplicamos uma fun√ß√£o softmax para obter a distribui√ß√£o de probabilidade:
>
>    $p' = softmax(p)$
>
>    Suponha que os termos com as maiores probabilidades sejam:
>
>    | Termo     | Probabilidade |
>    |-----------|---------------|
>    | gato      | 0.15          |
>    | dorme     | 0.12          |
>    | tapete    | 0.10          |
>    | felino    | 0.08          |
>    | soneca    | 0.07          |
>    | ...       | ...           |
>
>    Os termos "felino" e "soneca" (que n√£o estavam originalmente no documento) poderiam ser adicionados para expandir o documento.

#### SparTerm: Term Importance Distribution com Gating Mechanism

**SparTerm** [Bai et al. 2020] adota uma abordagem diferente, calculando um *modelo de linguagem do documento* para cada token de sa√≠da do BERT, incluindo [CLS] [^36]. Estes modelos s√£o ent√£o somados para computar uma *distribui√ß√£o de import√¢ncia de termos*.

> *A principal inova√ß√£o aqui √© o uso de um mecanismo de gating aprendido para manter um subconjunto esparso destes termos, resultando em um conte√∫do final do documento expandido.*

O processo √© descrito da seguinte forma:

1.  Para cada token no documento $d$, BERT computa um modelo de linguagem:
    $$
    p_i = BERT_{LM}(d_i)
    $$
    onde $d_i$ √© o $i$-√©simo token no documento.
2.  As distribui√ß√µes de probabilidade de cada token s√£o somadas:
    $$
    p_{total} = \sum_{i=1}^{|d|} p_i
    $$
3.  Um mecanismo de gating aprendido √© aplicado para selecionar um subconjunto esparso de termos:
    $$
    \gamma = Gate(p_{total})
    $$

    onde $\gamma$ representa os termos selecionados.

O mecanismo de gating garante que apenas os termos mais relevantes sejam mantidos, ajudando a reduzir o ru√≠do e melhorar a efici√™ncia [^36].

**Lema 2** [Limitantes na esparsidade de SparTerm]: A esparsidade do vetor $\gamma$ (i.e., o n√∫mero de elementos n√£o nulos) est√° limitada pela capacidade do modelo BERT e pela fun√ß√£o `Gate`.

*Proof strategy.* A prova pode envolver a an√°lise da fun√ß√£o Gate e demonstrar que a quantidade de informa√ß√£o que ela pode reter √© limitada pela dimensionalidade do embedding de entrada e pela capacidade de aprendizado da rede neural implementando a fun√ß√£o Gate.

> üí° **Exemplo Num√©rico:**
>
> Considere o mesmo documento: "gato dorme tapete".
>
> 1. **Modelos de Linguagem por Token:** O BERT calcula um modelo de linguagem para cada token:
>
>    $p_{gato} = [0.01, 0.005, \ldots, 0.001]$ (distribui√ß√£o sobre o vocabul√°rio)
>    $p_{dorme} = [0.002, 0.01, \ldots, 0.0005]$ (distribui√ß√£o sobre o vocabul√°rio)
>    $p_{tapete} = [0.001, 0.002, \ldots, 0.01]$ (distribui√ß√£o sobre o vocabul√°rio)
>
> 2. **Soma das Distribui√ß√µes:**  As distribui√ß√µes s√£o somadas:
>
>    $p_{total} = p_{gato} + p_{dorme} + p_{tapete}$
>
>    $p_{total} = [0.013, 0.017, \ldots, 0.0115]$
>
> 3. **Mecanismo de Gating:** A fun√ß√£o `Gate` seleciona os termos mais importantes. Suponha que o `Gate` foi treinado para manter apenas 2 termos:
>
>    $\gamma = Gate(p_{total})$
>
>    Suponha que os dois termos selecionados pelo `Gate` sejam "gato" e "tapete", com pesos normalizados.  O vetor resultante $\gamma$ pode ser representado como:
>
>    | Termo     | Import√¢ncia |
>    |-----------|-------------|
>    | gato      | 0.6         |
>    | tapete    | 0.4         |
>    | ...       | 0           | (para todos os outros termos)

**Corol√°rio 2.1** [Compromisso entre esparsidade e relev√¢ncia em SparTerm]: Existe um compromisso inerente entre a esparsidade de $\gamma$ e a relev√¢ncia dos termos selecionados. Aumentar a esparsidade excessivamente pode levar √† perda de termos relevantes, enquanto diminuir a esparsidade pode introduzir ru√≠do.

Este corol√°rio ressalta a import√¢ncia de ajustar cuidadosamente o mecanismo de gating para encontrar o equil√≠brio ideal entre esparsidade e relev√¢ncia.

### Conclus√£o

Ambas as abordagens, **TILDEv2** e **SparTerm**, oferecem m√©todos alternativos para o document expansion learning, evitando a gera√ß√£o de senten√ßas expl√≠citas. TILDEv2 usa o embedding [CLS] para criar um modelo de linguagem global do documento, enquanto SparTerm agrega modelos de linguagem token-a-token e utiliza um mecanismo de gating aprendido para esparsidade [^36].

A escolha entre esses m√©todos depende dos requisitos espec√≠ficos da aplica√ß√£o, incluindo a disponibilidade de recursos computacionais, a necessidade de esparsidade, e a import√¢ncia da precis√£o na sele√ß√£o de termos.

Al√©m disso, a combina√ß√£o de TILDEv2 e SparTerm poderia ser explorada. Por exemplo, a distribui√ß√£o de probabilidade gerada por TILDEv2 poderia ser usada como entrada para o mecanismo de gating de SparTerm.

**Proposi√ß√£o 3** [Hibridiza√ß√£o de TILDEv2 e SparTerm]: A distribui√ß√£o de probabilidade $p$ gerada por TILDEv2 pode ser utilizada como entrada para o mecanismo de gating de SparTerm, combinando as vantagens de ambos os m√©todos.

*Proof strategy.* A prova envolve a substitui√ß√£o da entrada $p_{total}$ da fun√ß√£o Gate em SparTerm pela distribui√ß√£o $p$ de TILDEv2, e demonstrar que o m√©todo h√≠brido resultante herda propriedades de ambos os modelos.

> üí° **Exemplo Num√©rico:**
>
> Usando os resultados dos exemplos anteriores, a distribui√ß√£o de probabilidade $p'$ gerada por TILDEv2 (ap√≥s o softmax) poderia ser diretamente utilizada como entrada para o mecanismo de gating de SparTerm.  Nesse caso, a fun√ß√£o Gate n√£o receberia a soma das distribui√ß√µes por token ($p_{total}$), mas sim a distribui√ß√£o global do documento calculada por TILDEv2.  Isso permitiria que o gating se baseasse em uma vis√£o mais hol√≠stica do documento, potencialmente melhorando a sele√ß√£o dos termos mais relevantes.
>
> Por exemplo, se $p' = [0.15, 0.12, 0.10, 0.08, 0.07, \ldots ]$ (para os termos "gato", "dorme", "tapete", "felino", "soneca", ...), ent√£o a fun√ß√£o Gate receberia este vetor como entrada.  O Gate ainda selecionaria os termos mais importantes, mas agora com base na distribui√ß√£o global calculada pelo TILDEv2.

### Refer√™ncias
[^36]: Se√ß√£o 5.1 do documento original.
<!-- END -->