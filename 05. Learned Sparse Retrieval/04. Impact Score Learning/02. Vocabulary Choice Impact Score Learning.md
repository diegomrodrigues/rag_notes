## Vocabul√°rio e Impacto no √çndice Invertido em Impact Score Learning

### Introdu√ß√£o
Como discutido na Se√ß√£o 5.2 [^37], **impact score learning** busca aproveitar *document embeddings* gerados por modelos *encoder-only* para computar um √∫nico valor inteiro, que ser√° armazenado nos *postings* e usado como um *proxy* da relev√¢ncia do termo no *posting* correspondente, ou seja, sua **term importance**. No entanto, um problema comum enfrentado em *impact score learning* √© a escolha do vocabul√°rio, que afeta diretamente a estrutura e efici√™ncia do √≠ndice invertido resultante [^37]. Esta se√ß√£o aprofunda-se nesse desafio, explorando as implica√ß√µes de diferentes abordagens de tokeniza√ß√£o no vocabul√°rio final, na densidade das *posting lists* e nos tempos de processamento de consultas.

### Escolha do Vocabul√°rio e suas Implica√ß√µes

Em *impact score learning*, a escolha do vocabul√°rio tem um impacto direto na estrutura do √≠ndice invertido. Os modelos *encoder-only* frequentemente utilizam *sub-word tokenizers*, como os inspirados no BERT [^15], enquanto outras abordagens empregam *word tokenizers* convencionais [^37]. Cada um deles resulta em um vocabul√°rio distinto e, consequentemente, em um √≠ndice invertido com caracter√≠sticas diferentes.

1.  ***Sub-word Tokenizers:*** Ao usar um *sub-word tokenizer*, o vocabul√°rio tende a ser menor se comparado a um vocabul√°rio baseado em palavras inteiras, uma vez que as palavras s√£o fragmentadas em unidades menores e mais frequentes [^16]. Isso leva a:
    *   **Menos termos no √≠ndice invertido:** Reduzindo o tamanho do vocabul√°rio ($|V|$) [^8].
    *   **Posting lists mais longas e densas:** Cada sub-palavra aparece em mais documentos, resultando em listas maiores, visto que um termo da sub-palavra pode ser parte de muitas palavras no documento [^38].

2.  ***Word Tokenizers:*** Optar por um *word tokenizer* resulta em um vocabul√°rio maior, com cada termo representando uma palavra inteira. Isso acarreta em:
    *   **Mais termos no √≠ndice invertido:** Aumentando o tamanho do vocabul√°rio ($|V|$) [^8].
    *   **Posting lists mais curtas e esparsas:** Cada palavra espec√≠fica ocorre em menos documentos em compara√ß√£o com as sub-palavras, levando a listas menores e mais fragmentadas [^38].

> üí° **Exemplo Num√©rico:**
>
> Considere a seguinte cole√ß√£o de documentos:
>
> *   Documento 1: "O gato preto corre r√°pido."
> *   Documento 2: "Um gato branco dorme."
> *   Documento 3: "Gatos pretos s√£o comuns."
>
> **Word Tokenizer:**
>
> Vocabul√°rio: `{O, gato, preto, corre, r√°pido, Um, branco, dorme, Gatos, s√£o, comuns}`
>
> Tamanho do Vocabul√°rio: $|V_{word}| = 11$
>
> Posting List para "gato": `{1, 2}`
> Posting List para "preto": `{1, 3}`
>
> **Sub-word Tokenizer (simplificado):**
>
> Suponha um tokenizer que divide palavras em peda√ßos menores, por exemplo: `{"gato": ["ga", "to"], "preto": ["pre", "to"], ...}`
>
> Vocabul√°rio: `{ga, to, pre, corre, rapido, Um, bran, co, dorme, sao, comum}` (simplificado para o exemplo)
>
> Tamanho do Vocabul√°rio: $|V_{sub}| = 11$ (neste exemplo simplificado, pode ser menor ou maior dependendo do algoritmo de sub-word tokenization)
>
>  Posting List para "to": `{1, 2}` (aparece em "gato" nos documentos 1 e 2)
>  Posting List para "pre": `{1, 3}` (aparece em "preto" nos documentos 1 e 3)
>
> Neste exemplo simplificado, embora o tamanho do vocabul√°rio seja semelhante, em um cen√°rio real, o sub-word tokenizer tenderia a ter um vocabul√°rio menor em um corpus maior, com posting lists potencialmente mais longas, especialmente para sub-palavras comuns.

A escolha entre esses dois tipos de *tokenizers* afeta o trade-off entre o tamanho do √≠ndice, a densidade das *posting lists* e a efici√™ncia do processamento de consultas. A densidade das *posting lists*, em particular, afeta como as opera√ß√µes de interse√ß√£o s√£o realizadas durante a recupera√ß√£o, impactando diretamente a lat√™ncia da consulta.

**Proposi√ß√£o 1** Dada uma cole√ß√£o de documentos *D*, a utiliza√ß√£o de um *sub-word tokenizer* resultar√° em um √≠ndice invertido com menor cardinalidade de vocabul√°rio, denotada por $|V_{sub}|$, em compara√ß√£o com a utiliza√ß√£o de um *word tokenizer*, que resulta em uma cardinalidade $|V_{word}|$. Formalmente, $|V_{sub}| < |V_{word}|$. Al√©m disso, o comprimento m√©dio das *posting lists* ser√° maior para o √≠ndice gerado com *sub-word tokenizer*.

*Prova (Estrat√©gia):* A prova segue diretamente da defini√ß√£o de *sub-word tokenizers* e *word tokenizers*. *Sub-word tokenizers* decomp√µem palavras em unidades menores, recombin√°veis, o que inerentemente leva a um vocabul√°rio menor e maior frequ√™ncia de ocorr√™ncia de cada token. A frequ√™ncia aumentada implica *posting lists* mais longas.

A necessidade de quantizar os valores reais de *term importance* em inteiros de 8-bit, como mencionado na Se√ß√£o 5.2 [^37], introduz outro desafio. A granularidade dessa quantiza√ß√£o, combinada com as escalas dos valores de *term importance* produzidos pelo modelo, pode afetar a precis√£o com que a import√¢ncia relativa de cada termo √© representada. O modelo deve ser treinado de forma a produzir *term importances* que se alinhem com a resolu√ß√£o da quantiza√ß√£o escolhida, maximizando a preserva√ß√£o das nuances na import√¢ncia dos termos.

> üí° **Exemplo Num√©rico:**
>
> Suponha que um modelo *encoder-only* produza os seguintes *term importances* para o termo "gato" em diferentes documentos:
>
> *   Documento 1: 0.85
> *   Documento 2: 0.62
> *   Documento 3: 0.91
>
> **Quantiza√ß√£o para 2-bits (4 n√≠veis: 0, 1, 2, 3):**
>
> Assumindo uma escala linear de 0 a 1, podemos mapear os valores para os inteiros mais pr√≥ximos:
>
> *   0.0 - 0.25 -> 0
> *   0.26 - 0.50 -> 1
> *   0.51 - 0.75 -> 2
> *   0.76 - 1.00 -> 3
>
> Ap√≥s a quantiza√ß√£o:
>
> *   Documento 1: 3 (0.85 -> 3)
> *   Documento 2: 2 (0.62 -> 2)
> *   Documento 3: 3 (0.91 -> 3)
>
> **Quantiza√ß√£o para 8-bits (256 n√≠veis):**
>
> Neste caso, a escala seria de 0 a 255. A f√≥rmula seria: $quantized\_value = round(term\_importance * 255)$
>
> *   Documento 1: round(0.85 * 255) = 217
> *   Documento 2: round(0.62 * 255) = 158
> *   Documento 3: round(0.91 * 255) = 232
>
> A quantiza√ß√£o para 8 bits preserva muito mais a varia√ß√£o nos *term importances* do que a quantiza√ß√£o para 2 bits. A quantiza√ß√£o para 2 bits agrupa os documentos 1 e 3, perdendo a distin√ß√£o entre eles. Essa perda de informa√ß√£o pode impactar negativamente a precis√£o da recupera√ß√£o.

Para analisar mais profundamente o impacto da quantiza√ß√£o, podemos considerar a seguinte proposi√ß√£o:

**Proposi√ß√£o 2:** A quantiza√ß√£o de *term importance* para *n* bits resulta em $2^n$ n√≠veis de granularidade. A precis√£o da representa√ß√£o da import√¢ncia relativa dos termos √© inversamente proporcional ao n√∫mero de n√≠veis de quantiza√ß√£o.

*Prova (Estrat√©gia):* Esta proposi√ß√£o reflete a natureza fundamental da quantiza√ß√£o. Menos bits significam menos n√≠veis distintos para representar diferentes valores, levando a uma representa√ß√£o mais grosseira. Um aumento no n√∫mero de bits aumenta a precis√£o da representa√ß√£o, mas tamb√©m o custo de armazenamento.

Em ess√™ncia, a escolha entre utilizar *sub-word tokenizers* ou *word tokenizers* depende do contexto espec√≠fico da aplica√ß√£o de *IR*, das caracter√≠sticas do *corpus*, e das restri√ß√µes de desempenho desejadas. Avaliar empiricamente o impacto dessas escolhas na precis√£o da recupera√ß√£o, no tamanho do √≠ndice e na lat√™ncia da consulta √© fundamental para determinar a abordagem mais adequada.

### Rela√ß√£o com os sistemas de Impact Score Learning
Os sistemas de *impact score learning*, como DeepCT e DeepImpact, ilustram diferentes abordagens para lidar com essa escolha de vocabul√°rio [^38]. DeepCT, por exemplo, aprende novas frequ√™ncias de termos para uso com fun√ß√µes de *ranking* cl√°ssicas, enquanto DeepImpact calcula um *impact score* diretamente, alterando a forma como o vocabul√°rio √© usado para determinar a relev√¢ncia [^38]. A quantiza√ß√£o linear dos *impact scores* para inteiros de 8 bits em DeepImpact [^39] representa uma forma de equilibrar a precis√£o da representa√ß√£o com o espa√ßo de armazenamento necess√°rio.

**Teorema 1:** Em sistemas de *impact score learning*, o desempenho de diferentes *tokenizers* (e.g., *word* vs. *sub-word*) √© condicionado √† estrat√©gia de *ranking* utilizada e √† distribui√ß√£o de frequ√™ncia dos termos no *corpus*.

*Prova (Estrat√©gia):* A prova envolve uma an√°lise emp√≠rica do desempenho de diferentes *tokenizers* em combina√ß√£o com diferentes fun√ß√µes de *ranking* (e.g., BM25, TF-IDF, *learning-to-rank*). O desempenho √© medido em m√©tricas de avalia√ß√£o de *IR* padr√£o (e.g., nDCG, MAP). Varia√ß√µes no desempenho observadas entre diferentes *tokenizers* para diferentes fun√ß√µes de *ranking* ou *corpora* demonstram a depend√™ncia condicional. Por exemplo, um *sub-word tokenizer* pode superar um *word tokenizer* com BM25 em *corpora* com muitas palavras raras, mas o inverso pode ser verdadeiro com fun√ß√µes de *ranking* mais complexas ou *corpora* mais uniformes.

> üí° **Exemplo Num√©rico:**
>
> Suponha que realizamos experimentos com dois *tokenizers* e duas fun√ß√µes de *ranking* em um *corpus* de artigos cient√≠ficos. Os resultados, medidos em nDCG@10, s√£o mostrados na tabela abaixo:
>
> | Tokenizer    | Ranking Function | nDCG@10 |
> | :----------- | :--------------- | :------ |
> | Word         | BM25             | 0.65    |
> | Word         | LambdaMART       | 0.78    |
> | Sub-word     | BM25             | 0.72    |
> | Sub-word     | LambdaMART       | 0.75    |
>
> **An√°lise:**
>
> *   O *sub-word tokenizer* supera o *word tokenizer* com BM25, indicando que ele pode ser mais eficaz para capturar informa√ß√µes relevantes em *corpora* com vocabul√°rio complexo quando usado com uma fun√ß√£o de *ranking* mais simples como BM25.
> *   LambdaMART, uma fun√ß√£o de *ranking* mais complexa, tem um desempenho melhor com o *word tokenizer* em compara√ß√£o com o *sub-word tokenizer*. Isso pode indicar que a fun√ß√£o de *ranking* mais complexa √© capaz de aproveitar melhor as informa√ß√µes fornecidas pelo vocabul√°rio mais rico do *word tokenizer*.
>
> Este exemplo num√©rico demonstra claramente como o desempenho de diferentes *tokenizers* √© condicionado √† estrat√©gia de *ranking* utilizada. Um teste estat√≠stico (e.g., teste t pareado) poderia ser usado para verificar se as diferen√ßas observadas s√£o estatisticamente significativas.

### Conclus√£o
A escolha do vocabul√°rio √© um passo cr√≠tico em *impact score learning*, com implica√ß√µes significativas no tamanho, estrutura e efici√™ncia do √≠ndice invertido. Ao considerar cuidadosamente os *trade-offs* associados a diferentes t√©cnicas de tokeniza√ß√£o e quantiza√ß√£o, √© poss√≠vel otimizar o desempenho dos sistemas de *IR* para tarefas e *datasets* espec√≠ficos. Sistemas como DeepCT, DeepImpact, TILDEv2 e UniCOIL [^38, ^39] exemplificam diferentes estrat√©gias para enfrentar esse desafio, cada uma com suas pr√≥prias vantagens e desvantagens, sublinhando a import√¢ncia de uma abordagem emp√≠rica e contextualizada para o projeto de sistemas de *IR*.

### Refer√™ncias
[^37]: Trecho do contexto fornecido sobre Impact Score Learning.
[^8]: Trecho do contexto fornecido sobre a defini√ß√£o de vocabul√°rio.
[^38]: Trecho do contexto fornecido sobre sistemas de Impact Score Learning existentes.
[^39]: Trecho do contexto fornecido sobre quantiza√ß√£o em Impact Score Learning.
[^15]: Trecho do contexto fornecido sobre Bidirectional Encoder Representations from Transformers (BERT).
[^16]: Trecho do contexto fornecido sobre tokeniza√ß√£o WordPiece.
<!-- END -->