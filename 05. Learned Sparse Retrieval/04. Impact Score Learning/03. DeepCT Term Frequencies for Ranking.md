## Impact Score Learning: Contextualized Term Importance and Weighting

### Introdu√ß√£o

A busca por aprimorar a efic√°cia dos sistemas de Information Retrieval (IR) levou ao desenvolvimento de t√©cnicas de *learned sparse retrieval*, que visam integrar os benef√≠cios das redes neurais com a efici√™ncia dos *inverted indexes*. Um dos componentes cruciais nesse contexto √© o **impact score learning**, que se concentra em aprender a import√¢ncia de cada termo em um documento, permitindo uma pondera√ß√£o mais precisa e informada durante o processo de ranking. Esta se√ß√£o detalha abordagens como DeepCT, DeepImpact, TILDEv2 e UniCOIL, explorando como cada uma utiliza representa√ß√µes contextuais de palavras e arquiteturas neurais para estimar a relev√¢ncia dos termos nos documentos.

### DeepCT: Contextualized Term Frequencies

DeepCT [^78, ^79] representa um dos primeiros exemplos de *term importance boosting*, aproveitando as representa√ß√µes contextuais de palavras fornecidas pelo **BERT** [^55] para aprender novas frequ√™ncias de termos in-document. O objetivo √© usar essas frequ√™ncias aprendidas com fun√ß√µes de ranking cl√°ssicas, como o **BM25** [^95].

Para cada termo $w_i \in V$ (onde $V$ √© o vocabul√°rio) em um dado documento, o DeepCT estima sua import√¢ncia espec√≠fica de contexto $z_i \in \mathbb{R}$. Este valor √© ent√£o escalado e arredondado para um valor inteiro semelhante √† frequ√™ncia (frequency-like integer value) $tf_i$, que pode ser armazenado em um *inverted index* [^9]. Formalmente, para cada documento $d \in D$, DeepCT projeta as representa√ß√µes *l*-dimensionais $\psi_i$ para cada token BERT de entrada $w_i$ no documento (com $i = 1, ..., |d|$) em uma import√¢ncia de termo escalar usando a matriz aprendida $W \in \mathbb{R}^{1 \times l}$:

$$
\begin{aligned}
[\psi_0, \psi_1, \ldots] &= Encoder(d) \\
z_i &= W \psi_i
\end{aligned}
$$

O modelo DeepCT √© treinado com uma tarefa de regress√£o *per-token*, visando prever a import√¢ncia dos termos. A import√¢ncia real do termo a ser prevista √© derivada do documento que cont√©m o termo ou de um conjunto de treinamento de pares de consulta/documento relevantes. Um termo que aparece em v√°rios documentos relevantes e em diferentes consultas tem uma import√¢ncia maior do que um termo que corresponde a menos documentos e/ou menos consultas [^79].

> üí° **Exemplo Num√©rico:**
>
> Considere um documento simples: "O gato preto". Suponha que o `Encoder(d)` do BERT retorne as seguintes representa√ß√µes para cada token (ap√≥s alguma simplifica√ß√£o e redu√ß√£o de dimensionalidade para facilitar a demonstra√ß√£o):
>
> *   `O`: $\psi_0 = [0.1, 0.2]$
> *   `gato`: $\psi_1 = [0.5, 0.6]$
> *   `preto`: $\psi_2 = [0.8, 0.9]$
>
> E que a matriz aprendida $W$ seja $W = [[0.5, 0.5]]$. Ent√£o, a import√¢ncia de cada termo seria:
>
> *   $z_0 = W \psi_0 = [0.5, 0.5] \cdot [0.1, 0.2] = 0.05 + 0.1 = 0.15$
> *   $z_1 = W \psi_1 = [0.5, 0.5] \cdot [0.5, 0.6] = 0.25 + 0.3 = 0.55$
> *   $z_2 = W \psi_2 = [0.5, 0.5] \cdot [0.8, 0.9] = 0.4 + 0.45 = 0.85$
>
> Esses valores $z_i$ seriam ent√£o escalados e arredondados para valores inteiros, que representariam a import√¢ncia do termo. Por exemplo, se o fator de escala for 10, ter√≠amos `tf_0 = 1`, `tf_1 = 6`, e `tf_2 = 9`.  O termo "preto" √© considerado o mais importante nesse contexto.

Para lidar com os *sub-word tokens* do BERT, o DeepCT usa a import√¢ncia do primeiro *sub-word token* para toda a palavra e, quando um termo ocorre v√°rias vezes em um documento, ele considera a import√¢ncia m√°xima entre as m√∫ltiplas ocorr√™ncias [^78].

**Proposi√ß√£o 1.** *Uma poss√≠vel extens√£o para o DeepCT seria incorporar a pondera√ß√£o da frequ√™ncia do termo original (TF) como um fator adicional na determina√ß√£o da import√¢ncia do termo. Isso permitiria que o modelo combinasse o aprendizado contextual com a frequ√™ncia bruta, potencialmente capturando informa√ß√µes complementares.* Formalmente, a import√¢ncia do termo $z_i$ poderia ser modificada para:

$$
z_i = W \psi_i + \alpha \cdot TF(w_i, d)
$$

onde $TF(w_i, d)$ representa a frequ√™ncia do termo $w_i$ no documento $d$, e $\alpha$ √© um hiperpar√¢metro ajust√°vel que controla a import√¢ncia relativa da frequ√™ncia do termo original.

> üí° **Exemplo Num√©rico:**
>
> Usando o mesmo documento "O gato preto" e os mesmos valores de $W$ e $\psi_i$ do exemplo anterior, suponha que $\alpha = 0.2$. A frequ√™ncia de cada termo √© 1 (TF = 1).
>
> *   $z_0 = 0.15 + (0.2 * 1) = 0.35$
> *   $z_1 = 0.55 + (0.2 * 1) = 0.75$
> *   $z_2 = 0.85 + (0.2 * 1) = 1.05$
>
> Neste caso, a frequ√™ncia do termo adiciona um pequeno valor √† import√¢ncia calculada contextualmente, ajustando a import√¢ncia final. O ajuste garante que termos frequentes, mesmo com menor import√¢ncia contextual, n√£o sejam completamente ignorados.

Para lidar com os *sub-word tokens* do BERT, o DeepCT usa a import√¢ncia do primeiro *sub-word token* para toda a palavra e, quando um termo ocorre v√°rias vezes em um documento, ele considera a import√¢ncia m√°xima entre as m√∫ltiplas ocorr√™ncias [^78].

### DeepImpact: Impact Scores Diretos

DeepImpact [^64] prop√µe, pela primeira vez, computar diretamente um *impact score* para cada termo √∫nico em um documento, sem recorrer √†s fun√ß√µes de ranking cl√°ssicas. Em vez disso, ele simplesmente soma, no momento do processamento da consulta, os impactos dos termos da consulta que aparecem em um documento para computar seu *relevance score*.

Para cada termo $w_i \in V$ em um dado documento $d \in D$, DeepImpact estima seu impacto espec√≠fico de contexto $z_i \in \mathbb{R}$. O DeepImpact alimenta o modelo *encoder-only* com os tokens *sub-word* do documento, produzindo um *embedding* para cada *token* de entrada. Uma *gating layer* n√£o aprendida (**Mask**) remove os *embeddings* dos *tokens sub-word* que n√£o correspondem ao primeiro *sub-token* da palavra inteira. Em seguida, DeepImpact transforma as representa√ß√µes *l*-dimensionais restantes com duas *feed forward networks* com ativa√ß√µes **ReLU**. A primeira rede tem uma matriz de peso $W_1 \in \mathbb{R}^{l \times l}$, e a segunda rede tem uma matriz de peso $W_2 \in \mathbb{R}^{1 \times l}$:

$$
\begin{aligned}
[\psi_0, \psi_1,\ldots] &= Encoder(DocT5Query(d)) \\
[x_0, x_1,\ldots] &= Mask(\psi_0, \psi_1,\ldots) \\
y_i &= ReLU(W_1 x_i) \\
z_i &= ReLU(W_2 y_i)
\end{aligned}
$$

Os n√∫meros reais de sa√≠da $z_i$ (com $i = 1, ..., |d|$), um por palavra inteira no documento de entrada, s√£o ent√£o quantizados linearmente em inteiros de 8 bits que podem ser armazenados em um *inverted index*. Isso produz um *score* de valor √∫nico para cada termo exclusivo no documento, representando seu impacto. Dado uma consulta $q$, o *score* do documento $d$ √© simplesmente a soma dos impactos para a interse√ß√£o de termos em $q$ e $d$. DeepImpact √© treinado com triplas de consulta, documento relevante e documento n√£o relevante e, para cada tripla, dois *scores* para os dois documentos correspondentes s√£o computados. O modelo √© otimizado via *pairwise cross-entropy loss* sobre os *document scores*. Al√©m disso, DeepImpact tem sido o primeiro modelo esparso aprendido alavancando ao mesmo tempo o aprendizado de expans√£o de documentos e o aprendizado de *impact scores* [^64]. Na verdade, DeepImpact alavanca DocT5Query [^56] para enriquecer a cole√ß√£o de documentos antes de aprender o impacto do termo.

> üí° **Exemplo Num√©rico:**
>
> Documento: "Cachorro late alto." Consulta: "Cachorro barulhento."
>
> 1.  **Expans√£o do Documento (DocT5Query):** Suponha que DocT5Query expanda o documento para "Cachorro late alto animal dom√©stico".
> 2.  **Encoder:** Suponha que o encoder retorne os seguintes embeddings (ap√≥s a aplica√ß√£o da m√°scara para manter apenas o primeiro subtoken de cada palavra e simplifica√ß√£o para $l=2$):
>     *   `Cachorro`: $\psi_0 = [0.4, 0.5]$
>     *   `late`: $\psi_1 = [0.2, 0.3]$
>     *   `alto`: $\psi_2 = [0.1, 0.15]$
>     *   `animal`: $\psi_3 = [0.05, 0.05]$
>     *   `dom√©stico`: $\psi_4 = [0.08, 0.1]$
> 3.  **Feed Forward Networks:** Sejam $W_1 = [[0.5, 0.5], [0.5, 0.5]]$ e $W_2 = [[0.5, 0.5]]$.
>
>     *   $y_0 = ReLU(W_1 \psi_0) = ReLU([[0.45], [0.45]]) = [[0.45], [0.45]]$
>     *   $z_0 = ReLU(W_2 y_0) = ReLU([0.45]) = 0.45$
>
>     Similarmente:
>
>     *   $z_1 = 0.25$
>     *   $z_2 = 0.125$
>     *   $z_3 = 0.05$
>     *   $z_4 = 0.09$
> 4.  **Quantiza√ß√£o:** Suponha que a quantiza√ß√£o mapeie esses valores para inteiros de 8 bits.
>
> 5.  **C√°lculo do Score:** A consulta cont√©m o termo "Cachorro". O score do documento para essa consulta √© o impacto do termo "Cachorro" no documento, que √© 0.45 (ou seu valor quantizado).  A expans√£o do documento ajudou a adicionar termos relevantes que podem corresponder a outras consultas, aumentando o recall potencial.
>
>     Se a consulta fosse "Cachorro dom√©stico", o score seria 0.45 + 0.09 = 0.54.

![Example of DocT5Query model generating related queries for document expansion.](./../images/image1.png)

**Teorema 2.** *A arquitetura do DeepImpact pode ser generalizada para N camadas de feed forward networks com ativa√ß√µes ReLU.* Especificamente, considere N matrizes de peso $W_k \in \mathbb{R}^{l \times l}$ para $k=1, \ldots, N$. As equa√ß√µes seriam ent√£o:

$$
\begin{aligned}
[\psi_0, \psi_1,\ldots] &= Encoder(DocT5Query(d)) \\
[x_0, x_1,\ldots] &= Mask(\psi_0, \psi_1,\ldots) \\
y_i^{(1)} &= ReLU(W_1 x_i) \\
y_i^{(2)} &= ReLU(W_2 y_i^{(1)}) \\
&\vdots \\
y_i^{(N-1)} &= ReLU(W_{N-1} y_i^{(N-2)}) \\
z_i &= ReLU(W_N y_i^{(N-1)})
\end{aligned}
$$

A escolha de $N$ e a inicializa√ß√£o das matrizes $W_k$ podem ser otimizadas via valida√ß√£o cruzada. Uma vantagem desta generaliza√ß√£o seria a possibilidade de aprender representa√ß√µes mais complexas da import√¢ncia dos termos.

### TILDEv2: Term Impact with Document Expansion

TILDEv2 [^113] computa o impacto dos termos com uma abordagem semelhante ao DeepImpact. As principais diferen√ßas s√£o:

*   O uso de uma √∫nica camada *feed forward network* com ativa√ß√µes ReLU, em vez de uma rede de duas camadas, para projetar os *document embeddings* em um valor escalar positivo √∫nico usando uma matriz aprendida $W \in \mathbb{R}^{1 \times l}$.
*   O uso de sua pr√≥pria t√©cnica de expans√£o de documentos (como discutido na Se√ß√£o 5.1) [^100].
*   O uso de um *index* com termos *sub-word* em vez de termos de palavras inteiras.
*   A sele√ß√£o do *impact score* de maior valor para um *token* se esse *token* aparecer v√°rias vezes em um documento [^113].

$$
\begin{aligned}
[\psi_0, \psi_1,\ldots] &= Encoder(TILDEv2(d)) \\
z_i &= ReLU(W \psi_i)
\end{aligned}
$$

Os *scores* $z_i$ s√£o ent√£o somados, obtendo um *query-document score* acumulado.

> üí° **Exemplo Num√©rico:**
>
> Documento: "r√°pido carro azul"
>
> Suponha que a expans√£o do documento do TILDEv2 adicione o termo "autom√≥vel".
>
> 1.  **Encoder:** Ap√≥s a expans√£o e aplica√ß√£o do encoder, temos os seguintes embeddings (com $l=2$):
>     *   `r√°pido`: $\psi_0 = [0.3, 0.4]$
>     *   `carro`: $\psi_1 = [0.5, 0.6]$
>     *   `azul`: $\psi_2 = [0.2, 0.25]$
>     *   `autom√≥vel`: $\psi_3 = [0.45, 0.55]$
> 2.  **Feed Forward Network:** Seja $W = [[0.5, 0.5]]$.
>     *   $z_0 = ReLU(W \psi_0) = ReLU([0.35]) = 0.35$
>     *   $z_1 = ReLU(W \psi_1) = ReLU([0.55]) = 0.55$
>     *   $z_2 = ReLU(W \psi_2) = ReLU([0.225]) = 0.225$
>     *   $z_3 = ReLU(W \psi_3) = ReLU([0.5]) = 0.5$
> 3.  **Query-Document Score:** O score do documento √© a soma dos $z_i$: $0.35 + 0.55 + 0.225 + 0.5 = 1.625$.

**Corol√°rio 3.** *Dado que o TILDEv2 utiliza uma √∫nica camada feed forward network, uma alternativa seria explorar diferentes fun√ß√µes de ativa√ß√£o al√©m da ReLU.* Por exemplo, a fun√ß√£o de ativa√ß√£o Sigmoid ($\sigma(x) = \frac{1}{1 + e^{-x}}$) ou a Tanh ($\tanh(x)$) poderiam ser utilizadas. A escolha da fun√ß√£o de ativa√ß√£o pode impactar a capacidade do modelo de aprender diferentes padr√µes de import√¢ncia dos termos. Assim, a equa√ß√£o para $z_i$ se tornaria:

$$
z_i = f(W \psi_i)
$$

onde $f$ representa a fun√ß√£o de ativa√ß√£o, que poderia ser ReLU, Sigmoid ou Tanh.

### UniCOIL: COIL-based Term Weighting

UniCOIL [^52] explora a abordagem **COIL** (Contextualized Inverted List) (ver Sec. 3), mas em vez de projetar os *query* e *document embeddings* em 8-32 dimens√µes [^11], ele os projeta em *query weights* e *document weights* de dimens√£o √∫nica. Em UniCOIL, os *embeddings* de consulta e documento [CLS] n√£o s√£o usados, e os *embeddings* correspondentes aos *tokens* de consulta e documento normais s√£o projetados em valores escalares √∫nicos $v_i$ usando uma matriz aprendida $W \in \mathbb{R}^{1 \times l}$, com ativa√ß√µes ReLU nos pesos dos termos de sa√≠da [^52]:

$$
\begin{aligned}
[\phi_0, \phi_1,\ldots] &= Encoder(q) \\
[\psi_0, \psi_1,\ldots] &= Encoder(DocT5Query(d)) \\
[v_1, v_2,\ldots] &= [W \phi_1, W \phi_2, \ldots] \\
[z_1, z_2,\ldots] &= [W \psi_1, W \psi_2, \ldots] \\
s(q,d) &= \sum_{t_i \in q} \max_{t_j \in d, t_j = t_i} v_i z_j
\end{aligned}
$$

Os pesos do documento $z_i$ s√£o ent√£o quantizados linearmente em inteiros de 8 bits, e o *query-document score* final √© computado somando os *document impact scores* de maior valor vezes seu *query weight* $v_i$, computado no momento do processamento da consulta, conforme em Eq. (27) [^52]. O modelo base COIL para for√ßar o modelo a gerar pesos n√£o negativos [^52].

> üí° **Exemplo Num√©rico:**
>
> Consulta (q): "melhor carro"
> Documento (d): "Este carro √© o melhor carro do mundo."
>
> 1.  **Encoding:**
>     *   Consulta:
>         *   `melhor`: $\phi_1 = [0.7, 0.8]$
>         *   `carro`: $\phi_2 = [0.9, 0.95]$
>     *   Documento (ap√≥s expans√£o via DocT5Query, que n√£o adiciona termos neste exemplo):
>         *   `Este`: $\psi_1 = [0.1, 0.15]$
>         *   `carro`: $\psi_2 = [0.85, 0.9]$
>         *   `√©`: $\psi_3 = [0.05, 0.05]$
>         *   `o`: $\psi_4 = [0.01, 0.02]$
>         *   `melhor`: $\psi_5 = [0.75, 0.8]$
>         *   `carro`: $\psi_6 = [0.8, 0.85]$
>         *   `do`: $\psi_7 = [0.02, 0.03]$
>         *   `mundo`: $\psi_8 = [0.3, 0.35]$
> 2.  **Weight Calculation:** Seja $W = [[0.5, 0.5]]$.
>     *   Consulta:
>         *   $v_1 = W \phi_1 = [0.75]$
>         *   $v_2 = W \phi_2 = [0.925]$
>     *   Documento:
>         *   $z_2 = W \psi_2 = [0.875]$
>         *   $z_5 = W \psi_5 = [0.775]$
>         *   $z_6 = W \psi_6 = [0.825]$
> 3.  **Similarity Score:**
>     *   Para o termo "melhor": $\max(v_1 * z_5) = 0.75 * 0.775 = 0.58125$
>     *   Para o termo "carro": $\max(v_2 * z_2, v_2 * z_6) = \max(0.925 * 0.875, 0.925 * 0.825) = \max(0.809375, 0.763125) = 0.809375$
>     *   $s(q, d) = 0.58125 + 0.809375 = 1.390625$

**Lema 4.** *A fun√ß√£o de similaridade $s(q, d)$ em UniCOIL pode ser modificada para incluir um fator de normaliza√ß√£o baseado no tamanho do documento.* Isso ajudaria a evitar que documentos mais longos recebam *scores* artificialmente maiores devido √† soma de mais *impact scores*. Uma poss√≠vel modifica√ß√£o seria:

$$
s(q, d) = \frac{\sum_{t_i \in q} \max_{t_j \in d, t_j = t_i} v_i z_j}{|d|^{\beta}}
$$

onde $|d|$ representa o tamanho do documento (e.g., n√∫mero de termos), e $\beta$ √© um hiperpar√¢metro ajust√°vel que controla o grau de normaliza√ß√£o.

> üí° **Exemplo Num√©rico:**
>
> Usando o mesmo exemplo anterior (UniCOIL), e supondo que $|d| = 8$ (n√∫mero de termos no documento) e $\beta = 0.5$.
>
> $$
> s(q, d) = \frac{1.390625}{8^{0.5}} = \frac{1.390625}{2.828427} \approx 0.4916
> $$
>
> A normaliza√ß√£o reduz o score para levar em conta o tamanho do documento.  Isso √© especialmente √∫til quando comparando documentos de diferentes comprimentos.

### Conclus√£o
<!-- END -->