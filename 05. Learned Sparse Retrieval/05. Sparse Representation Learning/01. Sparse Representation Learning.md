## 5.3 Sparse Representation Learning: Simultaneous Expansion and Term Importance Estimation

### Introdu√ß√£o
Em continuidade com as t√©cnicas de **Learned Sparse Retrieval**, a se√ß√£o anterior introduziu as abordagens de *document expansion learning* e *impact score learning* [^35]. Este cap√≠tulo foca em uma t√©cnica mais integrada: *sparse representation learning*. Diferentemente das abordagens anteriores, que tratam a expans√£o de documentos e a atribribui√ß√£o de pontua√ß√µes de impacto de termos como etapas separadas, a sparse representation learning busca aprender ambos simultaneamente [^40]. Esta abordagem hol√≠stica permite uma representa√ß√£o mais coerente e eficiente dos documentos, capturando melhor as rela√ß√µes sem√¢nticas entre os termos e a relev√¢ncia geral do documento.

### Conceitos Fundamentais
O objetivo principal da **sparse representation learning** √© projetar os *output embeddings* de um modelo *encoder-only* diretamente no vocabul√°rio de entrada [^40]. Para cada termo de entrada no documento, um modelo de linguagem √© computado, resultando em uma distribui√ß√£o de probabilidade sobre todo o vocabul√°rio. Essa abordagem possibilita capturar as correla√ß√µes sem√¢nticas entre o termo de entrada e todos os outros termos na cole√ß√£o. Essas correla√ß√µes podem ser utilizadas para:

1.  **Expandir o texto de entrada** com termos altamente correlacionados.
2.  **Comprimir o texto de entrada** removendo termos com baixas probabilidades em rela√ß√£o aos outros termos.

Modelos *encoder-only*, como BERT [^15], j√° computam modelos de linguagem baseados em termos como parte de seu treinamento como modelos de linguagem mascarados. Formalmente, dado um documento $d$, juntamente com os *output embeddings* $\Psi_{[CLS]}, \Psi_1, ..., \Psi_{|d|}$, um modelo *encoder-only* tamb√©m retorna os *masked language heads* $X_1, ..., X_{|d|}$, um para cada token no documento [^40]. Aqui, $X_i \in \mathbb{R}^{|V|}$ para $i = 1, ..., |d|$ √© uma estimativa da import√¢ncia de cada palavra no vocabul√°rio, impl√≠cita pelo $i$-√©simo token no documento $d$.

O sistema SPLADE (Sparse Lexical and Expansion Model for First Stage Ranking) [Formal et al. 2021] √© um exemplo paradigm√°tico dessa abordagem [^40]. Para um dado documento $d \in D$, SPLADE calcula seus *masked language heads* $X_1, ..., X_{|d|}$ utilizando o BERT, filtra e soma esses vetores de tamanho do vocabul√°rio em um √∫nico vetor $\gamma(d) \in \mathbb{R}^{|V|}$ que representa o documento todo, e ent√£o usa esse vetor para representar o documento em si, junto com as pontua√ß√µes de import√¢ncia de termos. A f√≥rmula para $\gamma(d)$ √© dada por [^40]:

$$
\gamma(d) = \sum_{i=1}^{|d|} log(1 + ReLU(X_i)) \qquad (28)
$$

onde o logaritmo e as fun√ß√µes ReLU s√£o computadas *element-wise* [^41]. O logaritmo evita que alguns termos com valores grandes dominem, e a fun√ß√£o ReLU lida com os componentes negativos de $\gamma(d)$.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um documento simples com a frase "gato preto dorme". Suponha que ap√≥s passar pelo BERT, os *masked language heads* $X_i$ para cada token resultem nos seguintes valores (apenas para alguns termos do vocabul√°rio para simplificar):
>
> | Termo     | $X_{gato}$ | $X_{preto}$ | $X_{dorme}$ | $X_{cachorro}$ | $X_{corre}$ | ... |
> |-----------|-------------|-------------|-------------|---------------|-------------|-----|
> | gato      | 2.0         | 0.5         | -0.2        | 0.1           | -0.3        | ... |
> | preto     | 0.6         | 1.5         | 0.0         | -0.1          | -0.2        | ... |
> | dorme     | -0.1        | 0.2         | 1.8         | -0.2          | 0.4         | ... |
>
> $\text{Passo 1: Aplicar ReLU(X_i)}$:  Substitu√≠mos todos os valores negativos por 0.
>
> | Termo     | $ReLU(X_{gato})$ | $ReLU(X_{preto})$ | $ReLU(X_{dorme})$ |
> |-----------|--------------------|--------------------|--------------------|
> | gato      | 2.0               | 0.5               | 0.0               |
> | preto     | 0.6               | 1.5               | 0.0               |
> | dorme     | 0.0               | 0.2               | 1.8               |
>
> $\text{Passo 2: Aplicar log(1 + ReLU(X_i))}$:
>
> | Termo     | $log(1 + ReLU(X_{gato}))$ | $log(1 + ReLU(X_{preto}))$ | $log(1 + ReLU(X_{dorme}))$ |
> |-----------|-----------------------------|-----------------------------|-----------------------------|
> | gato      | log(3.0) ‚âà 1.10           | log(1.6) ‚âà 0.47           | log(1.0) = 0.0              |
> | preto     | log(1.6) ‚âà 0.47           | log(2.5) ‚âà 0.92           | log(1.0) = 0.0              |
> | dorme     | log(1.0) = 0.0              | log(1.2) ‚âà 0.18           | log(2.8) ‚âà 1.03           |
>
> $\text{Passo 3: Somar os vetores element-wise para obter } \gamma(d)$:
>
> | Termo     | $\gamma(d)$          |
> |-----------|-----------------------|
> | gato      | 1.10 + 0.47 + 0.0 ‚âà 1.57 |
> | preto     | 0.47 + 0.92 + 0.18 ‚âà 1.57 |
> | dorme     | 0.0 + 0.0 + 1.03 ‚âà 1.03 |
> | cachorro  | ...                   |
> | corre     | ...                   |
>
> Interpreta√ß√£o:  Os termos "gato" e "preto" t√™m pontua√ß√µes de import√¢ncia similares, enquanto "dorme" tem uma pontua√ß√£o ligeiramente menor.  Outros termos no vocabul√°rio, n√£o presentes no documento original, tamb√©m ter√£o pontua√ß√µes em $\gamma(d)$ devido √† expans√£o sem√¢ntica.  O ReLU remove ativa√ß√µes negativas, enquanto o logaritmo modera a influ√™ncia de ativa√ß√µes muito altas, impedindo que um √∫nico termo domine a representa√ß√£o.

**Observa√ß√£o:** A escolha do logaritmo e da fun√ß√£o ReLU na Equa√ß√£o (28) √© crucial para o desempenho do SPLADE. Outras fun√ß√µes poderiam ser utilizadas, mas a combina√ß√£o destas duas se mostrou eficaz na pr√°tica. Em particular, o ReLU garante que apenas termos com impacto positivo sejam considerados, enquanto o logaritmo atenua o impacto de termos muito frequentes ou com pontua√ß√µes muito altas.

A representa√ß√£o do documento $\gamma$ potencialmente cont√©m todos os termos no vocabul√°rio, mesmo se o logaritmo e as fun√ß√µes ReLU na Eq. (28) zerarem alguns de seus componentes [^41]. Para aprender a "tornar esparsas" as representa√ß√µes do documento, Formal et al. [2021] aproveitam o regularizador FLOPS $\mathcal{L}_{FLOPS}$ [Paria et al. 2020] [^41]. Como parte da fun√ß√£o de perda SPLADE utilizada durante o treinamento, a perda FLOPS √© computada como a soma, atrav√©s dos termos no vocabul√°rio, da probabilidade quadr√°tica $p^2$ que um termo $w$ tem um peso n√£o-zero em um documento [^41]. Minimizar a perda FLOPS coincide com minimizar os pesos n√£o-zero em um documento, i.e., maximizar o n√∫mero de pesos zero em um documento. A opera√ß√£o quadrada auxilia na redu√ß√£o de pesos de termos altos mais do que pesos de termos baixos.

A probabilidade que um termo $w \in V$ tem um peso n√£o-zero em um documento $d$ √© proporcional ao peso m√©dio daquele termo $\gamma_t(d)$ estimado atrav√©s da cole√ß√£o toda. Para tornar a computa√ß√£o poss√≠vel, a m√©dia √© computada em um *batch* $b$ de documentos durante o treinamento, considerado como uma amostra representativa da cole√ß√£o toda. Assim, a perda FLOPS √© dada por [^41]:

$$
\mathcal{L}_{FLOPS} = P^2 = \sum_{t \in V} (\frac{1}{|b|} \sum_{d \in b} \gamma_t(d))^2 \qquad (29)
$$

> üí° **Exemplo Num√©rico:**
>
> Considere um vocabul√°rio reduzido $V = \{gato, preto, dorme, cachorro, corre\}$ e um batch $b$ de tamanho 2.  Suponha que os vetores $\gamma(d)$ para os dois documentos no batch (ap√≥s a aplica√ß√£o de ReLU e log) sejam:
>
> $\gamma(d_1) = [1.57, 1.57, 1.03, 0.0, 0.0]$
> $\gamma(d_2) = [1.20, 0.0, 0.80, 1.10, 0.50]$
>
> $\text{Passo 1: Calcular a m√©dia de } \gamma_t(d) \text{ para cada termo } t \text{ no vocabul√°rio}$:
>
> $\text{M√©dia}(gato) = (1.57 + 1.20) / 2 = 1.385$
> $\text{M√©dia}(preto) = (1.57 + 0.0) / 2 = 0.785$
> $\text{M√©dia}(dorme) = (1.03 + 0.80) / 2 = 0.915$
> $\text{M√©dia}(cachorro) = (0.0 + 1.10) / 2 = 0.55$
> $\text{M√©dia}(corre) = (0.0 + 0.50) / 2 = 0.25$
>
> $\text{Passo 2: Elevar ao quadrado as m√©dias}$:
>
> $(1.385)^2 ‚âà 1.92$
> $(0.785)^2 ‚âà 0.62$
> $(0.915)^2 ‚âà 0.84$
> $(0.55)^2 ‚âà 0.30$
> $(0.25)^2 ‚âà 0.06$
>
> $\text{Passo 3: Somar os valores ao quadrado para obter } \mathcal{L}_{FLOPS}$:
>
> $\mathcal{L}_{FLOPS} = 1.92 + 0.62 + 0.84 + 0.30 + 0.06 = 3.74$
>
> Interpreta√ß√£o: A perda $\mathcal{L}_{FLOPS}$ √© uma medida da "densidade" da representa√ß√£o. Quanto maior a perda, mais termos t√™m pesos significativos, e menos esparsa √© a representa√ß√£o. O objetivo do treinamento SPLADE √© *minimizar* essa perda, for√ßando o modelo a concentrar a import√¢ncia em um subconjunto menor de termos relevantes. Termos como "gato" contribuem mais para a perda FLOPS porque, em m√©dia, t√™m um peso maior nos documentos do batch.

**Teorema 1:** A minimiza√ß√£o da perda $\mathcal{L}_{FLOPS}$ induz uma distribui√ß√£o de probabilidade esparsa sobre o vocabul√°rio.

*Prova*: A perda $\mathcal{L}_{FLOPS}$ penaliza a presen√ßa de termos com pesos n√£o-nulos, proporcionalmente ao quadrado da sua frequ√™ncia m√©dia no batch. Ao minimizar essa perda, o modelo √© incentivado a reduzir o n√∫mero de termos com pesos significativos, concentrando a probabilidade em um subconjunto menor do vocabul√°rio, gerando assim uma representa√ß√£o esparsa. Formalmente, a minimiza√ß√£o da Equa√ß√£o (29) busca um ponto de equil√≠brio onde apenas os termos mais relevantes para a representa√ß√£o do documento mant√™m pesos significativos, enquanto os demais s√£o for√ßados a zero. $\blacksquare$

**Corol√°rio 1:** Uma representa√ß√£o esparsa de documentos e queries, obtida atrav√©s da minimiza√ß√£o de $\mathcal{L}_{FLOPS}$, pode levar a uma redu√ß√£o significativa no custo computacional durante a indexa√ß√£o e a recupera√ß√£o.

*Prova*: A esparsidade reduz o n√∫mero de opera√ß√µes necess√°rias para calcular similaridades entre documentos e queries. Com menos termos n√£o-zero, o produto escalar entre vetores esparsos se torna mais eficiente, resultando em menor lat√™ncia na busca. $\blacksquare$

### Observa√ß√µes
*   SPLADE n√£o limita a expans√£o apenas aos documentos [^41].
*   A Eq. (28) pode ser aplicada a uma query $q$, para computar o vetor correspondente $\gamma(q) \in \mathbb{R}^{|V|}$.
*   Essa expans√£o de query deve ser realizada no tempo de processamento da query.
*   Para reduzir a lat√™ncia, a query expandida deve ser muito mais esparsa que um documento.
*   Formal et al. [2021] adotam dois regularizadores FLOPS distintos para documentos e queries [^41].

**Proposi√ß√£o 1:** A utiliza√ß√£o de regularizadores FLOPS distintos para documentos e queries permite um controle mais granular sobre a esparsidade de cada um, possibilitando otimizar o balanceamento entre efic√°cia e lat√™ncia na recupera√ß√£o.

*Prova*: Ao ajustar os par√¢metros dos regularizadores FLOPS separadamente, √© poss√≠vel controlar o n√≠vel de esparsidade desejado para documentos e queries. Queries podem ser representadas de forma mais esparsa para reduzir a lat√™ncia na busca, enquanto documentos podem manter uma representa√ß√£o um pouco mais densa para garantir uma melhor precis√£o. A escolha dos par√¢metros dos regularizadores pode ser otimizada atrav√©s de experimentos, considerando as caracter√≠sticas espec√≠ficas da cole√ß√£o e os requisitos de desempenho do sistema de busca. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que tenhamos ajustado os regularizadores FLOPS para documentos e queries de forma diferente, resultando nas seguintes esparsidades m√©dias ap√≥s o treinamento:
>
> | Tipo       | Regularizador FLOPS | Esparsidade M√©dia | Lat√™ncia M√©dia (ms) | Precis√£o M√©dia |
> |------------|---------------------|-------------------|-----------------------|----------------|
> | Documento  | $\lambda_d = 0.1$   | 70%               | N/A                   | 0.75           |
> | Query      | $\lambda_q = 0.5$   | 90%               | 50                    | 0.70           |
>
> Interpreta√ß√£o:  A query tem uma esparsidade maior (90%) do que o documento (70%) devido a um regularizador FLOPS mais forte ($\lambda_q > \lambda_d$). Isso resulta em uma lat√™ncia menor no tempo de busca, mas tamb√©m em uma pequena queda na precis√£o. Este √© um exemplo do *trade-off* entre lat√™ncia e precis√£o que pode ser controlado ajustando os regularizadores FLOPS de forma independente. Um sistema de busca pode optar por priorizar a lat√™ncia (usando queries mais esparsas) ou a precis√£o (usando documentos mais densos e, potencialmente, queries mais densas tamb√©m). A escolha ideal depender√° dos requisitos da aplica√ß√£o.
>
> **An√°lise de Erro:** Se a precis√£o da query cair muito ao aumentar a esparsidade, isso pode indicar que termos importantes est√£o sendo removidos.  Nesse caso, pode ser necess√°rio refinar o processo de treinamento ou usar uma estrat√©gia de expans√£o de query mais seletiva.

### Conclus√£o
A sparse representation learning oferece uma abordagem poderosa para integrar a expans√£o de documentos e a estimativa da import√¢ncia de termos no contexto de *neural information retrieval* [^41]. Ao projetar *embeddings* no vocabul√°rio de entrada, essas t√©cnicas capturam as rela√ß√µes sem√¢nticas cruciais, levando a representa√ß√µes de documentos mais informativas e eficientes. As vantagens da sparse representation learning incluem melhorias na efic√°cia da busca, custos reduzidos de computa√ß√£o e a capacidade de lidar com vocabul√°rios grandes. Ao contr√°rio das abordagens tradicionais, que tratam a expans√£o de documentos e a pontua√ß√£o de impacto de termos separadamente, a sparse representation learning permite que ambos os processos informem e se beneficiem uns dos outros, levando a um modelo de recupera√ß√£o mais coerente e eficaz.
### Refer√™ncias
[^15]: J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. NAACL.

[^35]: Ver a se√ß√£o anterior "Learned Sparse Retrieval"

[^40]: T. Formal, B. Piwowarski, and S. Clinchant. 2021. SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking. In Proc. SIGIR.

[^41]: Vide SPLADE description in section 5.3
<!-- END -->