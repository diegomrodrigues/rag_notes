## Sparse Representation Learning com Modelos Encoder-Only: SPLADE

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre *Learned Sparse Retrieval*, este cap√≠tulo se aprofunda em t√©cnicas que visam incorporar a efic√°cia dos *neural networks* em *inverted indexes* [^35]. O foco desta se√ß√£o √© o modelo SPLADE (Sparse Lexical and Expansion Model for First Stage Ranking) [^40], que se destaca por utilizar modelos *encoder-only*, como BERT, para computar *term-based language models* e gerar *masked language heads* como parte de seu treinamento. Essas *masked language heads* fornecem uma estimativa da import√¢ncia de cada palavra no vocabul√°rio para cada token no documento, permitindo uma representa√ß√£o esparsa e eficaz dos documentos.

### Modelagem com SPLADE
O SPLADE adota uma abordagem inovadora para representar documentos e suas respectivas *term importance scores* [^40]. Em vez de depender de expans√£o de documentos ou *impact score learning* separadamente, o SPLADE integra esses conceitos projetando os *output embeddings* de um modelo *encoder-only* diretamente no vocabul√°rio de entrada. Para cada termo de entrada no documento, o modelo computa um *language model*, resultando em uma distribui√ß√£o de probabilidade sobre todo o vocabul√°rio.

Formalmente, dado um documento $d$, um modelo *encoder-only* retorna n√£o apenas os *output embeddings* $\psi_{[CLS]}, \psi_1, ..., \psi_{|d|}$, mas tamb√©m os *masked language heads* $X_1, ..., X_{|d|}$. Cada $X_i \in \mathbb{R}^{|V|}$ (onde $|V|$ √© o tamanho do vocabul√°rio) representa uma estimativa da import√¢ncia de cada palavra no vocabul√°rio, impl√≠cita pelo *i-√©simo* token no documento $d$ [^40].

O SPLADE calcula ent√£o um vetor $\gamma(d) \in \mathbb{R}^{|V|}$ que representa o documento como um todo, juntamente com seus *term importance scores*. Este vetor √© computado como:

$$
\gamma(d) = \sum_{i=1}^{|d|} \log (1 + ReLU(X_i))
$$

onde:

- $X_i$ √© o *masked language head* para o *i-√©simo* token no documento [^40].
- $ReLU$ √© a fun√ß√£o de ativa√ß√£o ReLU (Rectified Linear Unit), definida como $ReLU(x) = max(0, x)$ [^40].
- $log$ √© o logaritmo natural [^40].

#### Detalhes da Equa√ß√£o
1. **Masked Language Heads:** Os vetores $X_i$ s√£o obtidos como parte do treinamento do modelo *encoder-only*, como BERT [^40]. No treinamento com *masked language modeling* (MLM), o modelo aprende a prever *tokens* mascarados (ocultos) em uma sequ√™ncia de entrada. Os $X_i$ representam a capacidade do modelo de prever cada *token* do vocabul√°rio, dado o contexto do *i-√©simo* token [^15, 40].
2. **ReLU Activation:** A fun√ß√£o ReLU √© aplicada para lidar com componentes negativos nos vetores $X_i$. ReLU define todos os valores negativos como zero, permitindo que o modelo se concentre em *tokens* positivamente correlacionados [^40].
3. **Logarithm:** O logaritmo √© aplicado para mitigar o efeito de termos com valores muito altos, evitando que dominem a representa√ß√£o do documento [^40]. Isso ajuda a equilibrar a import√¢ncia de v√°rios termos no documento.
4. **Summation:** A soma dos vetores transformados (pelo logaritmo e ReLU) fornece um vetor final $\gamma(d)$ que representa o documento [^40]. Cada entrada neste vetor corresponde a um *token* no vocabul√°rio e indica a relev√¢ncia desse *token* para o documento.

#### Exemplo
Considere um documento com dois *tokens*, $t_1$ e $t_2$, com *masked language heads* $X_1$ e $X_2$. O vetor $\gamma(d)$ ser√° a soma dos vetores transformados correspondentes a cada *token*: $\gamma(d) = \log(1 + ReLU(X_1)) + \log(1 + ReLU(X_2))$ [^40].

Para ilustrar melhor, vamos considerar um exemplo simplificado onde os *masked language heads* $X_1$ e $X_2$ s√£o representados como vetores com tr√™s elementos, correspondendo a tr√™s termos do vocabul√°rio, $w_1$, $w_2$ e $w_3$.

$X_1 = [0.5, -0.2, 1.0]$
$X_2 = [-0.1, 0.8, 0.3]$

Aplicando a fun√ß√£o ReLU:

$ReLU(X_1) = [0.5, 0.0, 1.0]$
$ReLU(X_2) = [0.0, 0.8, 0.3]$

Adicionando 1 e aplicando o logaritmo natural:

$\log(1 + ReLU(X_1)) = \log([1.5, 1.0, 2.0]) \approx [0.405, 0.0, 0.693]$
$\log(1 + ReLU(X_2)) = \log([1.0, 1.8, 1.3]) \approx [0.0, 0.588, 0.262]$

Finalmente, somando os vetores resultantes:

$\gamma(d) = [0.405 + 0.0, 0.0 + 0.588, 0.693 + 0.262] = [0.405, 0.588, 0.955]$

Neste exemplo simplificado, o vetor $\gamma(d)$ indica que o termo $w_3$ √© o mais relevante para o documento $d$, seguido por $w_2$ e $w_1$.

> üí° **Exemplo Num√©rico:** Para entender melhor o impacto da fun√ß√£o ReLU, vamos considerar um termo com valor negativo em $X_1$, por exemplo, -1.0.
> $X_1 = [0.5, -1.0, 1.0]$
> $ReLU(X_1) = [0.5, 0.0, 1.0]$
> O valor negativo √© zerado, removendo a influ√™ncia negativa do termo. Isso √© crucial para garantir que apenas termos positivamente correlacionados contribuam para a representa√ß√£o do documento.

**Proposi√ß√£o 1** (Normaliza√ß√£o de $\gamma(d)$): O vetor $\gamma(d)$ pode ser normalizado para obter uma distribui√ß√£o de probabilidade sobre os termos do vocabul√°rio.

*Prova*: Para normalizar $\gamma(d)$, podemos aplicar a fun√ß√£o softmax:

$$
p_t = \frac{e^{\gamma_t(d)}}{\sum_{t' \in V} e^{\gamma_{t'}(d)}}
$$

Onde $p_t$ √© a probabilidade do termo $t$ ser relevante para o documento $d$. Essa normaliza√ß√£o garante que $\sum_{t \in V} p_t = 1$, transformando $\gamma(d)$ em uma distribui√ß√£o de probabilidade.

> üí° **Exemplo Num√©rico:** Usando o $\gamma(d)$ calculado anteriormente: $\gamma(d) = [0.405, 0.588, 0.955]$. Aplicando softmax:
>
> $p_1 = \frac{e^{0.405}}{e^{0.405} + e^{0.588} + e^{0.955}} \approx \frac{1.499}{1.499 + 1.801 + 2.599} \approx 0.254$
> $p_2 = \frac{e^{0.588}}{e^{0.405} + e^{0.588} + e^{0.955}} \approx \frac{1.801}{1.499 + 1.801 + 2.599} \approx 0.305$
> $p_3 = \frac{e^{0.955}}{e^{0.405} + e^{0.588} + e^{0.955}} \approx \frac{2.599}{1.499 + 1.801 + 2.599} \approx 0.441$
>
> Portanto, $p = [0.254, 0.305, 0.441]$. A soma das probabilidades √© aproximadamente 1. O softmax real√ßa ainda mais a import√¢ncia do termo $w_3$.

### Regulariza√ß√£o e Esparsidade
A representa√ß√£o do documento $\gamma$ pode conter termos de todo o vocabul√°rio, mesmo que as fun√ß√µes de logaritmo e ReLU possam zerar alguns componentes [^40]. Para tornar as representa√ß√µes de documentos mais esparsas e eficientes, o SPLADE emprega um regularizador FLOPS (Floating Point Operations) [^40].

O regularizador FLOPS, denotado como $\mathcal{L}_{FLOPS}$, √© definido como:

$$
\mathcal{L}_{FLOPS} = \sum_{t \in V} p_t^2
$$

Onde $p_t$ √© a probabilidade de que um termo $t$ tenha um peso diferente de zero em um documento. Minimizar $\mathcal{L}_{FLOPS}$ coincide com a minimiza√ß√£o dos pesos diferentes de zero em um documento, ou seja, maximizar o n√∫mero de pesos zero [^40].

Em termos pr√°ticos, a probabilidade $p_t$ de que um termo $w$ tenha um peso diferente de zero em um documento $d$ √© proporcional ao peso m√©dio desse termo $\gamma_t(d)$ estimado em toda a cole√ß√£o [^40]. Para tornar a computa√ß√£o vi√°vel, a m√©dia √© computada em um *batch* $b$ de documentos durante o treinamento, considerado como uma amostra representativa de toda a cole√ß√£o [^40].

$$
\mathcal{L}_{FLOPS} = \sum_{t \in V} \left( \frac{1}{|b|} \sum_{d \in b} \gamma_t(d) \right)^2
$$

> üí° **Exemplo Num√©rico:** Considere um batch de 2 documentos. Para o termo $w_3$, temos $\gamma_3(d_1) = 0.955$ e $\gamma_3(d_2) = 0.2$. O peso m√©dio para $w_3$ √© $\frac{0.955 + 0.2}{2} = 0.5775$. O regularizador FLOPS penaliza termos que, em m√©dia, t√™m pesos altos, incentivando a esparsidade.

Uma alternativa √† regulariza√ß√£o FLOPS √© o uso de regulariza√ß√£o L1, que penaliza a soma dos valores absolutos dos pesos.

**Proposi√ß√£o 2** (Regulariza√ß√£o L1): A regulariza√ß√£o L1 tamb√©m pode ser usada para promover a esparsidade em $\gamma(d)$.

*Prova*: A regulariza√ß√£o L1 √© definida como:

$$
\mathcal{L}_{L1} = \lambda \sum_{t \in V} |\gamma_t(d)|
$$

Onde $\lambda$ √© um hiperpar√¢metro que controla a for√ßa da regulariza√ß√£o. Adicionar $\mathcal{L}_{L1}$ √† fun√ß√£o de perda durante o treinamento incentivar√° o modelo a definir muitos dos $\gamma_t(d)$ como zero, promovendo a esparsidade.

> üí° **Exemplo Num√©rico:**  Se $\lambda = 0.1$ e $\gamma(d) = [0.405, 0.588, 0.955]$, ent√£o
> $\mathcal{L}_{L1} = 0.1 * (0.405 + 0.588 + 0.955) = 0.1 * 1.948 = 0.1948$.
> Minimizar essa perda durante o treinamento incentivar√° o modelo a reduzir os valores em $\gamma(d)$, tornando a representa√ß√£o mais esparsa.

### Aplica√ß√£o a Consultas
A expans√£o com SPLADE n√£o se limita apenas a documentos. A equa√ß√£o para $\gamma$ pode ser aplicada a uma consulta $q$ para computar o vetor correspondente $\gamma(q) \in \mathbb{R}^{|V|}$ [^40]. No entanto, a expans√£o da consulta deve ser realizada em *query processing time*, para reduzir a lat√™ncia [^40]. Portanto, a consulta expandida deve ser mais esparsa do que um documento. Para impor esse comportamento diferente, o modelo SPLADE adota dois regularizadores FLOPS distintos para documentos e consultas [^40].

Para garantir que a consulta expandida seja mais esparsa que o documento, pode-se definir um hiperpar√¢metro $\alpha$ tal que $\mathcal{L}_{FLOPS}^{query} = \alpha \mathcal{L}_{FLOPS}^{document}$, onde $\alpha > 1$. Isso imp√µe uma penalidade maior para termos n√£o-zero nas consultas.

**Teorema 1** (Impacto de $\alpha$ na Sparsidade da Consulta): Aumentar o valor de $\alpha$ resulta em representa√ß√µes de consulta mais esparsas.

*Prova*: Um valor maior de $\alpha$ significa que o modelo √© mais fortemente penalizado por ter termos n√£o-zero na representa√ß√£o da consulta. Portanto, durante o treinamento, o modelo ir√° favorecer representa√ß√µes de consulta com menos termos n√£o-zero, ou seja, representa√ß√µes mais esparsas.

> üí° **Exemplo Num√©rico:** Seja $\mathcal{L}_{FLOPS}^{document} = 0.5$ e $\alpha = 2$. Ent√£o $\mathcal{L}_{FLOPS}^{query} = 2 * 0.5 = 1.0$.  A regulariza√ß√£o FLOPS para a consulta √© duas vezes maior que para o documento. Isso significa que o modelo ser√° mais penalizado por ter termos n√£o-zero na consulta, incentivando uma representa√ß√£o mais esparsa.
> |  | Term 1 | Term 2 | Term 3 | L1 Norm | FLOPS |
> |---|---|---|---|---|---|
> | Document  ($\gamma(d)$) | 0.405 | 0.588 | 0.955 | 1.948 | 1.27 |
> | Query ($\gamma(q), \alpha=2$) | 0.1 | 0.2 | 0.3 | 0.6 | 0.14 |
> O exemplo mostra uma redu√ß√£o nos valores dos termos para a consulta, indicando maior esparsidade. O valor FLOPS tamb√©m diminui dramaticamente.



![Example of DocT5Query model generating related queries for document expansion.](./../images/image1.png)

### Conclus√£o
O SPLADE representa um avan√ßo significativo na *learned sparse retrieval*, aproveitando *masked language models* para capturar a sem√¢ntica e a import√¢ncia dos termos nos documentos e consultas [^40]. Atrav√©s do uso estrat√©gico das fun√ß√µes de logaritmo e ReLU e da regulariza√ß√£o FLOPS, SPLADE consegue criar representa√ß√µes esparsas que s√£o eficazes e computacionalmente gerenci√°veis, abrindo caminho para uma integra√ß√£o eficiente de t√©cnicas neurais em *inverted indexes*. A capacidade de aplicar princ√≠pios semelhantes a documentos e consultas, mantendo diferentes graus de *sparsidade*, aumenta ainda mais a versatilidade desta abordagem.

### Refer√™ncias
[^15]: Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)* (pp. 4171-4186).
[^35]: Ver texto anterior.
[^40]: Formal, T., Piwowarski, B., & Clinchant, S. (2021). SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking. In *Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval* (pp. 2288-2292).
<!-- END -->