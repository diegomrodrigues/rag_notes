## Sparse Representation Learning with FLOPS Regularization in SPLADE

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre *Learned Sparse Retrieval* (LSR), esta se√ß√£o se aprofunda em uma t√©cnica espec√≠fica, focando em como o modelo **SPLADE** (Sparse Lexical and Expansion Model for First Stage Ranking) utiliza o **FLOPS regularizer** para obter representa√ß√µes documentais esparsas [^40]. Conforme introduzido anteriormente, o objetivo geral do LSR √© incorporar os ganhos de efic√°cia dos modelos neurais de *Information Retrieval* (IR) em estruturas de √≠ndice invertido, aproveitando seus eficientes algoritmos de processamento de consultas [^35]. SPLADE, em particular, projeta os *output embeddings* de um modelo *encoder-only* no vocabul√°rio de entrada, calculando para cada termo de entrada em um documento um modelo de linguagem, ou seja, uma distribui√ß√£o de probabilidade sobre todo o vocabul√°rio [^40].

**Proposi√ß√£o 1.** *O uso de modelos encoder-only para projetar embeddings no vocabul√°rio permite que o SPLADE capture rela√ß√µes sem√¢nticas entre termos, indo al√©m da simples co-ocorr√™ncia.*

*Proof.* Modelos encoder-only, como Transformers, s√£o treinados para entender o contexto das palavras. Ao projetar os embeddings no vocabul√°rio, o modelo aprende a associar pesos mais altos aos termos que s√£o semanticamente relacionados ao documento, mesmo que n√£o estejam presentes explicitamente no texto. Essa capacidade de inferir rela√ß√µes sem√¢nticas √© fundamental para melhorar a precis√£o da recupera√ß√£o.

> üí° **Exemplo Num√©rico:** Suponha que temos um documento contendo a frase "gato preto". Um modelo encoder-only pode associar pesos altos n√£o apenas aos termos "gato" e "preto", mas tamb√©m a termos como "felino" e "animal de estima√ß√£o", mesmo que estes n√£o estejam presentes no documento original. Isso ocorre porque o modelo aprendeu, durante o treinamento, que esses termos est√£o semanticamente relacionados a "gato".

### Sparsifica√ß√£o com FLOPS Regularizer
O n√∫cleo da contribui√ß√£o de SPLADE reside no uso estrat√©gico do **FLOPS regularizer** para induzir a *sparsidade* nas representa√ß√µes dos documentos [^40]. A motiva√ß√£o por tr√°s dessa abordagem √© dupla:
1.  **Redu√ß√£o da complexidade computacional**: Representa√ß√µes esparsas permitem que os √≠ndices invertidos armazenem apenas os termos mais relevantes para cada documento, diminuindo o espa√ßo de armazenamento e acelerando o processamento das consultas [^9].
2.  **Melhoria da interpretabilidade**: Ao for√ßar o modelo a se concentrar nos termos mais importantes, a representa√ß√£o resultante se torna mais f√°cil de interpretar e entender, oferecendo *insights* sobre o conte√∫do do documento [^10].

O **FLOPS regularizer** √© formulado de forma a minimizar os pesos n√£o nulos em um documento, ou seja, maximizar o n√∫mero de pesos zerados [^41]. Matematicamente, isso √© expresso como:

$$
\mathcal{L}_{FLOPS} = P^2 = \sum_{t \in \mathcal{V}} p_t^2 = \sum_{t \in \mathcal{V}} \left( \frac{1}{|\mathcal{D}|} \sum_{d \in \mathcal{D}} \gamma_t(d) \right)^2
$$

onde:
*   $\mathcal{V}$ representa o vocabul√°rio [^6].
*   $p_t$ √© a probabilidade de que um termo $t$ tenha um peso diferente de zero em um documento [^41].
*   $\mathcal{D}$ √© o conjunto de documentos [^8].
*   $\gamma_t(d)$ √© o peso do termo $t$ no documento $d$, estimado atrav√©s da cole√ß√£o inteira [^41].

A opera√ß√£o de quadratura na f√≥rmula beneficia a redu√ß√£o de pesos maiores em rela√ß√£o aos menores, incentivando o modelo a remover completamente os termos menos relevantes [^41]. Para tornar o c√°lculo vi√°vel, a m√©dia √© estimada em um *batch* $b$ de documentos durante o treinamento, considerado uma amostra representativa de toda a cole√ß√£o:

$$
\mathcal{L}_{FLOPS} = \sum_{t \in \mathcal{V}} p_t^2 = \sum_{t \in \mathcal{V}} \left( \frac{1}{|b|} \sum_{d \in b} \gamma_t(d) \right)^2
$$

Essa formula√ß√£o do FLOPS regularizer √© integrada √† fun√ß√£o de perda do SPLADE durante o treinamento, guiando o modelo para aprender representa√ß√µes esparsas que preservem a capacidade de classifica√ß√£o [^41].

> üí° **Exemplo Num√©rico:** Considere um vocabul√°rio $\mathcal{V} = \{termo_1, termo_2, termo_3, termo_4\}$ e um batch de dois documentos ($|b|=2$). Ap√≥s a proje√ß√£o, temos os seguintes pesos para cada termo em cada documento:
>
> *   Documento 1: $\gamma(d_1) = \{0.8, 0.2, 0.0, 0.0\}$
> *   Documento 2: $\gamma(d_2) = \{0.7, 0.3, 0.0, 0.0\}$
>
> $\text{Step 1: Calcular a probabilidade } p_t \text{ para cada termo:}$
>
> $p_{termo_1} = \frac{0.8 + 0.7}{2} = 0.75$
> $p_{termo_2} = \frac{0.2 + 0.3}{2} = 0.25$
> $p_{termo_3} = \frac{0.0 + 0.0}{2} = 0.0$
> $p_{termo_4} = \frac{0.0 + 0.0}{2} = 0.0$
>
> $\text{Step 2: Calcular } \mathcal{L}_{FLOPS}:$
>
> $\mathcal{L}_{FLOPS} = 0.75^2 + 0.25^2 + 0.0^2 + 0.0^2 = 0.5625 + 0.0625 = 0.625$
>
> O FLOPS regularizer penaliza a presen√ßa dos termos $termo_1$ e $termo_2$ na representa√ß√£o dos documentos, incentivando o modelo a tornar esses pesos mais pr√≥ximos de zero em itera√ß√µes futuras do treinamento.  A quadratura favorece a redu√ß√£o do peso de $termo_1$ em rela√ß√£o a $termo_2$ por ter um peso maior inicialmente.

**Teorema 2.** *A converg√™ncia do treinamento com o FLOPS regularizer implica na obten√ß√£o de um conjunto de termos representativos para cada documento.*

*Proof.* (Esbo√ßo) A fun√ß√£o de perda, ao incluir o FLOPS regularizer, penaliza a magnitude dos pesos dos termos. Ao longo do treinamento, o modelo ajusta os pesos para minimizar a perda total. A converg√™ncia implica que o modelo encontrou um equil√≠brio onde os termos com pesos significativos s√£o aqueles que contribuem mais para a precis√£o da recupera√ß√£o, ou seja, os termos representativos. A penaliza√ß√£o dos pesos dos termos n√£o representativos for√ßa seus pesos a se aproximarem de zero.

**Lema 2.1.** *A escolha do tamanho do batch $b$ influencia a estabilidade e a velocidade de converg√™ncia do treinamento.*

*Proof.* Um batch muito pequeno pode levar a uma estimativa ruidosa da probabilidade $p_t$, resultando em instabilidade no treinamento. Um batch muito grande pode tornar o treinamento computacionalmente caro e lento. A escolha ideal do tamanho do batch depende do tamanho do vocabul√°rio e da distribui√ß√£o dos termos na cole√ß√£o de documentos.

> üí° **Exemplo Num√©rico:** Se usarmos um batch size de 1 (apenas um documento por vez), a probabilidade $p_t$ seria simplesmente o peso do termo naquele documento. No exemplo anterior, para o Documento 1, ter√≠amos $p_{termo_1} = 0.8$, $p_{termo_2} = 0.2$, e assim por diante. Isso seria uma estimativa muito ruidosa da verdadeira probabilidade de cada termo, pois depende apenas de um √∫nico documento. Um batch maior fornece uma estimativa mais robusta, mas exige mais poder computacional.

### Expans√£o de Consultas e Regulariza√ß√£o Distinta
SPLADE tamb√©m permite a expans√£o de consultas atrav√©s da computa√ß√£o de um vetor correspondente $\gamma(q) \in \mathbb{R}^{|\mathcal{V}|}$ [^41]. No entanto, para evitar lat√™ncia excessiva durante o tempo de consulta, a expans√£o da consulta deve ser mais esparsa do que a expans√£o do documento [^41]. Para impor esse comportamento, o SPLADE adota dois regularizadores FLOPS distintos para documentos e consultas [^41]. Isso garante que a expans√£o da consulta, embora presente, seja suficientemente esparsa para manter a efici√™ncia de pesquisa [^35].

> üí° **Exemplo Num√©rico:** Suponha que definimos um peso de regulariza√ß√£o FLOPS $\lambda_{doc}$ para documentos e $\lambda_{query}$ para consultas, com $\lambda_{query} > \lambda_{doc}$. Isso significa que estamos penalizando mais os termos n√£o nulos nas consultas do que nos documentos. Por exemplo, $\lambda_{doc} = 0.1$ e $\lambda_{query} = 0.5$. Durante o treinamento, a fun√ß√£o de perda para documentos ser√° $\mathcal{L} + \lambda_{doc} \mathcal{L}_{FLOPS}^{doc}$, e para consultas ser√° $\mathcal{L} + \lambda_{query} \mathcal{L}_{FLOPS}^{query}$. A maior penalidade para consultas resultar√° em vetores de consulta mais esparsos.





![Example of DocT5Query model generating related queries for document expansion.](./../images/image1.png)

**Teorema 3.** *A aplica√ß√£o de regularizadores FLOPS distintos para documentos e consultas otimiza o trade-off entre precis√£o e lat√™ncia na recupera√ß√£o.*

*Proof.* (Esbo√ßo) Ao impor maior sparsidade nas consultas expandidas, o n√∫mero de termos considerados durante a busca √© reduzido, diminuindo a lat√™ncia. Ao mesmo tempo, ao permitir uma representa√ß√£o menos esparsa dos documentos, o modelo mant√©m a capacidade de capturar nuances sem√¢nticas importantes para a precis√£o da recupera√ß√£o. A otimiza√ß√£o dos pesos dos regularizadores permite ajustar o equil√≠brio entre esses dois fatores.

**Corol√°rio 3.1.** *A diferen√ßa na intensidade dos regularizadores FLOPS para documentos e consultas √© um hiperpar√¢metro crucial que afeta o desempenho do SPLADE.*

> üí° **Exemplo Num√©rico:**  Imagine que, ap√≥s treinar o SPLADE com diferentes valores para $\lambda_{doc}$ e $\lambda_{query}$, obtemos os seguintes resultados em um conjunto de testes:
>
> | $\lambda_{doc}$ | $\lambda_{query}$ | Precis√£o | Lat√™ncia (ms) |
> |-----------------|-------------------|----------|-----------------|
> | 0.1             | 0.1             | 0.75     | 50              |
> | 0.1             | 0.5             | 0.72     | 30              |
> | 0.5             | 0.5             | 0.68     | 25              |
>
> A tabela mostra que aumentar $\lambda_{query}$ reduz a lat√™ncia (torna a busca mais r√°pida) √† custa de uma pequena diminui√ß√£o na precis√£o. A escolha dos valores ideais depender√° dos requisitos espec√≠ficos da aplica√ß√£o (priorizar precis√£o ou lat√™ncia).

### Conclus√£o
SPLADE demonstra como o uso estrat√©gico de t√©cnicas de regulariza√ß√£o, como o **FLOPS regularizer**, pode efetivamente promover a *sparsidade* em representa√ß√µes de documentos neurais [^41]. Ao minimizar o n√∫mero de termos ativos em um documento, SPLADE consegue melhorar a efici√™ncia computacional e a interpretabilidade sem sacrificar a precis√£o da recupera√ß√£o [^35]. A capacidade de aplicar regulariza√ß√£o diferenciada para documentos e consultas permite um ajuste fino adicional, resultando em um sistema de *Information Retrieval* (IR) eficiente e eficaz [^40].
### Refer√™ncias
[^6]: Pag 6, Lecture Notes on Neural Information Retrieval.
[^8]: Pag 8, Lecture Notes on Neural Information Retrieval.
[^9]: Pag 9, Lecture Notes on Neural Information Retrieval.
[^10]: Pag 10, Lecture Notes on Neural Information Retrieval.
[^35]: Pag 35, Lecture Notes on Neural Information Retrieval.
[^40]: Pag 40, Lecture Notes on Neural Information Retrieval.
[^41]: Pag 41, Lecture Notes on Neural Information Retrieval.
<!-- END -->