## AvaliaÃ§Ãµes (Evals) na QuantificaÃ§Ã£o do Desempenho em RAG

### IntroduÃ§Ã£o

No contexto de sistemas de Neural Information Retrieval (NIR) e Retrieval-Augmented Generation (RAG) com Large Language Models (LLMs), a avaliaÃ§Ã£o rigorosa do desempenho Ã© fundamental para garantir a eficÃ¡cia e a confiabilidade. As avaliaÃ§Ãµes, ou *evals*, desempenham um papel crucial ao fornecer uma mÃ©trica quantitativa da capacidade do modelo em tarefas especÃ­ficas. A necessidade de *evals* torna-se ainda mais evidente Ã  medida que sistemas complexos evoluem, integrando componentes como LLMs, templates de prompts e parÃ¢metros de inferÃªncia. Este capÃ­tulo detalha a importÃ¢ncia das *evals*, seus benefÃ­cios e como elas permitem o rastreamento preciso e a detecÃ§Ã£o de regressÃµes ao longo do tempo [^1].

Para complementar esta introduÃ§Ã£o, podemos observar que diferentes tipos de *evals* se concentram em diferentes aspectos do sistema RAG. Por exemplo, algumas *evals* podem focar na relevÃ¢ncia dos documentos recuperados, enquanto outras podem se concentrar na qualidade da resposta gerada pelo LLM. A escolha da *eval* apropriada depende dos objetivos especÃ­ficos da avaliaÃ§Ã£o.

### Conceitos Fundamentais

**ImportÃ¢ncia da QuantificaÃ§Ã£o do Desempenho:**

A quantificaÃ§Ã£o do desempenho Ã© essencial para entender como os LLMs se comportam em diversas tarefas dentro dos sistemas RAG. As *evals* fornecem uma base objetiva para a comparaÃ§Ã£o de diferentes modelos, arquiteturas ou configuraÃ§Ãµes [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere dois sistemas RAG, $S_1$ e $S_2$. ApÃ³s *evals* em um conjunto de 100 perguntas, obtivemos as seguintes taxas de acerto (proporÃ§Ã£o de respostas corretas):
>
> *   $S_1$: 75 respostas corretas (75%)
> *   $S_2$: 85 respostas corretas (85%)
>
> Usando a mÃ©trica de taxa de acerto, podemos quantificar que $S_2$ supera $S_1$ em 10 pontos percentuais. Este nÃºmero fornece uma base objetiva para comparar o desempenho dos dois sistemas.
>
> Podemos tambÃ©m aplicar um teste estatÃ­stico, como um teste t, para verificar se a diferenÃ§a de 10% Ã© estatisticamente significativa, ou se poderia ser devido ao acaso.

**Teorema 1** (Comparabilidade via MÃ©tricas): Dados dois sistemas RAG, $S_1$ e $S_2$, e um conjunto de *evals* $E = \{e_1, e_2, \ldots, e_n\}$ com mÃ©tricas associadas $M = \{m_1, m_2, \ldots, m_n\}$, onde $m_i$ quantifica o desempenho em relaÃ§Ã£o a $e_i$, entÃ£o $S_1$ pode ser objetivamente comparado a $S_2$ se e somente se $M$ Ã© bem definido e consistente.

*Prova (esboÃ§o):* A comparabilidade requer uma base comum de avaliaÃ§Ã£o. As *evals* definem os aspectos a serem comparados, e as mÃ©tricas fornecem a quantificaÃ§Ã£o. Se as mÃ©tricas sÃ£o ambÃ­guas ou inconsistentes, a comparaÃ§Ã£o perde seu significado objetivo. A consistÃªncia implica que a mesma mÃ©trica aplicada em diferentes momentos ou em diferentes sistemas deve produzir resultados comparÃ¡veis, considerando as variaÃ§Ãµes inerentes ao processo.

**IdentificaÃ§Ã£o de RegressÃµes:**

Sistemas RAG estÃ£o em constante evoluÃ§Ã£o. MudanÃ§as nos LLMs subjacentes, nos templates de prompts ou nos parÃ¢metros de inferÃªncia podem inadvertidamente introduzir regressÃµes no desempenho. As *evals* permitem a identificaÃ§Ã£o dessas regressÃµes de forma rÃ¡pida e eficiente, permitindo correÃ§Ãµes proativas [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Um sistema RAG tem um desempenho inicial de 80% de precisÃ£o. ApÃ³s uma atualizaÃ§Ã£o do LLM subjacente, as *evals* revelam que a precisÃ£o caiu para 70%.
>
> Aqui, $\delta = 10\%$.  A regressÃ£o Ã© evidente e indica a necessidade de investigar e corrigir a atualizaÃ§Ã£o. A equipe pode entÃ£o analisar os dados das *evals* para identificar os tipos de perguntas onde a regressÃ£o ocorreu e ajustar o sistema de acordo.

**ProposiÃ§Ã£o 1** (DetecÃ§Ã£o de RegressÃ£o): Seja $P_t$ o desempenho do sistema RAG no tempo $t$ avaliado por um conjunto de *evals* $E$. Uma regressÃ£o no desempenho Ã© detectada se existe um $\delta > 0$ e um tempo $t' > t$ tal que $P_{t'} < P_t - \delta$, onde $P_t$ e $P_{t'}$ sÃ£o vetores de mÃ©tricas de desempenho.

*Prova (esboÃ§o):* Essa proposiÃ§Ã£o formaliza a noÃ§Ã£o intuitiva de regressÃ£o. Ela estabelece que uma diminuiÃ§Ã£o significativa no desempenho, maior que um limiar $\delta$, em relaÃ§Ã£o a um ponto de referÃªncia anterior, indica uma regressÃ£o. A avaliaÃ§Ã£o vetorial permite capturar regressÃµes em mÃºltiplos aspectos do sistema RAG.

**MensuraÃ§Ã£o EscalÃ¡vel de MudanÃ§as no Sistema:**

A inspeÃ§Ã£o manual das saÃ­das dos LLMs pode ser impraticÃ¡vel e demorada, especialmente em sistemas complexos. *Evals*, usando conjuntos representativos de dados, oferecem uma alternativa escalÃ¡vel, permitindo a avaliaÃ§Ã£o rÃ¡pida e automatizada do impacto das mudanÃ§as no sistema [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Um sistema RAG Ã© avaliado usando 1000 perguntas. A avaliaÃ§Ã£o manual levaria semanas, enquanto as *evals* automatizadas podem fornecer resultados em poucas horas. Se a taxa de custo para avaliaÃ§Ã£o manual Ã© de R\$50/hora e uma pessoa leva 1 hora por pergunta, enquanto uma *eval* automatizada custa R\$100 e leva 2 horas, o custo para avaliaÃ§Ã£o manual seria de R\$50.000, enquanto o custo para *eval* automatizada seria de R\$100.
>
> Este exemplo ilustra a escalabilidade e a economia de tempo proporcionadas pelas *evals*.

**AvaliaÃ§Ã£o Quantitativa e Qualitativa:**

As *evals* nÃ£o se limitam apenas a mÃ©tricas quantitativas. Elas tambÃ©m podem fornecer *insights* qualitativos sobre o comportamento do modelo, identificando padrÃµes de erro ou Ã¡reas onde o modelo tem dificuldade. Esta combinaÃ§Ã£o de avaliaÃ§Ã£o quantitativa e qualitativa Ã© essencial para um entendimento completo do desempenho do sistema [^1].

Para ilustrar a importÃ¢ncia da avaliaÃ§Ã£o qualitativa, considere um cenÃ¡rio onde a mÃ©trica quantitativa de precisÃ£o permanece constante apÃ³s uma atualizaÃ§Ã£o do sistema RAG. Uma anÃ¡lise qualitativa pode revelar que, embora a precisÃ£o geral nÃ£o tenha mudado, o sistema agora comete erros em categorias especÃ­ficas de perguntas que antes respondia corretamente. Isso destacaria uma regressÃ£o sutil que nÃ£o seria detectada apenas por mÃ©tricas quantitativas.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que um sistema RAG tenha uma precisÃ£o de 85% antes e depois de uma atualizaÃ§Ã£o. Uma anÃ¡lise qualitativa revela que antes da atualizaÃ§Ã£o, os erros eram distribuÃ­dos aleatoriamente entre diferentes tÃ³picos. ApÃ³s a atualizaÃ§Ã£o, todos os erros estÃ£o concentrados em perguntas sobre "histÃ³ria da arte". Embora a precisÃ£o geral seja a mesma, a anÃ¡lise qualitativa revela um problema especÃ­fico que precisa ser abordado.

**Rastreamento e DetecÃ§Ã£o de RegressÃµes ao Longo do Tempo:**

A capacidade de rastrear o desempenho ao longo do tempo Ã© crucial para garantir a estabilidade do sistema. *Evals* regulares permitem a detecÃ§Ã£o precoce de regressÃµes, permitindo que as equipes de desenvolvimento tomem medidas corretivas antes que problemas impactem os usuÃ¡rios [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Um sistema RAG Ã© avaliado mensalmente usando um conjunto de *evals*. Os resultados sÃ£o registrados em um *dashboard* de desempenho. ApÃ³s 6 meses, o *dashboard* revela uma tendÃªncia de queda na precisÃ£o nas Ãºltimas duas avaliaÃ§Ãµes. Isso alerta a equipe para investigar possÃ­veis causas, como mudanÃ§as nos dados de treinamento ou nos parÃ¢metros do LLM.
>
> Uma tabela pode representar essa evoluÃ§Ã£o:
>
> | MÃªs   | PrecisÃ£o | Recall |
> | :----- | :------- | :----- |
> | Jan    | 85%      | 70%    |
> | Fev    | 86%      | 72%    |
> | Mar    | 84%      | 71%    |
> | Abr    | 85%      | 70%    |
> | Maio   | 82%      | 68%    |
> | Jun    | 79%      | 65%    |
>
> A tabela mostra uma queda notÃ¡vel na precisÃ£o e no recall nos meses de Maio e Junho, indicando uma possÃ­vel regressÃ£o.

**BenefÃ­cios das Evals:**

*   **Objetividade:** As *evals* fornecem uma medida objetiva do desempenho, livre de viÃ©ses subjetivos.
*   **Reprodutibilidade:** Os resultados das *evals* sÃ£o reproduzÃ­veis, permitindo a comparaÃ§Ã£o consistente do desempenho ao longo do tempo.

![Comparativo da reprodutibilidade e comparabilidade de avaliaÃ§Ãµes de modelos de machine learning com ROUGE.](./../images/image9.jpg)

*   **Escalabilidade:** As *evals* podem ser automatizadas e aplicadas a grandes conjuntos de dados, tornando-as escalÃ¡veis para sistemas complexos.
*   **DetecÃ§Ã£o Precoce de Problemas:** As *evals* permitem a identificaÃ§Ã£o precoce de regressÃµes, permitindo correÃ§Ãµes proativas.
*   **Melhoria ContÃ­nua:** Os resultados das *evals* podem ser usados para identificar Ã¡reas de melhoria, impulsionando o desenvolvimento contÃ­nuo do sistema.

Adicionalmente aos benefÃ­cios listados, as *evals* tambÃ©m facilitam a comunicaÃ§Ã£o entre diferentes equipes (por exemplo, pesquisa, engenharia e produto) ao fornecer uma linguagem comum para discutir o desempenho do sistema. Elas tambÃ©m permitem a criaÃ§Ã£o de *dashboards* de desempenho que podem ser monitorados por todas as partes interessadas.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Uma equipe de desenvolvimento usa *evals* para comparar duas abordagens diferentes para a recuperaÃ§Ã£o de documentos em um sistema RAG. A abordagem A tem uma precisÃ£o de 70% e um recall de 60%, enquanto a abordagem B tem uma precisÃ£o de 80% e um recall de 75%. Com base nesses resultados, a equipe decide implementar a abordagem B, pois ela oferece um melhor equilÃ­brio entre precisÃ£o e recall.
>
> | Abordagem | PrecisÃ£o | Recall |
> | :-------- | :------- | :----- |
> | A         | 70%      | 60%    |
> | B         | 80%      | 75%    |
<!-- END -->