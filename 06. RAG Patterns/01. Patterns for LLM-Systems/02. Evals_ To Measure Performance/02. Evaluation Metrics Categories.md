## MÃ©tricas de AvaliaÃ§Ã£o Contextuais e Acontextuais em RAG

### IntroduÃ§Ã£o

A avaliaÃ§Ã£o da performance de modelos de *Retrieval-Augmented Generation* (RAG) Ã© uma tarefa complexa, que exige mÃ©tricas capazes de capturar nuances tanto na qualidade da recuperaÃ§Ã£o de informaÃ§Ã£o quanto na geraÃ§Ã£o de respostas coerentes e relevantes. Uma forma Ãºtil de categorizar as mÃ©tricas de avaliaÃ§Ã£o Ã© separÃ¡-las em dois grupos principais: mÃ©tricas dependentes do contexto e mÃ©tricas independentes do contexto (context-free) [^1]. Este capÃ­tulo explora em detalhe essas duas categorias, analisando suas caracterÃ­sticas, vantagens, desvantagens e aplicabilidade em diferentes cenÃ¡rios de avaliaÃ§Ã£o de RAG.

### Conceitos Fundamentais

**MÃ©tricas Dependentes do Contexto:**

As mÃ©tricas dependentes do contexto sÃ£o aquelas que consideram o contexto especÃ­fico da tarefa em avaliaÃ§Ã£o. Isso significa que elas levam em conta fatores como o domÃ­nio do conhecimento, o tipo de pergunta, o formato da resposta esperada e outras caracterÃ­sticas especÃ­ficas do problema em questÃ£o [^1]. Devido a essa dependÃªncia, essas mÃ©tricas geralmente exigem ajustes ou adaptaÃ§Ãµes para serem reutilizadas em diferentes tarefas.

A principal vantagem das mÃ©tricas dependentes do contexto Ã© sua capacidade de fornecer uma avaliaÃ§Ã£o mais precisa e relevante da performance do modelo em uma tarefa especÃ­fica. Ao considerar o contexto, elas podem capturar nuances que mÃ©tricas genÃ©ricas podem nÃ£o detectar. No entanto, essa mesma dependÃªncia Ã© tambÃ©m sua principal desvantagem: a necessidade de adaptaÃ§Ã£o limita sua reutilizaÃ§Ã£o e pode aumentar o custo e a complexidade do processo de avaliaÃ§Ã£o [^1].

Exemplos de mÃ©tricas dependentes do contexto incluem:

*   **PrecisÃ£o e RevocaÃ§Ã£o (Precision and Recall) Adaptadas:** Em um sistema RAG, a precisÃ£o pode ser definida como a proporÃ§Ã£o de documentos relevantes recuperados em relaÃ§Ã£o ao total de documentos recuperados, enquanto a revocaÃ§Ã£o Ã© a proporÃ§Ã£o de documentos relevantes recuperados em relaÃ§Ã£o ao total de documentos relevantes existentes no corpus. No entanto, para serem aplicadas de forma eficaz, essas mÃ©tricas precisam ser adaptadas ao contexto especÃ­fico da tarefa, definindo critÃ©rios claros para determinar o que constitui um documento "relevante" [^1].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos um sistema RAG que recupera 10 documentos em resposta a uma consulta. ApÃ³s a avaliaÃ§Ã£o manual, determinamos que 7 desses documentos sÃ£o realmente relevantes para a consulta. AlÃ©m disso, sabemos que existem um total de 15 documentos relevantes no corpus completo.
>
> *   **PrecisÃ£o:** (Documentos relevantes recuperados) / (Total de documentos recuperados) = 7 / 10 = 0.7 ou 70%
> *   **RevocaÃ§Ã£o:** (Documentos relevantes recuperados) / (Total de documentos relevantes no corpus) = 7 / 15 = 0.47 ou 47%
>
> Isso significa que o sistema recuperou 70% dos documentos que retornou eram relevantes, mas sÃ³ conseguiu encontrar 47% de todos os documentos relevantes existentes.
>
> Uma baixa revocaÃ§Ã£o pode indicar que o sistema precisa melhorar sua capacidade de encontrar todos os documentos relevantes, possivelmente ajustando os parÃ¢metros de busca ou expandindo o corpus de conhecimento.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Dados do exemplo
> precisao = 0.7
> revocacao = 0.47
>
> # Preparar dados para o grÃ¡fico de barras
> labels = ['PrecisÃ£o', 'RevocaÃ§Ã£o']
> valores = [precisao, revocacao]
>
> # Criar o grÃ¡fico
> plt.figure(figsize=(8, 6))
> plt.bar(labels, valores, color=['blue', 'green'])
>
> # Adicionar tÃ­tulo e rÃ³tulos
> plt.title('PrecisÃ£o e RevocaÃ§Ã£o do Sistema RAG')
> plt.ylabel('Taxa')
> plt.ylim(0, 1)  # Definir limite do eixo Y de 0 a 1
>
> # Adicionar valores acima das barras
> for i, valor in enumerate(valores):
>     plt.text(i, valor + 0.02, str(round(valor, 2)), ha='center')
>
> # Adicionar grade
> plt.grid(axis='y', linestyle='--')
>
> # Mostrar o grÃ¡fico
> plt.tight_layout()
> plt.show()
> ```

*   **F1-Score Adaptado:** O F1-Score, que Ã© a mÃ©dia harmÃ´nica da precisÃ£o e revocaÃ§Ã£o, tambÃ©m precisa de adaptaÃ§Ã£o para refletir a importÃ¢ncia relativa da precisÃ£o e revocaÃ§Ã£o no contexto da tarefa.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Usando os valores de precisÃ£o (0.7) e revocaÃ§Ã£o (0.47) do exemplo anterior:
>
> $$\text{F1-Score} = 2 \cdot \frac{\text{PrecisÃ£o} \cdot \text{RevocaÃ§Ã£o}}{\text{PrecisÃ£o} + \text{RevocaÃ§Ã£o}}$$
>
> $$\text{F1-Score} = 2 \cdot \frac{0.7 \cdot 0.47}{0.7 + 0.47} = 2 \cdot \frac{0.329}{1.17} = 0.562$$
>
> Um F1-Score de 0.562 indica um equilÃ­brio moderado entre precisÃ£o e revocaÃ§Ã£o. Se a tarefa exigir alta precisÃ£o, podemos priorizar ajustes que aumentem a precisÃ£o, mesmo que isso diminua um pouco a revocaÃ§Ã£o e vice-versa.

*   **MÃ©tricas de Qualidade da Resposta EspecÃ­ficas:** MÃ©tricas que avaliam a correÃ§Ã£o, coerÃªncia, relevÃ¢ncia e completude da resposta gerada pelo modelo, considerando o contexto da pergunta e a informaÃ§Ã£o recuperada. Estas mÃ©tricas podem exigir a definiÃ§Ã£o de *guidelines* detalhadas e a utilizaÃ§Ã£o de *annotators* humanos para garantir a consistÃªncia e a objetividade da avaliaÃ§Ã£o [^1].

Para complementar a discussÃ£o sobre mÃ©tricas de qualidade da resposta, podemos considerar tambÃ©m a avaliaÃ§Ã£o da fidelidade da resposta ao contexto recuperado. Uma mÃ©trica importante nesse sentido Ã© a **Confiabilidade da Resposta (Answer Faithfulness)**.

**Teorema 1 (Confiabilidade da Resposta):** A Confiabilidade da Resposta mede o grau em que as informaÃ§Ãµes presentes na resposta gerada pelo modelo RAG sÃ£o suportadas pelas informaÃ§Ãµes contidas nos documentos recuperados. Uma resposta com alta confiabilidade Ã© aquela que se baseia primariamente nas evidÃªncias fornecidas pelo contexto recuperado, minimizando a introduÃ§Ã£o de informaÃ§Ãµes espÃºrias ou nÃ£o suportadas.

A avaliaÃ§Ã£o da Confiabilidade da Resposta pode ser realizada atravÃ©s de mÃ©todos de AnÃ¡lise da Linguagem Natural (NLP), como a identificaÃ§Ã£o de Named Entity Recognition (NER) e a verificaÃ§Ã£o da presenÃ§a e suporte dessas entidades nos documentos de contexto. Alternativamente, pode ser avaliada por *annotators* humanos, que verificam se cada afirmaÃ§Ã£o na resposta Ã© suportada pelas informaÃ§Ãµes nos documentos recuperados.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Imagine que um avaliador humano analisa a resposta gerada por um modelo RAG e a compara com os documentos de contexto recuperados. O avaliador identifica 5 afirmaÃ§Ãµes distintas na resposta. ApÃ³s anÃ¡lise, determina-se que 4 dessas 5 afirmaÃ§Ãµes sÃ£o diretamente suportadas por evidÃªncias encontradas nos documentos de contexto.
>
> $$\text{Confiabilidade da Resposta} = \frac{\text{AfirmaÃ§Ãµes suportadas}}{\text{Total de afirmaÃ§Ãµes}} = \frac{4}{5} = 0.8$$ ou 80%
>
> Neste caso, a confiabilidade da resposta Ã© de 80%, indicando que a grande maioria das informaÃ§Ãµes apresentadas na resposta Ã© baseada em evidÃªncias recuperadas. Um valor baixo poderia indicar alucinaÃ§Ãµes ou a inclusÃ£o de informaÃ§Ãµes externas nÃ£o suportadas pelo contexto.

**MÃ©tricas Acontextuais (Context-Free):**

As mÃ©tricas acontextuais, por outro lado, comparam a saÃ­da do modelo diretamente com referÃªncias consideradas "ouro" (gold references), sem levar em conta o contexto especÃ­fico da tarefa [^1]. Isso as torna mais facilmente aplicÃ¡veis em diferentes tarefas, pois nÃ£o exigem adaptaÃ§Ãµes complexas.

A principal vantagem das mÃ©tricas acontextuais Ã© sua simplicidade e generalidade. Elas podem ser aplicadas em uma ampla gama de tarefas sem a necessidade de ajustes significativos. No entanto, essa mesma generalidade Ã© tambÃ©m sua principal desvantagem: elas podem nÃ£o ser capazes de capturar nuances importantes relacionadas ao contexto da tarefa, levando a avaliaÃ§Ãµes menos precisas em alguns casos [^1].

Exemplos de mÃ©tricas acontextuais incluem:

*   **BLEU (Bilingual Evaluation Understudy):** Uma mÃ©trica amplamente utilizada em traduÃ§Ã£o automÃ¡tica, que mede a similaridade entre a saÃ­da do modelo e uma ou mais referÃªncias "ouro" [^1]. Ela calcula a precisÃ£o de *n-grams* (sequÃªncias de *n* palavras) na saÃ­da do modelo em relaÃ§Ã£o Ã s referÃªncias, penalizando frases muito curtas.
*   **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Um conjunto de mÃ©tricas que avaliam a qualidade do resumo automÃ¡tico, medindo a revocaÃ§Ã£o de *n-grams*, *longest common subsequences* e outras caracterÃ­sticas da saÃ­da do modelo em relaÃ§Ã£o Ã s referÃªncias [^1].

![Percentage of ROUGE package citations referencing software with scoring errors.](./../images/image13.jpg)

*   **METEOR (Metric for Evaluation of Translation with Explicit Ordering):** Uma mÃ©trica que tenta melhorar o BLEU, considerando sinÃ´nimos, stemizaÃ§Ã£o e outras variaÃ§Ãµes linguÃ­sticas [^1].

AlÃ©m das mÃ©tricas mencionadas, Ã© importante considerar mÃ©tricas que avaliam a similaridade semÃ¢ntica entre a resposta gerada e as referÃªncias "ouro". Uma mÃ©trica relevante nesse contexto Ã© o **BERTScore**.

**Teorema 2 (BERTScore):** O BERTScore utiliza embeddings contextuais gerados pelo modelo BERT para calcular a similaridade entre tokens na resposta gerada e tokens na referÃªncia "ouro". Essa mÃ©trica considera o significado das palavras no contexto da frase, proporcionando uma avaliaÃ§Ã£o mais precisa da similaridade semÃ¢ntica do que as mÃ©tricas baseadas em *n-grams*.

O BERTScore calcula a precisÃ£o, revocaÃ§Ã£o e F1-Score com base na similaridade dos embeddings contextuais. Ele oferece uma avaliaÃ§Ã£o mais robusta da qualidade da resposta, especialmente em casos onde a resposta gerada utiliza palavras diferentes, mas transmite o mesmo significado da referÃªncia.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que a resposta gerada pelo modelo RAG seja: "O gato estava sentado no tapete." e a referÃªncia "ouro" seja: "Havia um gato sentado no tapete."
>
> MÃ©tricas tradicionais baseadas em *n-grams* podem penalizar a resposta gerada devido Ã s pequenas diferenÃ§as nas palavras ("O" vs "Havia"). No entanto, o BERTScore, ao considerar os embeddings contextuais das palavras, reconheceria que ambas as frases transmitem o mesmo significado.
>
> $$\text{BERTScore (PrecisÃ£o)} = 0.92$$
> $$\text{BERTScore (RevocaÃ§Ã£o)} = 0.95$$
> $$\text{BERTScore (F1-Score)} = 0.93$$
>
> Estes valores altos indicam que, semanticamente, a resposta gerada Ã© muito similar Ã  referÃªncia "ouro", mesmo que as palavras exatas nÃ£o correspondam perfeitamente.



![Illustration contrasting BERTScore's one-to-one alignment with MoverScore's many-to-one mapping of semantically related words.](./../images/image12.jpg)

**ConsideraÃ§Ãµes sobre a Escolha da MÃ©trica:**

A escolha entre mÃ©tricas dependentes do contexto e acontextuais depende dos objetivos da avaliaÃ§Ã£o e dos recursos disponÃ­veis. Se o objetivo Ã© obter uma avaliaÃ§Ã£o precisa e relevante da performance do modelo em uma tarefa especÃ­fica, mÃ©tricas dependentes do contexto sÃ£o geralmente preferÃ­veis, mesmo que exijam mais esforÃ§o de adaptaÃ§Ã£o [^1]. Se o objetivo Ã© obter uma avaliaÃ§Ã£o rÃ¡pida e geral da performance do modelo em vÃ¡rias tarefas, mÃ©tricas acontextuais podem ser mais adequadas, embora possam ser menos precisas em alguns casos [^1].

Em muitos casos, uma combinaÃ§Ã£o de mÃ©tricas dependentes do contexto e acontextuais pode ser a melhor abordagem, permitindo capturar diferentes aspectos da performance do modelo e obter uma avaliaÃ§Ã£o mais completa e equilibrada. AlÃ©m disso, a escolha das mÃ©tricas deve levar em conta a natureza dos dados e o tipo de tarefa.

**ProposiÃ§Ã£o 1 (AdaptaÃ§Ã£o da MÃ©trica para Dados MultilÃ­ngues):** Ao avaliar modelos RAG em contextos multilÃ­ngues, Ã© essencial adaptar as mÃ©tricas de avaliaÃ§Ã£o para lidar com as particularidades de cada idioma. MÃ©tricas como BLEU e ROUGE podem apresentar limitaÃ§Ãµes devido Ã s diferenÃ§as na estrutura gramatical e no vocabulÃ¡rio entre os idiomas. Nesses casos, Ã© recomendÃ¡vel utilizar mÃ©tricas que considerem a similaridade semÃ¢ntica, como o BERTScore, ou adaptar as mÃ©tricas existentes para cada idioma, utilizando recursos de traduÃ§Ã£o automÃ¡tica ou *embeddings* especÃ­ficos de cada idioma.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que estejamos avaliando um modelo RAG que gera respostas em portuguÃªs a partir de documentos em inglÃªs. Se usarmos o BLEU diretamente, podemos obter resultados baixos porque a estrutura das frases em portuguÃªs pode diferir significativamente da estrutura das frases originais em inglÃªs, mesmo que o significado seja o mesmo.
>
> Nesse caso, podemos usar um modelo de traduÃ§Ã£o automÃ¡tica para traduzir a referÃªncia "ouro" para portuguÃªs e, em seguida, calcular o BLEU entre a resposta gerada e a referÃªncia traduzida. Alternativamente, podemos usar mÃ©tricas como o BERTScore, que sÃ£o menos sensÃ­veis Ã s diferenÃ§as na estrutura da frase, pois consideram a similaridade semÃ¢ntica.
>
> | MÃ©trica              | Resultado sem AdaptaÃ§Ã£o | Resultado com AdaptaÃ§Ã£o (TraduÃ§Ã£o) |
> | --------------------- | ------------------------ | ------------------------------------ |
> | BLEU                 | 0.25                     | 0.68                                 |
> | BERTScore (F1-Score) | 0.72                     | 0.75                                 |
>
> Este exemplo ilustra como a adaptaÃ§Ã£o das mÃ©tricas para contextos multilÃ­ngues pode levar a uma avaliaÃ§Ã£o mais precisa e representativa da qualidade do modelo RAG.

### ConclusÃ£o

A avaliaÃ§Ã£o de modelos RAG exige uma consideraÃ§Ã£o cuidadosa das mÃ©tricas a serem utilizadas. A escolha entre mÃ©tricas dependentes do contexto e acontextuais deve ser baseada nos objetivos especÃ­ficos da avaliaÃ§Ã£o, nos recursos disponÃ­veis e na natureza da tarefa em questÃ£o. Compreender as vantagens e desvantagens de cada tipo de mÃ©trica Ã© fundamental para garantir uma avaliaÃ§Ã£o precisa, relevante e Ãºtil da performance do modelo. A utilizaÃ§Ã£o de uma combinaÃ§Ã£o de mÃ©tricas, tanto contextuais quanto acontextuais, pode proporcionar uma visÃ£o mais abrangente e equilibrada da capacidade do sistema RAG. AlÃ©m disso, a adaptaÃ§Ã£o das mÃ©tricas para diferentes idiomas e a consideraÃ§Ã£o da fidelidade da resposta ao contexto sÃ£o aspectos cruciais para uma avaliaÃ§Ã£o completa e confiÃ¡vel.

### ReferÃªncias

[^1]: 06. RAG Patterns. Evals: To Measure Performance. 02. Evaluation metrics can be grouped into two categories: context-dependent, which consider the task context and require adjustments for reuse, and context-free, which compare the model output with gold references, and are thus more easily applicable across different tasks.
<!-- END -->