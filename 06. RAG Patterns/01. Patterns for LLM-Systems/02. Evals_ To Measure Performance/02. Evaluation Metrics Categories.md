## M√©tricas de Avalia√ß√£o Contextuais e Acontextuais em RAG

### Introdu√ß√£o

A avalia√ß√£o da performance de modelos de *Retrieval-Augmented Generation* (RAG) √© uma tarefa complexa, que exige m√©tricas capazes de capturar nuances tanto na qualidade da recupera√ß√£o de informa√ß√£o quanto na gera√ß√£o de respostas coerentes e relevantes. Uma forma √∫til de categorizar as m√©tricas de avalia√ß√£o √© separ√°-las em dois grupos principais: m√©tricas dependentes do contexto e m√©tricas independentes do contexto (context-free) [^1]. Este cap√≠tulo explora em detalhe essas duas categorias, analisando suas caracter√≠sticas, vantagens, desvantagens e aplicabilidade em diferentes cen√°rios de avalia√ß√£o de RAG.

### Conceitos Fundamentais

**M√©tricas Dependentes do Contexto:**

As m√©tricas dependentes do contexto s√£o aquelas que consideram o contexto espec√≠fico da tarefa em avalia√ß√£o. Isso significa que elas levam em conta fatores como o dom√≠nio do conhecimento, o tipo de pergunta, o formato da resposta esperada e outras caracter√≠sticas espec√≠ficas do problema em quest√£o [^1]. Devido a essa depend√™ncia, essas m√©tricas geralmente exigem ajustes ou adapta√ß√µes para serem reutilizadas em diferentes tarefas.

A principal vantagem das m√©tricas dependentes do contexto √© sua capacidade de fornecer uma avalia√ß√£o mais precisa e relevante da performance do modelo em uma tarefa espec√≠fica. Ao considerar o contexto, elas podem capturar nuances que m√©tricas gen√©ricas podem n√£o detectar. No entanto, essa mesma depend√™ncia √© tamb√©m sua principal desvantagem: a necessidade de adapta√ß√£o limita sua reutiliza√ß√£o e pode aumentar o custo e a complexidade do processo de avalia√ß√£o [^1].

Exemplos de m√©tricas dependentes do contexto incluem:

*   **Precis√£o e Revoca√ß√£o (Precision and Recall) Adaptadas:** Em um sistema RAG, a precis√£o pode ser definida como a propor√ß√£o de documentos relevantes recuperados em rela√ß√£o ao total de documentos recuperados, enquanto a revoca√ß√£o √© a propor√ß√£o de documentos relevantes recuperados em rela√ß√£o ao total de documentos relevantes existentes no corpus. No entanto, para serem aplicadas de forma eficaz, essas m√©tricas precisam ser adaptadas ao contexto espec√≠fico da tarefa, definindo crit√©rios claros para determinar o que constitui um documento "relevante" [^1].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um sistema RAG que recupera 10 documentos em resposta a uma consulta. Ap√≥s a avalia√ß√£o manual, determinamos que 7 desses documentos s√£o realmente relevantes para a consulta. Al√©m disso, sabemos que existem um total de 15 documentos relevantes no corpus completo.
>
> *   **Precis√£o:** (Documentos relevantes recuperados) / (Total de documentos recuperados) = 7 / 10 = 0.7 ou 70%
> *   **Revoca√ß√£o:** (Documentos relevantes recuperados) / (Total de documentos relevantes no corpus) = 7 / 15 = 0.47 ou 47%
>
> Isso significa que o sistema recuperou 70% dos documentos que retornou eram relevantes, mas s√≥ conseguiu encontrar 47% de todos os documentos relevantes existentes.
>
> Uma baixa revoca√ß√£o pode indicar que o sistema precisa melhorar sua capacidade de encontrar todos os documentos relevantes, possivelmente ajustando os par√¢metros de busca ou expandindo o corpus de conhecimento.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Dados do exemplo
> precisao = 0.7
> revocacao = 0.47
>
> # Preparar dados para o gr√°fico de barras
> labels = ['Precis√£o', 'Revoca√ß√£o']
> valores = [precisao, revocacao]
>
> # Criar o gr√°fico
> plt.figure(figsize=(8, 6))
> plt.bar(labels, valores, color=['blue', 'green'])
>
> # Adicionar t√≠tulo e r√≥tulos
> plt.title('Precis√£o e Revoca√ß√£o do Sistema RAG')
> plt.ylabel('Taxa')
> plt.ylim(0, 1)  # Definir limite do eixo Y de 0 a 1
>
> # Adicionar valores acima das barras
> for i, valor in enumerate(valores):
>     plt.text(i, valor + 0.02, str(round(valor, 2)), ha='center')
>
> # Adicionar grade
> plt.grid(axis='y', linestyle='--')
>
> # Mostrar o gr√°fico
> plt.tight_layout()
> plt.show()
> ```

*   **F1-Score Adaptado:** O F1-Score, que √© a m√©dia harm√¥nica da precis√£o e revoca√ß√£o, tamb√©m precisa de adapta√ß√£o para refletir a import√¢ncia relativa da precis√£o e revoca√ß√£o no contexto da tarefa.

> üí° **Exemplo Num√©rico:**
>
> Usando os valores de precis√£o (0.7) e revoca√ß√£o (0.47) do exemplo anterior:
>
> $$\text{F1-Score} = 2 \cdot \frac{\text{Precis√£o} \cdot \text{Revoca√ß√£o}}{\text{Precis√£o} + \text{Revoca√ß√£o}}$$
>
> $$\text{F1-Score} = 2 \cdot \frac{0.7 \cdot 0.47}{0.7 + 0.47} = 2 \cdot \frac{0.329}{1.17} = 0.562$$
>
> Um F1-Score de 0.562 indica um equil√≠brio moderado entre precis√£o e revoca√ß√£o. Se a tarefa exigir alta precis√£o, podemos priorizar ajustes que aumentem a precis√£o, mesmo que isso diminua um pouco a revoca√ß√£o e vice-versa.

*   **M√©tricas de Qualidade da Resposta Espec√≠ficas:** M√©tricas que avaliam a corre√ß√£o, coer√™ncia, relev√¢ncia e completude da resposta gerada pelo modelo, considerando o contexto da pergunta e a informa√ß√£o recuperada. Estas m√©tricas podem exigir a defini√ß√£o de *guidelines* detalhadas e a utiliza√ß√£o de *annotators* humanos para garantir a consist√™ncia e a objetividade da avalia√ß√£o [^1].

Para complementar a discuss√£o sobre m√©tricas de qualidade da resposta, podemos considerar tamb√©m a avalia√ß√£o da fidelidade da resposta ao contexto recuperado. Uma m√©trica importante nesse sentido √© a **Confiabilidade da Resposta (Answer Faithfulness)**.

**Teorema 1 (Confiabilidade da Resposta):** A Confiabilidade da Resposta mede o grau em que as informa√ß√µes presentes na resposta gerada pelo modelo RAG s√£o suportadas pelas informa√ß√µes contidas nos documentos recuperados. Uma resposta com alta confiabilidade √© aquela que se baseia primariamente nas evid√™ncias fornecidas pelo contexto recuperado, minimizando a introdu√ß√£o de informa√ß√µes esp√∫rias ou n√£o suportadas.

A avalia√ß√£o da Confiabilidade da Resposta pode ser realizada atrav√©s de m√©todos de An√°lise da Linguagem Natural (NLP), como a identifica√ß√£o de Named Entity Recognition (NER) e a verifica√ß√£o da presen√ßa e suporte dessas entidades nos documentos de contexto. Alternativamente, pode ser avaliada por *annotators* humanos, que verificam se cada afirma√ß√£o na resposta √© suportada pelas informa√ß√µes nos documentos recuperados.

> üí° **Exemplo Num√©rico:**
>
> Imagine que um avaliador humano analisa a resposta gerada por um modelo RAG e a compara com os documentos de contexto recuperados. O avaliador identifica 5 afirma√ß√µes distintas na resposta. Ap√≥s an√°lise, determina-se que 4 dessas 5 afirma√ß√µes s√£o diretamente suportadas por evid√™ncias encontradas nos documentos de contexto.
>
> $$\text{Confiabilidade da Resposta} = \frac{\text{Afirma√ß√µes suportadas}}{\text{Total de afirma√ß√µes}} = \frac{4}{5} = 0.8$$ ou 80%
>
> Neste caso, a confiabilidade da resposta √© de 80%, indicando que a grande maioria das informa√ß√µes apresentadas na resposta √© baseada em evid√™ncias recuperadas. Um valor baixo poderia indicar alucina√ß√µes ou a inclus√£o de informa√ß√µes externas n√£o suportadas pelo contexto.

**M√©tricas Acontextuais (Context-Free):**

As m√©tricas acontextuais, por outro lado, comparam a sa√≠da do modelo diretamente com refer√™ncias consideradas "ouro" (gold references), sem levar em conta o contexto espec√≠fico da tarefa [^1]. Isso as torna mais facilmente aplic√°veis em diferentes tarefas, pois n√£o exigem adapta√ß√µes complexas.

A principal vantagem das m√©tricas acontextuais √© sua simplicidade e generalidade. Elas podem ser aplicadas em uma ampla gama de tarefas sem a necessidade de ajustes significativos. No entanto, essa mesma generalidade √© tamb√©m sua principal desvantagem: elas podem n√£o ser capazes de capturar nuances importantes relacionadas ao contexto da tarefa, levando a avalia√ß√µes menos precisas em alguns casos [^1].

Exemplos de m√©tricas acontextuais incluem:

*   **BLEU (Bilingual Evaluation Understudy):** Uma m√©trica amplamente utilizada em tradu√ß√£o autom√°tica, que mede a similaridade entre a sa√≠da do modelo e uma ou mais refer√™ncias "ouro" [^1]. Ela calcula a precis√£o de *n-grams* (sequ√™ncias de *n* palavras) na sa√≠da do modelo em rela√ß√£o √†s refer√™ncias, penalizando frases muito curtas.
*   **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Um conjunto de m√©tricas que avaliam a qualidade do resumo autom√°tico, medindo a revoca√ß√£o de *n-grams*, *longest common subsequences* e outras caracter√≠sticas da sa√≠da do modelo em rela√ß√£o √†s refer√™ncias [^1].

![Percentage of ROUGE package citations referencing software with scoring errors.](./../images/image13.jpg)

*   **METEOR (Metric for Evaluation of Translation with Explicit Ordering):** Uma m√©trica que tenta melhorar o BLEU, considerando sin√¥nimos, stemiza√ß√£o e outras varia√ß√µes lingu√≠sticas [^1].

Al√©m das m√©tricas mencionadas, √© importante considerar m√©tricas que avaliam a similaridade sem√¢ntica entre a resposta gerada e as refer√™ncias "ouro". Uma m√©trica relevante nesse contexto √© o **BERTScore**.

**Teorema 2 (BERTScore):** O BERTScore utiliza embeddings contextuais gerados pelo modelo BERT para calcular a similaridade entre tokens na resposta gerada e tokens na refer√™ncia "ouro". Essa m√©trica considera o significado das palavras no contexto da frase, proporcionando uma avalia√ß√£o mais precisa da similaridade sem√¢ntica do que as m√©tricas baseadas em *n-grams*.

O BERTScore calcula a precis√£o, revoca√ß√£o e F1-Score com base na similaridade dos embeddings contextuais. Ele oferece uma avalia√ß√£o mais robusta da qualidade da resposta, especialmente em casos onde a resposta gerada utiliza palavras diferentes, mas transmite o mesmo significado da refer√™ncia.

> üí° **Exemplo Num√©rico:**
>
> Suponha que a resposta gerada pelo modelo RAG seja: "O gato estava sentado no tapete." e a refer√™ncia "ouro" seja: "Havia um gato sentado no tapete."
>
> M√©tricas tradicionais baseadas em *n-grams* podem penalizar a resposta gerada devido √†s pequenas diferen√ßas nas palavras ("O" vs "Havia"). No entanto, o BERTScore, ao considerar os embeddings contextuais das palavras, reconheceria que ambas as frases transmitem o mesmo significado.
>
> $$\text{BERTScore (Precis√£o)} = 0.92$$
> $$\text{BERTScore (Revoca√ß√£o)} = 0.95$$
> $$\text{BERTScore (F1-Score)} = 0.93$$
>
> Estes valores altos indicam que, semanticamente, a resposta gerada √© muito similar √† refer√™ncia "ouro", mesmo que as palavras exatas n√£o correspondam perfeitamente.



![Illustration contrasting BERTScore's one-to-one alignment with MoverScore's many-to-one mapping of semantically related words.](./../images/image12.jpg)

**Considera√ß√µes sobre a Escolha da M√©trica:**

A escolha entre m√©tricas dependentes do contexto e acontextuais depende dos objetivos da avalia√ß√£o e dos recursos dispon√≠veis. Se o objetivo √© obter uma avalia√ß√£o precisa e relevante da performance do modelo em uma tarefa espec√≠fica, m√©tricas dependentes do contexto s√£o geralmente prefer√≠veis, mesmo que exijam mais esfor√ßo de adapta√ß√£o [^1]. Se o objetivo √© obter uma avalia√ß√£o r√°pida e geral da performance do modelo em v√°rias tarefas, m√©tricas acontextuais podem ser mais adequadas, embora possam ser menos precisas em alguns casos [^1].

Em muitos casos, uma combina√ß√£o de m√©tricas dependentes do contexto e acontextuais pode ser a melhor abordagem, permitindo capturar diferentes aspectos da performance do modelo e obter uma avalia√ß√£o mais completa e equilibrada. Al√©m disso, a escolha das m√©tricas deve levar em conta a natureza dos dados e o tipo de tarefa.

**Proposi√ß√£o 1 (Adapta√ß√£o da M√©trica para Dados Multil√≠ngues):** Ao avaliar modelos RAG em contextos multil√≠ngues, √© essencial adaptar as m√©tricas de avalia√ß√£o para lidar com as particularidades de cada idioma. M√©tricas como BLEU e ROUGE podem apresentar limita√ß√µes devido √†s diferen√ßas na estrutura gramatical e no vocabul√°rio entre os idiomas. Nesses casos, √© recomend√°vel utilizar m√©tricas que considerem a similaridade sem√¢ntica, como o BERTScore, ou adaptar as m√©tricas existentes para cada idioma, utilizando recursos de tradu√ß√£o autom√°tica ou *embeddings* espec√≠ficos de cada idioma.

> üí° **Exemplo Num√©rico:**
>
> Suponha que estejamos avaliando um modelo RAG que gera respostas em portugu√™s a partir de documentos em ingl√™s. Se usarmos o BLEU diretamente, podemos obter resultados baixos porque a estrutura das frases em portugu√™s pode diferir significativamente da estrutura das frases originais em ingl√™s, mesmo que o significado seja o mesmo.
>
> Nesse caso, podemos usar um modelo de tradu√ß√£o autom√°tica para traduzir a refer√™ncia "ouro" para portugu√™s e, em seguida, calcular o BLEU entre a resposta gerada e a refer√™ncia traduzida. Alternativamente, podemos usar m√©tricas como o BERTScore, que s√£o menos sens√≠veis √†s diferen√ßas na estrutura da frase, pois consideram a similaridade sem√¢ntica.
>
> | M√©trica              | Resultado sem Adapta√ß√£o | Resultado com Adapta√ß√£o (Tradu√ß√£o) |
> | --------------------- | ------------------------ | ------------------------------------ |
> | BLEU                 | 0.25                     | 0.68                                 |
> | BERTScore (F1-Score) | 0.72                     | 0.75                                 |
>
> Este exemplo ilustra como a adapta√ß√£o das m√©tricas para contextos multil√≠ngues pode levar a uma avalia√ß√£o mais precisa e representativa da qualidade do modelo RAG.

### Conclus√£o

A avalia√ß√£o de modelos RAG exige uma considera√ß√£o cuidadosa das m√©tricas a serem utilizadas. A escolha entre m√©tricas dependentes do contexto e acontextuais deve ser baseada nos objetivos espec√≠ficos da avalia√ß√£o, nos recursos dispon√≠veis e na natureza da tarefa em quest√£o. Compreender as vantagens e desvantagens de cada tipo de m√©trica √© fundamental para garantir uma avalia√ß√£o precisa, relevante e √∫til da performance do modelo. A utiliza√ß√£o de uma combina√ß√£o de m√©tricas, tanto contextuais quanto acontextuais, pode proporcionar uma vis√£o mais abrangente e equilibrada da capacidade do sistema RAG. Al√©m disso, a adapta√ß√£o das m√©tricas para diferentes idiomas e a considera√ß√£o da fidelidade da resposta ao contexto s√£o aspectos cruciais para uma avalia√ß√£o completa e confi√°vel.

### Refer√™ncias

[^1]: 06. RAG Patterns. Evals: To Measure Performance. 02. Evaluation metrics can be grouped into two categories: context-dependent, which consider the task context and require adjustments for reuse, and context-free, which compare the model output with gold references, and are thus more easily applicable across different tasks.
<!-- END -->