## ROUGE: AvaliaÃ§Ã£o Baseada em Recall para Textos Gerados

### IntroduÃ§Ã£o
Dentro do contexto de *Neural Information Retrieval and RAG with LLMs*, a avaliaÃ§Ã£o da qualidade dos textos gerados Ã© um aspecto crucial. MÃ©tricas como BLEU [06. RAG Patterns] focam na precisÃ£o do texto gerado em relaÃ§Ã£o a um texto de referÃªncia. No entanto, para avaliar o quÃ£o bem um texto gerado *cobre* o conteÃºdo essencial de um texto de referÃªncia, empregamos mÃ©tricas baseadas em *recall*. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) Ã© uma dessas mÃ©tricas, oferecendo uma perspectiva complementar Ã  precisÃ£o avaliada por BLEU. ROUGE quantifica a sobreposiÃ§Ã£o de palavras entre o texto gerado e o texto de referÃªncia [^1], sendo particularmente Ãºtil na avaliaÃ§Ã£o de tarefas de sumarizaÃ§Ã£o automÃ¡tica.

**Teorema 1:** *Complementaridade de ROUGE e BLEU*. ROUGE e BLEU avaliam diferentes aspectos da qualidade do texto gerado; enquanto BLEU avalia a precisÃ£o, ROUGE avalia a cobertura. Portanto, a combinaÃ§Ã£o de ambas as mÃ©tricas fornece uma avaliaÃ§Ã£o mais abrangente.

*Prova (EsboÃ§o):* A prova reside na definiÃ§Ã£o das mÃ©tricas. BLEU penaliza a geraÃ§Ã£o de palavras nÃ£o presentes na referÃªncia, focando na fidelidade. ROUGE recompensa a inclusÃ£o de informaÃ§Ãµes relevantes da referÃªncia no texto gerado, medindo a cobertura. A otimizaÃ§Ã£o de ambas as mÃ©tricas leva a textos precisos e completos. $\blacksquare$

### Conceitos Fundamentais
ROUGE Ã© uma famÃ­lia de mÃ©tricas, cada uma com um foco ligeiramente diferente na sobreposiÃ§Ã£o de palavras. As variaÃ§Ãµes mais comuns sÃ£o:

*   **ROUGE-N:** Mede a sobreposiÃ§Ã£o de *n-gramas* (sequÃªncias de *n* palavras) entre o texto gerado e o texto de referÃªncia. Por exemplo, ROUGE-1 avalia a sobreposiÃ§Ã£o de unigramas, ROUGE-2 avalia a sobreposiÃ§Ã£o de bigramas, e assim por diante. O *recall* ROUGE-N Ã© calculado como:

    $$
    ROUGE-N = \frac{\sum_{S \in \{\text{reference summaries}\}} \sum_{gram_n \in S} \text{Count}_{\text{match}}(gram_n)}{\sum_{S \in \{\text{reference summaries}\}} \sum_{gram_n \in S} \text{Count}(gram_n)}
    $$

    Onde:
    *   $N$ Ã© o tamanho do *n-gram*.
    *   $\text{Count}_{\text{match}}(gram_n)$ Ã© o nÃºmero de *n-gramas* coincidentes entre o texto gerado e o texto de referÃªncia.
    *   $\text{Count}(gram_n)$ Ã© o nÃºmero total de *n-gramas* no texto de referÃªncia.

    > ğŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Considere o seguinte texto de referÃªncia e texto gerado:
    >
    > *   Texto de ReferÃªncia: "The cat sat on the mat."
    > *   Texto Gerado: "The cat sat on mat."
    >
    > Para calcular ROUGE-1:
    >
    > *   Unigramas no texto de referÃªncia: `['the', 'cat', 'sat', 'on', 'the', 'mat']`
    > *   Unigramas coincidentes: `['the', 'cat', 'sat', 'on', 'mat']`
    > *   $\text{Count}_{\text{match}}(gram_1) = 5$
    > *   $\text{Count}(gram_1) = 6$
    > *   $ROUGE-1 = \frac{5}{6} \approx 0.833$
    >
    > Este resultado indica que o texto gerado cobre aproximadamente 83.3% dos unigramas presentes no texto de referÃªncia.
    >
    > Para calcular ROUGE-2:
    > *   Bigramas no texto de referÃªncia: `['the cat', 'cat sat', 'sat on', 'on the', 'the mat']`
    > *   Bigramas coincidentes: `['the cat', 'cat sat', 'sat on']`
    > *   $\text{Count}_{\text{match}}(gram_2) = 3$
    > *   $\text{Count}(gram_2) = 5$
    > *   $ROUGE-2 = \frac{3}{5} = 0.6$
    >
    > O ROUGE-2 score de 0.6 sugere que 60% dos bigramas no texto de referÃªncia estÃ£o presentes no texto gerado, indicando uma cobertura razoÃ¡vel de sequÃªncias de duas palavras.

*   **ROUGE-L:** Baseia-se na maior subsequÃªncia comum (Longest Common Subsequence - LCS) entre o texto gerado e o texto de referÃªncia. O LCS considera a ordem das palavras, mas nÃ£o exige que as palavras sejam consecutivas. Isso Ã© Ãºtil para capturar a similaridade entre textos, mesmo quando hÃ¡ inserÃ§Ãµes ou exclusÃµes.

    Seja $X$ o texto gerado e $Y$ o texto de referÃªncia. Seja $LCS(X, Y)$ a maior subsequÃªncia comum entre $X$ e $Y$. O *recall* ROUGE-L Ã© calculado como:

    $$
    R_{lcs} = \frac{\text{length}(LCS(X, Y))}{\text{length}(Y)}
    $$

    A precisÃ£o ROUGE-L Ã©:

    $$
    P_{lcs} = \frac{\text{length}(LCS(X, Y))}{\text{length}(X)}
    $$

    E a pontuaÃ§Ã£o F Ã© calculada como a mÃ©dia harmÃ´nica de $R_{lcs}$ e $P_{lcs}$:

    $$
    F_{lcs} = \frac{(1 + \beta^2)R_{lcs}P_{lcs}}{R_{lcs} + \beta^2P_{lcs}}
    $$

    Onde $\beta$ geralmente Ã© definido para dar igual importÃ¢ncia a *recall* e precisÃ£o.

    **ProposiÃ§Ã£o 1:** A pontuaÃ§Ã£o $F_{lcs}$ Ã© maximizada quando a precisÃ£o e o recall sÃ£o iguais.

    *Prova:* Para maximizar $F_{lcs}$, podemos tomar a derivada em relaÃ§Ã£o a $R_{lcs}$ e $P_{lcs}$ e igualar a zero. Ou, de forma mais simples, observar que a mÃ©dia harmÃ´nica atinge seu valor mÃ¡ximo quando os termos sÃ£o iguais. Portanto, a pontuaÃ§Ã£o F Ã© maximizada quando $R_{lcs} = P_{lcs}$. $\blacksquare$

    > ğŸ’¡ **Exemplo NumÃ©rico:**
    >
    > *   Texto de ReferÃªncia (Y): "The quick brown fox jumps over the lazy dog." (9 palavras)
    > *   Texto Gerado (X): "The brown fox jumps over the dog." (7 palavras)
    >
    > A maior subsequÃªncia comum (LCS) Ã© "The brown fox jumps over the dog." (7 palavras)
    >
    > *   $\text{length}(LCS(X, Y)) = 7$
    > *   $\text{length}(Y) = 9$
    > *   $\text{length}(X) = 7$
    >
    > *Recall*: $R_{lcs} = \frac{7}{9} \approx 0.778$
    > *PrecisÃ£o*: $P_{lcs} = \frac{7}{7} = 1.0$
    >
    > Assumindo $\beta = 1$ (igual importÃ¢ncia para recall e precisÃ£o):
    >
    > $F_{lcs} = \frac{(1 + 1^2) \times 0.778 \times 1.0}{0.778 + 1^2 \times 1.0} = \frac{2 \times 0.778}{1.778} \approx 0.875$
    >
    > Neste exemplo, o ROUGE-L F-score Ã© 0.875, refletindo a alta similaridade sequencial entre os textos. A precisÃ£o Ã© perfeita porque cada palavra no texto gerado estÃ¡ presente e na ordem correta dentro do texto de referÃªncia.

*   **ROUGE-W:** Uma extensÃ£o do ROUGE-L que tambÃ©m considera o peso de cada subsequÃªncia comum. Isso significa que subsequÃªncias mais longas recebem maior importÃ¢ncia, o que pode ser Ãºtil para penalizar textos gerados que apenas capturam pequenas partes do texto de referÃªncia.

*   **ROUGE-S:** TambÃ©m conhecida como *Skip-Bigram Co-Occurrence Statistics*. ROUGE-S mede a sobreposiÃ§Ã£o de pares de palavras, permitindo lacunas arbitrÃ¡rias entre as palavras. Isso torna a mÃ©trica mais robusta a variaÃ§Ãµes na ordem das palavras.

### AplicaÃ§Ãµes e InterpretaÃ§Ãµes
ROUGE Ã© amplamente utilizada para avaliar sistemas de sumarizaÃ§Ã£o automÃ¡tica, traduÃ§Ã£o automÃ¡tica e geraÃ§Ã£o de texto em geral. Ao contrÃ¡rio de BLEU, que penaliza a omissÃ£o de palavras presentes no texto de referÃªncia, ROUGE recompensa a cobertura do conteÃºdo essencial.

*   **ROUGE-1 e ROUGE-2:** SÃ£o frequentemente usadas para avaliar a qualidade geral da sumarizaÃ§Ã£o. ROUGE-1 captura a sobreposiÃ§Ã£o bÃ¡sica de palavras, enquanto ROUGE-2 avalia a fluÃªncia e a coesÃ£o local.
*   **ROUGE-L:** Ã‰ Ãºtil para avaliar a capacidade de um sistema de capturar as relaÃ§Ãµes semÃ¢nticas de longo alcance no texto.
*   **ROUGE-S:** Ã‰ particularmente Ãºtil em cenÃ¡rios onde a ordem das palavras pode variar significativamente.

A escolha da mÃ©trica ROUGE apropriada depende da tarefa especÃ­fica e das caracterÃ­sticas dos textos a serem avaliados. Em geral, Ã© recomendÃ¡vel usar uma combinaÃ§Ã£o de diferentes mÃ©tricas ROUGE para obter uma avaliaÃ§Ã£o mais completa.

**Teorema 2:** *RelaÃ§Ã£o entre ROUGE-N e ROUGE-1*. Se um texto gerado tem um alto escore ROUGE-N para um N grande, entÃ£o ele tambÃ©m terÃ¡ um alto escore ROUGE-1.

*Prova (EsboÃ§o):* Um alto escore ROUGE-N implica que longas sequÃªncias de palavras do texto de referÃªncia estÃ£o presentes no texto gerado. Consequentemente, cada palavra individual (unigrama) dessas sequÃªncias tambÃ©m estarÃ¡ presente, resultando em um alto escore ROUGE-1. No entanto, o inverso nÃ£o Ã© necessariamente verdadeiro. Um alto ROUGE-1 nÃ£o garante um alto ROUGE-N, pois as palavras individuais podem estar presentes sem formar sequÃªncias significativas. $\blacksquare$

### Vantagens e Desvantagens
**Vantagens:**

*   Simplicidade: FÃ¡cil de entender e implementar.
*   Boa CorrelaÃ§Ã£o: Geralmente apresenta boa correlaÃ§Ã£o com avaliaÃ§Ãµes humanas, especialmente para tarefas de sumarizaÃ§Ã£o.
*   Foco no *Recall*: Complementa mÃ©tricas baseadas em precisÃ£o, fornecendo uma visÃ£o mais completa da qualidade do texto gerado.

**Desvantagens:**

*   Superficialidade: Baseia-se apenas na sobreposiÃ§Ã£o de palavras, ignorando aspectos como gramaticalidade, coerÃªncia e relevÃ¢ncia.
*   LimitaÃ§Ãµes em DomÃ­nios EspecÃ­ficos: Pode nÃ£o ser adequada para avaliar textos em domÃ­nios altamente especializados, onde o vocabulÃ¡rio Ã© restrito e a semÃ¢ntica Ã© complexa.
*   Sensibilidade a PrÃ©-Processamento: Os resultados podem ser afetados por etapas de prÃ©-processamento como stemming e remoÃ§Ã£o de *stop words*.

**Lema 1:** *Impacto do Stemming em ROUGE*. A aplicaÃ§Ã£o de stemming pode aumentar os escores ROUGE, ao agrupar diferentes formas da mesma palavra.

*Prova (EsboÃ§o):* Stemming reduz as palavras Ã  sua raiz. Por exemplo, "running", "runs" e "run" sÃ£o reduzidos a "run". Isso aumenta a chance de encontrar correspondÃªncias entre o texto gerado e o texto de referÃªncia, resultando em escores ROUGE mais altos. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere o seguinte cenÃ¡rio:
>
> *   Texto de ReferÃªncia: "The player is running quickly."
> *   Texto Gerado: "The player runs fast."
>
> Sem stemming:
>
> *   Unigramas de ReferÃªncia: `['the', 'player', 'is', 'running', 'quickly']`
> *   Unigramas Gerados: `['the', 'player', 'runs', 'fast']`
> *   Unigramas Coincidentes: `['the', 'player']`
> *   $ROUGE-1 = \frac{2}{5} = 0.4$
>
> Com stemming (usando um stemmer simplificado que mapeia 'running' para 'run'):
>
> *   Unigramas de ReferÃªncia (apÃ³s stemming): `['the', 'player', 'is', 'run', 'quickly']`
> *   Unigramas Gerados (apÃ³s stemming): `['the', 'player', 'run', 'fast']`
> *   Unigramas Coincidentes: `['the', 'player', 'run']`
> *   $ROUGE-1 = \frac{3}{5} = 0.6$
>
> Este exemplo ilustra como o stemming pode aumentar o ROUGE-1, identificando "running" e "runs" como a mesma palavra raiz ("run"). Note que a qualidade da sumarizaÃ§Ã£o em si nÃ£o mudou, apenas a forma como a mÃ©trica a avalia.

### ConclusÃ£o
ROUGE Ã© uma mÃ©trica valiosa para avaliar a cobertura e a similaridade entre textos gerados e textos de referÃªncia. Ao complementar mÃ©tricas como BLEU, ROUGE oferece uma perspectiva mais abrangente da qualidade do texto, especialmente em tarefas de sumarizaÃ§Ã£o automÃ¡tica. A escolha da mÃ©trica ROUGE apropriada deve considerar as caracterÃ­sticas especÃ­ficas da tarefa e dos dados a serem avaliados.

### ReferÃªncias
[^1]: Recall-Oriented Understudy for Gisting Evaluation (ROUGE) quantifies the word overlap between the generated output and the reference text.
<!-- END -->