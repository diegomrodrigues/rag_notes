## MoverScore: Avalia√ß√£o Sem√¢ntica com Embeddings Contextuais

### Introdu√ß√£o

No contexto da avalia√ß√£o de modelos de *Retrieval-Augmented Generation* (RAG) e, mais amplamente, de modelos de linguagem (LLMs), a necessidade de m√©tricas que capturem nuances sem√¢nticas tornou-se premente. M√©tricas como BLEU e ROUGE, baseadas em sobreposi√ß√£o de *n-grams*, frequentemente falham em reconhecer a similaridade sem√¢ntica entre textos que usam palavras diferentes para expressar a mesma ideia. BERTScore representa um avan√ßo ao utilizar embeddings contextuais para avaliar a similaridade, mas ainda imp√µe um alinhamento r√≠gido entre tokens. MoverScore surge como uma alternativa que supera essa limita√ß√£o ao permitir alinhamentos *many-to-one* (soft alignment) e capturar rela√ß√µes sem√¢nticas de forma mais flex√≠vel [^6].

### Conceitos Fundamentais

MoverScore, ao contr√°rio de BERTScore, n√£o se limita a encontrar a correspond√™ncia mais pr√≥xima entre cada token no texto gerado e o texto de refer√™ncia. Em vez disso, ele busca uma solu√ß√£o √≥tima para o problema de "transformar" um texto no outro, minimizando o "esfor√ßo" necess√°rio para essa transforma√ß√£o [^6]. Esse esfor√ßo √© medido pela dist√¢ncia entre os embeddings contextuais dos tokens, ponderada por um fluxo que representa o quanto cada token no texto gerado "contribui" para cada token no texto de refer√™ncia.

Formalmente, podemos descrever o problema da seguinte forma:

Sejam $X = \{x_1, x_2, ..., x_m\}$ o conjunto de tokens no texto gerado e $Y = \{y_1, y_2, ..., y_n\}$ o conjunto de tokens no texto de refer√™ncia. Seja $e(x_i)$ o embedding contextual do token $x_i$ e $e(y_j)$ o embedding contextual do token $y_j$. A dist√¢ncia entre os embeddings $e(x_i)$ e $e(y_j)$ √© dada por $d(e(x_i), e(y_j))$. O objetivo √© encontrar um fluxo $F = \{f_{ij}\}$, onde $f_{ij}$ representa o fluxo do token $x_i$ para o token $y_j$, que minimize a seguinte fun√ß√£o de custo:

$$
\sum_{i=1}^{m} \sum_{j=1}^{n} f_{ij} d(e(x_i), e(y_j))
$$

sujeito √†s restri√ß√µes:

$$
\sum_{j=1}^{n} f_{ij} = 1, \forall i \in \{1, 2, ..., m\}
$$

$$
f_{ij} \geq 0, \forall i \in \{1, 2, ..., m\}, \forall j \in \{1, 2, ..., n\}
$$

A primeira restri√ß√£o garante que cada token no texto gerado contribua totalmente para os tokens no texto de refer√™ncia. A segunda restri√ß√£o imp√µe que o fluxo seja n√£o-negativo.

> üí° **Exemplo Num√©rico:**
>
> Considere um texto gerado $X$ = "The cat sat" e um texto de refer√™ncia $Y$ = "A feline was sitting".  Seja $m=3$ e $n=4$.
>
> Ap√≥s calcular os embeddings contextuais (e.g., usando BERT), suponha que as dist√¢ncias cosseno (j√° calculadas as dist√¢ncias usando a f√≥rmula da Defini√ß√£o 1) entre os tokens sejam as seguintes:
>
> |         | A     | feline | was   | sitting |
> | :------ | :---- | :----- | :---- | :------ |
> | The     | 0.6   | 0.5    | 0.7   | 0.8     |
> | cat     | 0.7   | 0.2    | 0.6   | 0.7     |
> | sat     | 0.9   | 0.8    | 0.4   | 0.3     |
>
> O objetivo √© encontrar o fluxo $F = \{f_{ij}\}$ que minimize o custo total.  Por exemplo, uma solu√ß√£o fact√≠vel (mas provavelmente n√£o √≥tima) seria:
>
> $f_{11} = 0, f_{12} = 1, f_{13} = 0, f_{14} = 0$ (The -> feline)
>
> $f_{21} = 0, f_{22} = 1, f_{23} = 0, f_{24} = 0$ (cat -> feline)
>
> $f_{31} = 0, f_{32} = 0, f_{33} = 0, f_{34} = 1$ (sat -> sitting)
>
> Neste caso, o custo seria $(1 * 0.5) + (1 * 0.2) + (1 * 0.3) = 1.0$. Um otimizador buscaria o fluxo $F$ que minimize a fun√ß√£o de custo descrita.

A solu√ß√£o para esse problema de otimiza√ß√£o com restri√ß√µes fornece o fluxo √≥timo $F^*$. O MoverScore √© ent√£o calculado como a m√©dia ponderada das dist√¢ncias, usando o fluxo √≥timo como pesos:

$$
\text{MoverScore} = \frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{n} f_{ij}^* d(e(x_i), e(y_j))
$$

> üí° **Exemplo Num√©rico (continua√ß√£o):**
>
> Suponha que o otimizador encontrou o seguinte fluxo √≥timo (simplificado para fins ilustrativos):
>
> |         | A      | feline | was    | sitting |
> | :------ | :----- | :----- | :----- | :------ |
> | The     | 0      | 0.8    | 0.2    | 0       |
> | cat     | 0      | 1      | 0      | 0       |
> | sat     | 0      | 0      | 0.1    | 0.9     |
>
> Ent√£o, o MoverScore seria calculado como:
>
> $\text{MoverScore} = \frac{1}{3} * [(0.8 * 0.5) + (0.2 * 0.7) + (1 * 0.2) + (0.1 * 0.4) + (0.9 * 0.3)] = \frac{1}{3} * [0.4 + 0.14 + 0.2 + 0.04 + 0.27] = \frac{1}{3} * 1.05 = 0.35$
>
> Uma interpreta√ß√£o desse resultado √© que, em m√©dia, a "dist√¢ncia" entre os tokens do texto gerado e os tokens do texto de refer√™ncia, ponderada pelo fluxo √≥timo, √© de 0.35 (usando dist√¢ncia cosseno). Um MoverScore menor indica uma maior similaridade sem√¢ntica.

Essa formula√ß√£o permite que o MoverScore capture rela√ß√µes sem√¢nticas sutis, uma vez que um token no texto gerado pode contribuir para m√∫ltiplos tokens no texto de refer√™ncia, e vice-versa, sem a rigidez imposta por m√©tricas baseadas em alinhamento um-para-um.

A grande vantagem do MoverScore reside na sua capacidade de lidar com sinon√≠mia e par√°frases. Se o texto gerado usa um sin√¥nimo de uma palavra no texto de refer√™ncia, os embeddings contextuais dessas palavras estar√£o pr√≥ximos no espa√ßo de embeddings, resultando em uma dist√¢ncia pequena e, portanto, em um MoverScore mais alto.

Para complementar a descri√ß√£o do MoverScore, √© importante notar que a escolha da fun√ß√£o de dist√¢ncia $d(e(x_i), e(y_j))$ afeta diretamente o comportamento da m√©trica. Uma escolha comum √© a dist√¢ncia cosseno, que mede o cosseno do √¢ngulo entre os vetores de embedding.

**Defini√ß√£o 1** (Dist√¢ncia Cosseno) A dist√¢ncia cosseno entre dois vetores $u$ e $v$ √© definida como:

$$
d(u, v) = 1 - \frac{u \cdot v}{\|u\| \|v\|}
$$

onde $u \cdot v$ √© o produto escalar de $u$ e $v$, e $\|u\|$ √© a norma de $u$.

> üí° **Exemplo Num√©rico:**
>
> Seja $u = [0.8, 0.6]$ e $v = [0.7, 0.7]$.
>
> $\text{Step 1: Calculate the dot product: } u \cdot v = (0.8 * 0.7) + (0.6 * 0.7) = 0.56 + 0.42 = 0.98$
>
> $\text{Step 2: Calculate the norms: } \|u\| = \sqrt{0.8^2 + 0.6^2} = \sqrt{0.64 + 0.36} = 1.0 \text{ and } \|v\| = \sqrt{0.7^2 + 0.7^2} = \sqrt{0.49 + 0.49} = \sqrt{0.98} \approx 0.99$
>
> $\text{Step 3: Calculate the cosine similarity: } \frac{u \cdot v}{\|u\| \|v\|} = \frac{0.98}{1.0 * 0.99} \approx 0.99$
>
> $\text{Step 4: Calculate the cosine distance: } d(u, v) = 1 - 0.99 = 0.01$
>
> A dist√¢ncia cosseno √© muito pequena, o que significa que os vetores $u$ e $v$ s√£o muito similares em dire√ß√£o.

Al√©m disso, a formula√ß√£o original do MoverScore pode ser vista como uma inst√¢ncia do problema de transporte √≥timo (Optimal Transport).

**Teorema 1** MoverScore √© uma solu√ß√£o para o problema de transporte √≥timo discreto entre as distribui√ß√µes de probabilidade dos embeddings contextuais dos tokens no texto gerado e no texto de refer√™ncia, onde a fun√ß√£o de custo √© dada pela dist√¢ncia entre os embeddings.

*Prova*. O problema de transporte √≥timo busca encontrar a maneira mais eficiente de transportar massa de um conjunto de pontos (texto gerado) para outro (texto de refer√™ncia), minimizando o custo total de transporte. No caso do MoverScore, a "massa" √© a import√¢ncia de cada token, e o "custo" √© a dist√¢ncia entre os embeddings contextuais. As restri√ß√µes garantem que toda a massa do texto gerado seja transportada para o texto de refer√™ncia. Portanto, MoverScore resolve o problema de transporte √≥timo discreto.

A escolha da dist√¢ncia cosseno como fun√ß√£o de custo √© particularmente interessante porque normaliza os embeddings, focando na dire√ß√£o dos vetores em vez de sua magnitude. No entanto, outras fun√ß√µes de dist√¢ncia tamb√©m podem ser usadas, dependendo da aplica√ß√£o. Uma alternativa seria utilizar a dist√¢ncia euclidiana.

**Defini√ß√£o 2** (Dist√¢ncia Euclidiana) A dist√¢ncia euclidiana entre dois vetores $u$ e $v$ √© definida como:

$$
d(u, v) = \sqrt{\sum_{k=1}^{D} (u_k - v_k)^2}
$$

onde $D$ √© a dimens√£o dos vetores.

> üí° **Exemplo Num√©rico:**
>
> Usando os mesmos vetores $u = [0.8, 0.6]$ e $v = [0.7, 0.7]$ do exemplo anterior:
>
> $\text{Step 1: Calculate the differences: } u - v = [0.8 - 0.7, 0.6 - 0.7] = [0.1, -0.1]$
>
> $\text{Step 2: Square the differences: } [0.1^2, (-0.1)^2] = [0.01, 0.01]$
>
> $\text{Step 3: Sum the squared differences: } 0.01 + 0.01 = 0.02$
>
> $\text{Step 4: Take the square root: } d(u, v) = \sqrt{0.02} \approx 0.14$
>
> Neste caso, a dist√¢ncia Euclidiana √© 0.14.  Comparando com a dist√¢ncia cosseno (0.01), a dist√¢ncia Euclidiana atribui uma maior dist√¢ncia (dissimilaridade) devido √† diferen√ßa nas magnitudes dos vetores.

A escolha entre a dist√¢ncia cosseno e a dist√¢ncia euclidiana pode impactar significativamente os resultados do MoverScore. A dist√¢ncia cosseno √© mais robusta a diferen√ßas de magnitude entre os embeddings, enquanto a dist√¢ncia euclidiana leva em considera√ß√£o tanto a dire√ß√£o quanto a magnitude.

Outro aspecto importante a ser considerado √© a escolha do modelo de embeddings contextuais. Modelos como BERT, RoBERTa e GPT-2 podem ser usados para gerar os embeddings. A escolha do modelo deve ser feita com base no dom√≠nio da aplica√ß√£o e no tipo de texto que est√° sendo avaliado.





**Teorema 1.1** A qualidade dos embeddings contextuais afeta diretamente a precis√£o do MoverScore.

*Prova.* A fun√ß√£o de custo do MoverScore √© baseada na dist√¢ncia entre os embeddings contextuais. Se os embeddings n√£o representarem adequadamente o significado dos tokens, a dist√¢ncia entre eles ser√° imprecisa, levando a um fluxo sub√≥timo e, consequentemente, a um MoverScore impreciso. Portanto, a qualidade dos embeddings √© crucial para o desempenho do MoverScore.



![Illustration contrasting BERTScore's one-to-one alignment with MoverScore's many-to-one mapping of semantically related words.](./../images/image12.jpg)

Por fim, √© relevante mencionar que a complexidade computacional do MoverScore pode ser um desafio, especialmente para textos longos. A solu√ß√£o do problema de otimiza√ß√£o de fluxo requer algoritmos eficientes, como o algoritmo de Sinkhorn, para reduzir o tempo de computa√ß√£o.
<!-- END -->