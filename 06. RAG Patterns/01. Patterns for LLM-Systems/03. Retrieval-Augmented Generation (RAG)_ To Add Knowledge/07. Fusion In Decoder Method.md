## Fusion-in-Decoder (FiD) para Recupera√ß√£o Aumentada Generativa

### Introdu√ß√£o

O padr√£o Retrieval-Augmented Generation (RAG) visa aprimorar a capacidade dos modelos de linguagem (LLMs) em tarefas de gera√ß√£o de texto, incorporando conhecimento externo recuperado em tempo real. Dentro do contexto de RAG, o m√©todo Fusion-in-Decoder (FiD) se destaca como uma abordagem sofisticada para lidar com m√∫ltiplas passagens recuperadas e integr√°-las de forma eficaz no processo de gera√ß√£o [^7]. Este cap√≠tulo se aprofunda no FiD, detalhando sua arquitetura, funcionamento e relev√¢ncia no campo de Neural Information Retrieval e RAG com LLMs.

### Conceitos Fundamentais

O Fusion-in-Decoder (FiD) √© uma metodologia que combina recupera√ß√£o de informa√ß√£o com modelos generativos, notavelmente aplicada em Question Answering (QA) de dom√≠nio aberto [^7]. Diferentemente de abordagens mais simples que selecionam apenas uma passagem para condicionar o LLM, o FiD processa *m√∫ltiplas* passagens recuperadas, permitindo que o modelo considere uma gama mais ampla de informa√ß√µes relevantes.

**Arquitetura e Funcionamento:**

A arquitetura do FiD se distingue pelo seu tratamento espec√≠fico das entradas. Para cada passagem recuperada, o modelo concatena o t√≠tulo da passagem com a pergunta, criando uma sequ√™ncia de entrada que ser√° processada pelo encoder [^7]. Essa concatena√ß√£o √© cuidadosamente formatada com tokens especiais para demarcar as diferentes se√ß√µes: 'question:', 'title:' e 'context:' [^7]. Essa estrat√©gia de marca√ß√£o expl√≠cita auxilia o modelo a diferenciar e compreender o papel de cada componente na sequ√™ncia de entrada.

Formalmente, para uma dada quest√£o $q$ e um conjunto de passagens recuperadas $P = \{p_1, p_2, ..., p_n\}$, onde cada $p_i$ possui um t√≠tulo $t_i$, a entrada para o encoder √© constru√≠da da seguinte forma para cada passagem $i$:

$$
\text{input}_i = \text{'question:' } + q + \text{ 'title:' } + t_i + \text{ 'context:' } + p_i
$$

Cada uma dessas entradas √© processada independentemente pelo encoder. No entanto, o ponto crucial do FiD reside na forma como o *decoder* processa as representa√ß√µes geradas pelo encoder. O decoder *atende* √† concatena√ß√£o de *todas* as passagens recuperadas, permitindo que ele sintetize informa√ß√µes de m√∫ltiplas fontes para gerar a resposta final [^7].

![Fusion-in-Decoder architecture illustrating independent encoding of question-passage pairs followed by concatenation and decoding for question answering.](./../images/image27.jpg)

> üí° **Exemplo Num√©rico:**
> Suponha a pergunta $q$ seja "Qual a capital da Fran√ßa?", e temos duas passagens recuperadas:
>
> *   $p_1$: "Paris √© a capital e a maior cidade da Fran√ßa." (t√≠tulo $t_1$: "Paris")
> *   $p_2$: "A Fran√ßa √© um pa√≠s na Europa Ocidental." (t√≠tulo $t_2$: "Fran√ßa")
>
> As entradas para o encoder seriam:
>
> *   $\text{input}_1 = \text{'question:' } + \text{"Qual a capital da Fran√ßa?" } + \text{'title:' } + \text{"Paris" } + \text{'context:' } + \text{"Paris √© a capital e a maior cidade da Fran√ßa."}$
> *   $\text{input}_2 = \text{'question:' } + \text{"Qual a capital da Fran√ßa?" } + \text{'title:' } + \text{"Fran√ßa" } + \text{'context:' } + \text{"A Fran√ßa √© um pa√≠s na Europa Ocidental."}$
>
> O encoder processaria cada uma dessas entradas independentemente. O decoder ent√£o atenderia √†s representa√ß√µes de *ambas* as passagens para gerar a resposta "Paris".  Observe que a primeira passagem cont√©m a resposta direta, enquanto a segunda fornece um contexto geogr√°fico, que pode ser √∫til.

**Lema 1:** *O processamento independente das passagens pelo encoder permite paraleliza√ß√£o, reduzindo o tempo total de infer√™ncia.*

*Prova:* Como cada passagem √© processada independentemente, o c√°lculo da representa√ß√£o $E_i$ para cada passagem $p_i$ pode ser realizado em paralelo. Isso significa que, com recursos computacionais adequados (e.g., m√∫ltiplas GPUs), o tempo para codificar todas as passagens pode ser significativamente reduzido em compara√ß√£o com o processamento sequencial. $\blacksquare$

**Aten√ß√£o no Decoder:**

A etapa de aten√ß√£o no decoder √© fundamental para o funcionamento do FiD. Ao atender a todas as passagens, o decoder pode identificar as informa√ß√µes mais relevantes em cada uma delas e combin√°-las para produzir uma resposta precisa e abrangente. Este mecanismo de aten√ß√£o permite que o modelo capture rela√ß√µes complexas entre as passagens e a pergunta, resultando em uma melhor performance em tarefas de QA.

Seja $E_i$ a representa√ß√£o da $i$-√©sima passagem produzida pelo encoder. O decoder, ao gerar cada token $y_t$ da resposta, calcula uma distribui√ß√£o de probabilidade sobre o vocabul√°rio $V$ com base em sua aten√ß√£o sobre todas as representa√ß√µes $E_i$:

$$
P(y_t | y_{<t}, q, P) = \text{Decoder}(y_{<t}, [E_1, E_2, ..., E_n])
$$

onde $[E_1, E_2, ..., E_n]$ representa a concatena√ß√£o das representa√ß√µes das passagens.

Para formalizar o processo de aten√ß√£o, podemos definir a distribui√ß√£o de aten√ß√£o $\alpha_{t,i}$ sobre a $i$-√©sima passagem no passo $t$ do decoder como:

$$
\alpha_{t,i} = \text{Attention}(y_{<t}, E_i)
$$

onde $\text{Attention}$ representa a fun√ß√£o de aten√ß√£o utilizada pelo decoder. A sa√≠da da camada de aten√ß√£o, $c_t$, √© ent√£o uma combina√ß√£o ponderada das representa√ß√µes das passagens:

$$
c_t = \sum_{i=1}^{n} \alpha_{t,i} E_i
$$

A probabilidade de gerar o token $y_t$ pode ent√£o ser reescrita como:

$$
P(y_t | y_{<t}, q, P) = \text{Decoder}(y_{<t}, c_t)
$$

> üí° **Exemplo Num√©rico:**
> Suponha que no passo $t$ do decoder, o modelo esteja considerando as representa√ß√µes das duas passagens do exemplo anterior, $E_1$ e $E_2$.  A fun√ß√£o de aten√ß√£o calcula pesos $\alpha_{t,1} = 0.8$ para a primeira passagem (Paris) e $\alpha_{t,2} = 0.2$ para a segunda passagem (Fran√ßa). Isso reflete a maior relev√¢ncia da primeira passagem para a pergunta.
>
> A sa√≠da da camada de aten√ß√£o seria:
>
> $c_t = 0.8 \cdot E_1 + 0.2 \cdot E_2$
>
> Este vetor $c_t$, que √© uma combina√ß√£o ponderada das representa√ß√µes das passagens, √© ent√£o usado pelo decoder para gerar o pr√≥ximo token. A maior contribui√ß√£o de $E_1$ direciona o decoder a gerar tokens relacionados a "Paris".

**Teorema 1:** *A complexidade computacional da etapa de aten√ß√£o no decoder do FiD √© linear no n√∫mero de passagens recuperadas.*

*Prova:* Para cada passo $t$ do decoder, a fun√ß√£o de aten√ß√£o √© calculada para cada uma das $n$ passagens. O c√°lculo de cada $\alpha_{t,i}$ tem complexidade constante (assumindo uma fun√ß√£o de aten√ß√£o com complexidade constante em rela√ß√£o ao tamanho da representa√ß√£o $E_i$). Portanto, a complexidade total para calcular a distribui√ß√£o de aten√ß√£o sobre todas as passagens √© $O(n)$.  A combina√ß√£o ponderada das representa√ß√µes das passagens tamb√©m tem complexidade $O(n)$.  Como a complexidade de cada passo do decoder √© dominada por essa etapa, a complexidade total da etapa de aten√ß√£o √© linear no n√∫mero de passagens. $\blacksquare$

**Vantagens do FiD:**

O FiD oferece diversas vantagens sobre abordagens de RAG mais simples:

*   **Aproveitamento de M√∫ltiplas Fontes:** Ao considerar m√∫ltiplas passagens recuperadas, o FiD pode acessar uma gama mais ampla de informa√ß√µes relevantes, mitigando o risco de depender de uma √∫nica fonte potencialmente incompleta ou imprecisa.
*   **Robustez:** A capacidade de atender a m√∫ltiplas passagens torna o FiD mais robusto a ru√≠dos e irrelev√¢ncias presentes nas passagens recuperadas. O modelo pode aprender a filtrar informa√ß√µes esp√∫rias e focar nos aspectos mais importantes para gerar a resposta.
*   **Desempenho Aprimorado:** Em diversas tarefas de QA de dom√≠nio aberto, o FiD tem demonstrado consistentemente um desempenho superior em compara√ß√£o com abordagens que utilizam apenas uma √∫nica passagem [^7].

> üí° **Exemplo Num√©rico:**
> Considere um cen√°rio onde uma das passagens recuperadas est√° ligeiramente incorreta, afirmando que a capital da Fran√ßa √© "Lyon". No entanto, outras passagens recuperadas corretamente afirmam que √© "Paris". O mecanismo de aten√ß√£o do FiD provavelmente atribuir√° pesos de aten√ß√£o significativamente menores √† passagem incorreta, permitindo que o modelo gere a resposta correta ("Paris") com base nas passagens mais precisas. Isso demonstra a robustez do FiD a informa√ß√µes ruidosas.

**Teorema 2:** *Em um cen√°rio onde as passagens recuperadas cont√™m informa√ß√µes redundantes, o mecanismo de aten√ß√£o do FiD permite uma aloca√ß√£o adaptativa de pesos, favorecendo passagens com informa√ß√µes √∫nicas ou mais relevantes.*

*Prova (Esbo√ßo):* Considere que algumas passagens em $P$ contenham informa√ß√µes que se sobrep√µem. O mecanismo de aten√ß√£o, treinado para maximizar a probabilidade da resposta correta, aprender√° a identificar padr√µes de ativa√ß√£o em passagens que contribuem de forma mais significativa para a predi√ß√£o. Passagens com informa√ß√µes redundantes receber√£o, em m√©dia, pesos de aten√ß√£o menores em compara√ß√£o com passagens que oferecem informa√ß√µes complementares ou que confirmam informa√ß√µes cruciais de outras passagens com maior confian√ßa. A prova formal envolveria analisar o gradiente da fun√ß√£o de perda em rela√ß√£o aos pesos de aten√ß√£o e demonstrar que ele tende a diminuir para passagens redundantes. $\blacksquare$

### Conclus√£o

O Fusion-in-Decoder (FiD) representa um avan√ßo significativo na √°rea de Retrieval-Augmented Generation, oferecendo uma abordagem eficaz para integrar conhecimento externo em modelos generativos [^7]. Sua capacidade de processar e atender a m√∫ltiplas passagens recuperadas permite que o modelo aproveite uma gama mais ampla de informa√ß√µes relevantes, resultando em respostas mais precisas, abrangentes e robustas. Ao empregar tokens especiais para estruturar as entradas e um mecanismo de aten√ß√£o sofisticado no decoder, o FiD demonstra o potencial de combinar recupera√ß√£o de informa√ß√£o e gera√ß√£o de texto para resolver problemas complexos de QA e outras tarefas relacionadas.

### Refer√™ncias

[^7]: Informa√ß√µes extra√≠das da descri√ß√£o fornecida no contexto.
<!-- END -->