## Retrieval-Enhanced Transformer (RETRO): Integra√ß√£o de Recupera√ß√£o e Gera√ß√£o

### Introdu√ß√£o
O Retrieval-Enhanced Transformer (RETRO) representa uma abordagem inovadora na arquitetura de modelos de linguagem, combinando a capacidade de **recupera√ß√£o de informa√ß√µes** com a **gera√ß√£o de texto**. Diferentemente dos modelos tradicionais que dependem exclusivamente do conhecimento param√©trico armazenado em seus pesos, o RETRO aproveita um mecanismo de *retrieval* para buscar informa√ß√µes relevantes em um banco de dados externo durante a gera√ß√£o. Este cap√≠tulo explorar√° em detalhes a arquitetura, o processo de *retrieval* e a aplica√ß√£o do RETRO, com foco nas suas vantagens e peculiaridades.

### Conceitos Fundamentais
O RETRO [^8] √© composto por tr√™s componentes principais:

1.  **Frozen BERT Retriever:** Um modelo BERT pr√©-treinado e congelado, respons√°vel por realizar a busca de documentos relevantes no banco de dados externo. O termo ‚Äúfrozen‚Äù indica que os pesos deste modelo n√£o s√£o atualizados durante o treinamento do RETRO.
2.  **Differentiable Encoder:** Um encoder diferenci√°vel que codifica a entrada e os documentos recuperados. A diferenciabilidade deste componente √© crucial para o treinamento end-to-end do modelo.
3.  **Chunked Cross-Attention:** Um mecanismo de aten√ß√£o que permite ao modelo combinar a informa√ß√£o da entrada com a informa√ß√£o dos documentos recuperados. Este mecanismo √© aplicado em *chunks* (peda√ßos) da sequ√™ncia, permitindo um processamento mais eficiente.

O processo de funcionamento do RETRO pode ser resumido nos seguintes passos:

1.  **Chunking:** A sequ√™ncia de entrada √© dividida em *chunks*.
2.  **Retrieval:** Para cada *chunk*, o Frozen BERT Retriever busca documentos relevantes no banco de dados externo. A busca √© baseada na similaridade entre o *chunk* atual e os documentos no banco de dados.
3.  **Encoding:** A entrada e os documentos recuperados s√£o codificados pelo Differentiable Encoder.
4.  **Chunked Cross-Attention:** O Chunked Cross-Attention combina a informa√ß√£o da entrada codificada com a informa√ß√£o dos documentos recuperados codificados, gerando uma representa√ß√£o contextualizada da entrada.
5.  **Gera√ß√£o:** A representa√ß√£o contextualizada √© utilizada para gerar a sa√≠da.

**Detalhamento do Processo de *Retrieval***

O processo de *retrieval* no RETRO [^8] √© um aspecto central da sua arquitetura. A ideia principal √© fornecer contexto adicional ao modelo durante a gera√ß√£o de texto, buscando informa√ß√µes relevantes que n√£o est√£o presentes nos seus par√¢metros internos.

1.  **Divis√£o em *Chunks***: A sequ√™ncia de entrada √© dividida em *chunks* de tamanho fixo. Esta divis√£o permite que o modelo busque documentos relevantes para cada segmento da entrada, em vez de considerar a sequ√™ncia inteira como um todo.

> üí° **Exemplo Num√©rico:** Suponha que a sequ√™ncia de entrada seja "O RETRO combina recupera√ß√£o e gera√ß√£o de texto." e o tamanho do chunk seja 3 tokens. A sequ√™ncia seria dividida em: ["O RETRO combina", "recupera√ß√£o e gera√ß√£o", "de texto."].

2.  **Busca de Textos Similares:** Para cada *chunk*, o Frozen BERT Retriever busca textos similares no banco de dados externo. A similaridade √© medida atrav√©s de uma fun√ß√£o de dist√¢ncia, como a dist√¢ncia do cosseno, entre a representa√ß√£o vetorial do *chunk* e as representa√ß√µes vetoriais dos documentos no banco de dados.

> üí° **Exemplo Num√©rico:**  Considere que o chunk "O RETRO combina" √© representado pelo vetor $v_c = [0.2, 0.5, 0.1, 0.2]$.  No banco de dados, temos dois documentos, $d_1$ e $d_2$, com vetores $v_1 = [0.1, 0.4, 0.2, 0.3]$ e $v_2 = [0.8, 0.1, 0.0, 0.1]$, respectivamente.  A similaridade do cosseno √© calculada como:
>
> $\text{Cosine Similarity}(v_c, v_1) = \frac{v_c \cdot v_1}{||v_c|| \cdot ||v_1||} = \frac{(0.2*0.1 + 0.5*0.4 + 0.1*0.2 + 0.2*0.3)}{\sqrt{0.2^2 + 0.5^2 + 0.1^2 + 0.2^2} \cdot \sqrt{0.1^2 + 0.4^2 + 0.2^2 + 0.3^2}} = \frac{0.3}{\sqrt{0.34} \cdot \sqrt{0.3}} \approx 0.97$
>
> $\text{Cosine Similarity}(v_c, v_2) = \frac{v_c \cdot v_2}{||v_c|| \cdot ||v_2||} = \frac{(0.2*0.8 + 0.5*0.1 + 0.1*0.0 + 0.2*0.1)}{\sqrt{0.34} \cdot \sqrt{0.66}} = \frac{0.23}{\sqrt{0.34} \cdot \sqrt{0.66}} \approx 0.49$
>
> Neste caso, o documento $d_1$ seria considerado mais similar ao chunk "O RETRO combina" do que o documento $d_2$.

3.  **Utiliza√ß√£o de *Neighbor chunks* e *Continuation chunks***: O RETRO utiliza dois tipos de *chunks* para melhorar a precis√£o do *retrieval*:
    *   ***Neighbor chunks***: S√£o *chunks* adjacentes ao *chunk* atual. A utiliza√ß√£o de *neighbor chunks* ajuda a capturar o contexto local da entrada.
    *   ***Continuation chunks***: S√£o *chunks* que continuam a sequ√™ncia do *chunk* atual. A utiliza√ß√£o de *continuation chunks* ajuda a prever o que vem a seguir na sequ√™ncia.

A combina√ß√£o de *neighbor chunks* e *continuation chunks* permite que o RETRO capture tanto o contexto local quanto o contexto global da entrada, resultando em um *retrieval* mais preciso e relevante.

Para complementar essa descri√ß√£o, podemos formalizar a combina√ß√£o de *neighbor chunks* e *continuation chunks*.

**Defini√ß√£o 1**
Seja $c_i$ o *chunk* atual. Definimos o conjunto de *neighbor chunks* $N(c_i)$ como $\{c_{i-l}, \ldots, c_{i-1}, c_{i+1}, \ldots, c_{i+l}\}$, onde $l$ √© o tamanho da janela de vizinhan√ßa. Similarmente, definimos o conjunto de *continuation chunks* $C(c_i)$ como $\{c_{i+k_1}, c_{i+k_2}, \ldots, c_{i+k_p}\}$, onde $k_1 < k_2 < \ldots < k_p$ s√£o os offsets que indicam quais *chunks* subsequentes ser√£o considerados.

**Teorema 1**
A similaridade combinada $S_{comb}(c_i, d_j)$ entre um *chunk* $c_i$ e um documento $d_j$, considerando *neighbor chunks* e *continuation chunks*, pode ser expressa como uma m√©dia ponderada das similaridades individuais:

$$S_{comb}(c_i, d_j) = \alpha s(c_i, d_j) + \beta \frac{1}{|N(c_i)|} \sum_{c_k \in N(c_i)} s(c_k, d_j) + \gamma \frac{1}{|C(c_i)|} \sum_{c_l \in C(c_i)} s(c_l, d_j)$$

onde $\alpha$, $\beta$, e $\gamma$ s√£o pesos que satisfazem $\alpha + \beta + \gamma = 1$, e $|\cdot|$ denota a cardinalidade do conjunto.

*Proof Strategy:* A prova decorre diretamente da defini√ß√£o de m√©dia ponderada. Os pesos $\alpha$, $\beta$, e $\gamma$ representam a import√¢ncia relativa do *chunk* atual, dos *neighbor chunks*, e dos *continuation chunks*, respectivamente.

> üí° **Exemplo Num√©rico:**  Considerando o exemplo anterior, e assumindo que o tamanho da janela de vizinhan√ßa $l = 1$ e temos um *continuation chunk* com offset $k_1 = 1$.  Sejam as similaridades:
>
> $s(c_i, d_j) = 0.97$ (similaridade do chunk atual com o documento j)
>
> $s(c_{i-1}, d_j) = 0.85$ (similaridade do neighbor chunk anterior com o documento j)
>
> $s(c_{i+1}, d_j) = 0.90$ (similaridade do neighbor chunk posterior com o documento j)
>
> $s(c_{i+k_1}, d_j) = s(c_{i+1}, d_j) = 0.90$ (similaridade do continuation chunk com o documento j, neste caso √© o mesmo que o neighbor chunk posterior)
>
> E os pesos $\alpha = 0.5$, $\beta = 0.3$, $\gamma = 0.2$. Ent√£o:
>
> $S_{comb}(c_i, d_j) = 0.5 * 0.97 + 0.3 * \frac{0.85 + 0.90}{2} + 0.2 * 0.90 = 0.485 + 0.3 * 0.875 + 0.18 = 0.485 + 0.2625 + 0.18 = 0.9275$
>
> Este valor representa a similaridade combinada do chunk atual com o documento *j*, levando em considera√ß√£o seus vizinhos e continuadores, ponderados pelos pesos $\alpha, \beta, \gamma$. A escolha desses pesos influencia a import√¢ncia relativa de cada tipo de chunk no processo de retrieval.

**Vantagens do RETRO**

*   **Acesso a Conhecimento Externo:** O RETRO pode acessar informa√ß√µes que n√£o est√£o presentes nos seus par√¢metros internos, permitindo gerar textos mais informativos e precisos.
*   **Adaptabilidade:** O RETRO pode ser adaptado a diferentes dom√≠nios e tarefas simplesmente alterando o banco de dados externo utilizado para o *retrieval*.

> üí° **Exemplo Num√©rico:** Considere um cen√°rio onde o RETRO √© inicialmente treinado para gerar textos sobre hist√≥ria. Ao mudar o banco de dados externo para um contendo informa√ß√µes sobre medicina, o RETRO pode ser adaptado para gerar textos na √°rea m√©dica, sem necessidade de um re-treinamento completo do modelo. Isso demonstra a adaptabilidade proporcionada pelo acesso a conhecimento externo.

*   **Efici√™ncia:** O Chunked Cross-Attention permite um processamento mais eficiente da informa√ß√£o, tornando o RETRO mais r√°pido do que outros modelos que utilizam mecanismos de aten√ß√£o mais complexos.

Para fortalecer o conceito de adaptabilidade, podemos adicionar o seguinte corol√°rio:

**Corol√°rio 1.1**
Seja $D_1$ e $D_2$ dois bancos de dados externos distintos. Um modelo RETRO treinado com $D_1$ pode ser adaptado para $D_2$ sem re-treinamento completo, ajustando-se os par√¢metros da fun√ß√£o de similaridade $s(c_i, d_{ij})$ e, possivelmente, os pesos $\alpha$, $\beta$, e $\gamma$ na similaridade combinada $S_{comb}(c_i, d_j)$.

*Proof Strategy:* A prova repousa sobre a modularidade do RETRO. Como o Frozen BERT Retriever √© congelado, a adapta√ß√£o para um novo banco de dados se resume a ajustar a fun√ß√£o de similaridade para refletir melhor a relev√¢ncia dos documentos em $D_2$ com rela√ß√£o aos *chunks* de entrada. Ajustar os pesos $\alpha$, $\beta$, e $\gamma$ permite refinar a import√¢ncia relativa dos diferentes tipos de *chunks* (atual, vizinhos e continuadores) na determina√ß√£o da similaridade.

**Considera√ß√µes Matem√°ticas**

Seja $x = (x_1, x_2, \ldots, x_n)$ a sequ√™ncia de entrada, onde $x_i$ representa o $i$-√©simo token da sequ√™ncia. O RETRO divide a sequ√™ncia em *chunks* de tamanho $k$, resultando em $m = \lceil \frac{n}{k} \rceil$ *chunks*: $c_1, c_2, \ldots, c_m$.

> üí° **Exemplo Num√©rico:** Se a sequ√™ncia de entrada tem 25 tokens e o tamanho do chunk √© 5 ($n=25, k=5$), ent√£o o n√∫mero de chunks ser√° $m = \lceil \frac{25}{5} \rceil = 5$.

Para cada *chunk* $c_i$, o Frozen BERT Retriever busca $r$ documentos relevantes no banco de dados externo, denotados por $d_{i1}, d_{i2}, \ldots, d_{ir}$. A similaridade entre o *chunk* $c_i$ e o documento $d_{ij}$ √© medida por uma fun√ß√£o $s(c_i, d_{ij})$, que pode ser, por exemplo, a dist√¢ncia do cosseno entre as representa√ß√µes vetoriais dos dois textos.

O Chunked Cross-Attention combina a representa√ß√£o do *chunk* $c_i$ com as representa√ß√µes dos documentos recuperados $d_{i1}, d_{i2}, \ldots, d_{ir}$. A sa√≠da do Chunked Cross-Attention √© uma representa√ß√£o contextualizada do *chunk* $c_i$, que √© utilizada para gerar a sa√≠da correspondente.

Podemos adicionar uma an√°lise sobre a complexidade computacional do processo de *retrieval*:

**Proposi√ß√£o 1**
A complexidade computacional do processo de *retrieval* para uma sequ√™ncia de entrada de tamanho $n$, dividida em $m$ *chunks*, buscando $r$ documentos por *chunk* em um banco de dados de tamanho $N$, √© $O(m \cdot (E + N))$, onde $E$ √© a complexidade da fun√ß√£o de similaridade $s(c_i, d_{ij})$ e assumindo que a busca exaustiva √© utilizada.

*Proof Strategy:* Para cada um dos $m$ *chunks*, o Frozen BERT Retriever calcula a similaridade entre o *chunk* e todos os $N$ documentos no banco de dados externo. O c√°lculo da similaridade tem complexidade $E$. Portanto, a complexidade total √© $O(m \cdot N \cdot E)$.  Se a similaridade utilizada for a dist√¢ncia do cosseno, e as representa√ß√µes vetoriais j√° estiverem pr√©-computadas, ent√£o $E$ ser√° a complexidade do produto interno entre dois vetores. Se for utilizada indexa√ß√£o (e.g., usando estruturas como Faiss), a busca pelos $r$ documentos mais similares pode ser feita em tempo sublinear em $N$, alterando a complexidade total. Na aus√™ncia de indexa√ß√£o, considera-se a busca exaustiva.

> üí° **Exemplo Num√©rico:**  Suponha que temos 10 chunks ($m=10$), um banco de dados com 1 milh√£o de documentos ($N = 1,000,000$), e a fun√ß√£o de similaridade tem complexidade linear no tamanho do vetor de embedding ($E = d$, onde $d$ √© a dimensionalidade do embedding). Sem indexa√ß√£o, a complexidade total do retrieval seria $O(10 * (d + 1,000,000))$.  Se $d$ for pequeno comparado a 1,000,000, a complexidade √© essencialmente $O(10,000,000)$.  Com indexa√ß√£o eficiente, a complexidade pode ser reduzida significativamente.
>
> Agora, suponha que estamos usando BM25 como fun√ß√£o de similaridade e queremos comparar com o uso de embeddings densos. Considere os seguintes par√¢metros:
>
> *   N√∫mero de Chunks (m): 10
> *   Tamanho do Banco de Dados (N): 1,000,000 documentos
> *   N√∫mero de Documentos Recuperados por Chunk (r): 5
>
> **BM25 (Sparse Retrieval):**
>
> BM25 envolve opera√ß√µes sobre um vocabul√°rio esparso (conjunto de termos). Assumindo que o c√°lculo do BM25 para um documento tem complexidade $E_{BM25}$, que depende do tamanho m√©dio do documento e do vocabul√°rio. A complexidade total √© $O(m \cdot N \cdot E_{BM25})$.
>
> **Dense Retrieval (e.g., Cosine Similarity):**
>
> O Dense Retrieval envolve a compara√ß√£o de embeddings densos. Se $d$ √© a dimens√£o do embedding, a complexidade de calcular a similaridade do cosseno para um documento √© $O(d)$. A complexidade total √© $O(m \cdot N \cdot d)$.
>
> Vamos supor que $E_{BM25}$ √© proporcional ao n√∫mero m√©dio de termos √∫nicos por documento, que chamaremos de $V$. Neste cen√°rio, $E_{BM25} \approx V$.
>
> Suponha que:
>
> *   $d = 768$ (dimens√£o do embedding BERT)
> *   $V = 200$ (n√∫mero m√©dio de termos √∫nicos por documento)
>
> Sem indexa√ß√£o:
>
> *   Complexidade BM25: $O(10 \cdot 1,000,000 \cdot 200) = O(2,000,000,000)$
> *   Complexidade Dense: $O(10 \cdot 1,000,000 \cdot 768) = O(7,680,000,000)$
>
> A complexidade de Dense Retrieval, neste exemplo, √© maior. No entanto, com indexa√ß√£o (e.g., usando FAISS para Dense Retrieval), a busca pelos $r$ documentos mais similares pode ser feita em tempo logar√≠tmico ou sublinear em $N$. Portanto, a complexidade se torna $O(m \cdot (d + \log(N)))$, que √© significativamente menor.
>
> Considerando a indexa√ß√£o para Dense Retrieval, a complexidade se torna:
>
> *   Complexidade Dense com indexa√ß√£o: $O(10 \cdot (768 + \log(1,000,000))) \approx O(10 \cdot (768 + 13.8)) \approx O(7,818)$
>
> **Tabela Comparativa:**
>
> | M√©todo               | Complexidade (Sem Indexa√ß√£o) | Complexidade (Com Indexa√ß√£o para Dense) |
> | -------------------- | --------------------------- | --------------------------------------- |
> | BM25 (Sparse)       | $O(2,000,000,000)$          | N/A (Indexa√ß√£o padr√£o invertida)         |
> | Dense (Cosine)      | $O(7,680,000,000)$          | $O(7,818)$                               |
>
> *Interpreta√ß√£o:* A indexa√ß√£o para Dense Retrieval reduz drasticamente a complexidade computacional, tornando-o mais eficiente do que BM25, especialmente quando o tamanho do banco de dados √© grande. BM25 usa indexa√ß√£o invertida por padr√£o, tornando a compara√ß√£o direta complexa sem especificar a estrutura e complexidade da indexa√ß√£o.
>
> **Prompt Engineering Exemplo:**
>
> Suponha que queremos que o modelo RETRO gere um resumo conciso de um artigo cient√≠fico.
>
> Prompt Inicial: "Resuma este artigo cient√≠fico:" + Artigo
>
> Chunking: O artigo √© dividido em chunks de 256 tokens.
>
> Retrieval: Para cada chunk, documentos relevantes s√£o recuperados do banco de dados.
>
> Prompt Aprimorado (considerando o Retrieval): "Resuma este artigo cient√≠fico, usando as seguintes informa√ß√µes de suporte: [Documento Recuperado 1], [Documento Recuperado 2]:" + Artigo
>
> Varia√ß√µes do Prompt:
>
> *   Prompt com Contexto Espec√≠fico: "Resuma este artigo cient√≠fico focando nos resultados experimentais, usando as seguintes informa√ß√µes de suporte: [Documento Recuperado 1 - Se√ß√£o de Resultados], [Documento Recuperado 2 - An√°lise Estat√≠stica]:" + Artigo
> *   Prompt para Compara√ß√£o: "Compare este artigo cient√≠fico com [Outro Artigo], usando as seguintes informa√ß√µes de suporte: [Documento Recuperado 1 - Discuss√£o de Trabalhos Relacionados], [Documento Recuperado 2 - Metodologias Similares]:" + Artigo
>
> A escolha do prompt influencia diretamente o conte√∫do gerado, permitindo direcionar o modelo para aspectos espec√≠ficos do artigo e integrar o conhecimento externo recuperado de forma mais eficaz.



![RETRO architecture showing chunked cross-attention and neighbor retrieval for enhanced transformer performance.](./../images/image5.jpg)

### Conclus√£o

O RETRO representa uma abordagem promissora para combinar a capacidade de *retrieval* com a gera√ß√£o de texto. A sua arquitetura modular e o seu processo de *retrieval* eficiente permitem que o modelo acesse conhecimento externo, adapte-se a diferentes dom√≠nios e gere textos mais informativos e precisos. A utiliza√ß√£o de *neighbor chunks* e *continuation chunks* melhora a precis√£o do *retrieval*, enquanto o Chunked Cross-Attention permite um processamento eficiente da informa√ß√£o. O RETRO demonstra o potencial da integra√ß√£o de *retrieval* e gera√ß√£o para aprimorar a capacidade dos modelos de linguagem.

### Refer√™ncias
[^8]: Retrieval-Enhanced Transformer (RETRO) combines a frozen BERT retriever, a differentiable encoder, and chunked cross-attention to generate output. It performs retrieval throughout the pre-training stage, fetching relevant documents based on chunks of the input. The retrieval process involves splitting the input sequence into chunks, finding similar texts to the previous chunks to provide context, and using Neighbor chunks and Continuation chunks to compute similarity and assist in text generation.
<!-- END -->