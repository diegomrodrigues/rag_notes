## Soft Prompt Tuning em RAG com LLMs

### Introdu√ß√£o
No contexto de *Retrieval-Augmented Generation* (RAG) com *Large Language Models* (LLMs), o ajuste fino (fine-tuning) emerge como uma estrat√©gia crucial para otimizar o desempenho do modelo em tarefas espec√≠ficas. Dentro do leque de t√©cnicas de ajuste fino, o **soft prompt tuning** se destaca como uma abordagem eficiente e flex√≠vel. A t√©cnica consiste em adicionar um tensor trein√°vel √†s *input embeddings* do modelo, criando um *soft prompt* que √© otimizado via retropropaga√ß√£o (*backpropagation*) [^1]. Isso permite a incorpora√ß√£o de sinais a partir de um n√∫mero arbitr√°rio de exemplos rotulados [^1].

### Conceitos Fundamentais

#### Soft Prompts: A Ess√™ncia do Ajuste Fino

Diferente dos *hard prompts*, que consistem em sequ√™ncias discretas de tokens, os *soft prompts* s√£o representa√ß√µes cont√≠nuas aprendidas atrav√©s do treinamento. Essa natureza cont√≠nua oferece maior flexibilidade e capacidade de adapta√ß√£o em compara√ß√£o com os *hard prompts*. O *soft prompt tuning* explora essa caracter√≠stica ao introduzir um tensor trein√°vel que √© concatenado √†s *input embeddings* do modelo [^1]. Esse tensor, denominado *soft prompt*, √© otimizado durante o processo de treinamento, permitindo que o modelo capture nuances e informa√ß√µes espec√≠ficas da tarefa em quest√£o.

#### Implementa√ß√£o Matem√°tica

Formalmente, seja $E \in \mathbb{R}^{V \times d}$ a matriz de *embeddings* do modelo, onde $V$ √© o tamanho do vocabul√°rio e $d$ √© a dimens√£o da *embedding*. Para uma sequ√™ncia de entrada $x = (x_1, x_2, ..., x_n)$, as *embeddings* correspondentes s√£o dadas por $e = (E_{x_1}, E_{x_2}, ..., E_{x_n})$, onde $E_{x_i} \in \mathbb{R}^d$ √© a *embedding* do token $x_i$.

No *soft prompt tuning*, introduzimos um tensor trein√°vel $P \in \mathbb{R}^{l \times d}$, onde $l$ √© o comprimento do *soft prompt*. O *soft prompt* √© concatenado √†s *embeddings* de entrada, resultando em uma nova sequ√™ncia de *embeddings* $e' = (P_1, P_2, ..., P_l, E_{x_1}, E_{x_2}, ..., E_{x_n})$, onde $P_i \in \mathbb{R}^d$ √© a *embedding* do $i$-√©simo token do *soft prompt*.

Durante o treinamento, apenas os par√¢metros do tensor $P$ s√£o atualizados, enquanto os demais par√¢metros do LLM permanecem congelados. Isso torna o *soft prompt tuning* uma t√©cnica *parameter-efficient*, pois requer um n√∫mero significativamente menor de par√¢metros a serem treinados em compara√ß√£o com o ajuste fino completo.

> üí° **Exemplo Num√©rico:**
> Suponha que temos um LLM com $N = 100$ milh√µes de par√¢metros ($10^8$).  Vamos criar um soft prompt com um comprimento de $l = 20$ tokens e uma dimens√£o de embedding de $d = 768$.
>
> O n√∫mero de par√¢metros trein√°veis no soft prompt √© $l \times d = 20 \times 768 = 15360$.
>
> A raz√£o entre o n√∫mero de par√¢metros trein√°veis no soft prompt e o n√∫mero total de par√¢metros no modelo √©:
>
> $$\frac{15360}{100,000,000} = 0.0001536$$
>
> Expressa em porcentagem, essa raz√£o √© $0.01536\%$. Isso ilustra a efici√™ncia param√©trica do soft prompt tuning, pois apenas uma pequena fra√ß√£o dos par√¢metros do modelo precisa ser ajustada.
>
> Uma abordagem de fine-tuning completa ajustaria todos os $10^8$ par√¢metros. Portanto, a economia de par√¢metros √© significativa.

**Proposi√ß√£o 1:** *A raz√£o entre o n√∫mero de par√¢metros trein√°veis no soft prompt tuning e o n√∫mero total de par√¢metros no modelo √© tipicamente muito pequena, refletindo a efici√™ncia param√©trica da t√©cnica.*

*Demonstra√ß√£o:* Seja $N$ o n√∫mero total de par√¢metros no LLM, e $l \times d$ o n√∫mero de par√¢metros no tensor $P$. A raz√£o √© dada por $\frac{l \times d}{N}$. Como $l$ e $d$ s√£o geralmente ordens de magnitude menores que $N$, a raz√£o √© muito pequena.

#### Otimiza√ß√£o via Retropropaga√ß√£o

O *soft prompt* √© otimizado utilizando o algoritmo de retropropaga√ß√£o (*backpropagation*) [^1]. Dado um conjunto de dados de treinamento rotulado, o modelo √© treinado para minimizar uma fun√ß√£o de perda espec√≠fica da tarefa. O gradiente da fun√ß√£o de perda em rela√ß√£o aos par√¢metros do *soft prompt* √© calculado e utilizado para atualizar o tensor $P$ atrav√©s de um otimizador como o Adam ou o SGD.

> üí° **Exemplo Num√©rico:**
> Imagine que estamos usando um dataset de treinamento com 1000 exemplos. A fun√ß√£o de perda escolhida √© a cross-entropy loss. Ap√≥s cada batch de exemplos (digamos, um batch size de 32), o gradiente da fun√ß√£o de perda em rela√ß√£o ao tensor $P$ √© calculado.  Suponha que ap√≥s um determinado batch, o gradiente para o primeiro elemento $P_1$ do tensor $P$ (que √© um vetor de dimens√£o $d$) seja:
>
> $\nabla_{P_1} L = [0.01, -0.02, 0.005, ..., 0.015]$ (um vetor de dimens√£o 768, no nosso exemplo anterior).
>
> Usando o otimizador Adam com uma taxa de aprendizado de $\alpha = 0.001$, a atualiza√ß√£o para $P_1$ seria:
>
> $P_1^{t+1} = P_1^{t} - \alpha \cdot \nabla_{P_1} L$
>
> $P_1^{t+1} = P_1^{t} - 0.001 \cdot [0.01, -0.02, 0.005, ..., 0.015]$
>
> Essa atualiza√ß√£o √© aplicada a todos os elementos do tensor $P$ iterativamente at√© a converg√™ncia. A converg√™ncia √© avaliada monitorando a fun√ß√£o de perda no conjunto de valida√ß√£o.

**Teorema 1:** *O processo de otimiza√ß√£o do soft prompt converge para um ponto estacion√°rio da fun√ß√£o de perda, desde que a fun√ß√£o de perda seja suficientemente suave e o otimizador utilizado satisfa√ßa certas condi√ß√µes de converg√™ncia.*

*Demonstra√ß√£o (Esbo√ßo):* A demonstra√ß√£o segue os resultados padr√£o de converg√™ncia para algoritmos de otimiza√ß√£o baseados em gradiente. As condi√ß√µes de suavidade da fun√ß√£o de perda garantem que o gradiente seja Lipschitz cont√≠nuo, e as condi√ß√µes sobre o otimizador (e.g., Adam, SGD com decaimento da taxa de aprendizado) garantem que o algoritmo n√£o oscile demais e se aproxime de um m√≠nimo local. A demonstra√ß√£o completa dependeria da escolha espec√≠fica do otimizador e da fun√ß√£o de perda.

**Lema 1.1:** *Se a fun√ß√£o de perda √© convexa, ent√£o o ponto estacion√°rio encontrado √© um m√≠nimo global.*

*Demonstra√ß√£o:* Por defini√ß√£o, uma fun√ß√£o convexa tem apenas um m√≠nimo global. Se o algoritmo de otimiza√ß√£o converge para um ponto estacion√°rio, e a fun√ß√£o √© convexa, ent√£o esse ponto deve ser o m√≠nimo global.

#### Vantagens do Soft Prompt Tuning

*   **Efici√™ncia Param√©trica:** O *soft prompt tuning* requer o ajuste de apenas um pequeno n√∫mero de par√¢metros, tornando-o mais eficiente em termos de computa√ß√£o e mem√≥ria do que o ajuste fino completo.
*   **Flexibilidade:** O *soft prompt* pode ser adaptado a diferentes tarefas e dom√≠nios simplesmente treinando um novo tensor $P$.
*   **Preserva√ß√£o do Conhecimento Pr√©vio:** Ao manter os par√¢metros do LLM congelados, o *soft prompt tuning* preserva o conhecimento pr√©vio do modelo, evitando o esquecimento catastr√≥fico.
*   **Aplicabilidade:** Permite a incorpora√ß√£o de sinais de qualquer n√∫mero de exemplos rotulados [^1].

#### Desafios e Considera√ß√µes

Apesar de suas vantagens, o *soft prompt tuning* tamb√©m apresenta alguns desafios. A escolha do comprimento do *soft prompt* ($l$) √© um hiperpar√¢metro importante que pode afetar o desempenho do modelo. Um comprimento muito curto pode limitar a capacidade do *soft prompt* de capturar informa√ß√µes relevantes, enquanto um comprimento muito longo pode levar ao *overfitting*. Al√©m disso, a inicializa√ß√£o do tensor $P$ tamb√©m pode influenciar o desempenho do modelo.

> üí° **Exemplo Num√©rico:**
> Considere um cen√°rio onde estamos ajustando um LLM para a tarefa de responder perguntas sobre artigos cient√≠ficos.
>
> *   **Caso 1: Comprimento do soft prompt muito curto (l=5):**  O soft prompt pode n√£o ter capacidade suficiente para guiar o LLM a extrair informa√ß√µes relevantes dos documentos recuperados. O modelo pode ter dificuldade em discernir entre diferentes tipos de informa√ß√µes (e.g., resultados, metodologia, conclus√µes).
> *   **Caso 2: Comprimento do soft prompt muito longo (l=100):** O soft prompt pode se tornar muito espec√≠fico para o conjunto de treinamento, levando ao overfitting. O modelo pode memorizar detalhes dos documentos de treinamento em vez de aprender a generalizar para novos documentos.
>
> A escolha ideal do comprimento do soft prompt geralmente requer experimenta√ß√£o. Uma abordagem comum √© testar diferentes comprimentos (e.g., 10, 20, 50) e avaliar o desempenho no conjunto de valida√ß√£o.

**Proposi√ß√£o 2:** *A escolha do comprimento l do soft prompt tem uma rela√ß√£o com a complexidade da tarefa.*

*Justificativa:* Tarefas mais complexas podem exigir soft prompts mais longos para capturar as nuances necess√°rias, enquanto tarefas mais simples podem ser resolvidas com soft prompts mais curtos. Um soft prompt muito longo para uma tarefa simples pode levar ao overfitting, como mencionado anteriormente.

### Soft Prompt Tuning no Contexto do RAG

No contexto do RAG, o *soft prompt tuning* pode ser utilizado para otimizar a intera√ß√£o entre o componente de recupera√ß√£o e o componente de gera√ß√£o. Por exemplo, um *soft prompt* pode ser treinado para direcionar o modelo a prestar mais aten√ß√£o a certos tipos de documentos recuperados ou a gerar respostas mais relevantes para a consulta do usu√°rio. Al√©m disso, o *soft prompt tuning* pode ser utilizado para adaptar o modelo a um dom√≠nio espec√≠fico, melhorando a precis√£o e a relev√¢ncia das respostas geradas.



> üí° **Exemplo Num√©rico:**
> Suponha que temos um sistema RAG para responder a perguntas sobre documentos m√©dicos.
>
> **Cen√°rio:** Um usu√°rio pergunta: "Quais s√£o os sintomas da gripe?"
>
> 1.  **Recupera√ß√£o:** O sistema recupera os seguintes documentos:
>     *   Documento 1: "A gripe √© causada pelo v√≠rus influenza. Sintomas comuns incluem febre, tosse e dor de garganta."
>     *   Documento 2: "A COVID-19 √© uma doen√ßa respirat√≥ria causada pelo SARS-CoV-2. Os sintomas podem variar, incluindo febre, tosse e perda de olfato."
>
> 2.  **Soft Prompt Tuning:** Treinamos um soft prompt para que o modelo preste mais aten√ß√£o a documentos que mencionam explicitamente os sintomas. O soft prompt aprende a ponderar as embeddings de palavras como "sintomas", "febre", "tosse", etc.
>
> 3.  **Gera√ß√£o:**
>     *   **Sem soft prompt tuning:** O modelo pode gerar uma resposta gen√©rica que combina informa√ß√µes dos dois documentos, como: "Os sintomas incluem febre, tosse e podem incluir perda de olfato." (incorreto, pois perda de olfato √© mais associado √† COVID-19).
>     *   **Com soft prompt tuning:** O modelo, guiado pelo soft prompt, presta mais aten√ß√£o ao Documento 1 e gera uma resposta mais precisa: "Os sintomas da gripe incluem febre, tosse e dor de garganta."
>
> Isso demonstra como o soft prompt tuning pode melhorar a precis√£o das respostas geradas pelo sistema RAG, direcionando o modelo a utilizar as informa√ß√µes mais relevantes dos documentos recuperados.
>
> Considere a seguinte tabela de resultados hipot√©ticos, medidos em um conjunto de teste:
>
> | M√©todo                       | Precis√£o | Recall | F1-Score |
> | ----------------------------- | -------- | ------ | -------- |
> | RAG sem soft prompt tuning    | 0.75     | 0.70   | 0.72     |
> | RAG com soft prompt tuning   | 0.85     | 0.80   | 0.82     |
>
> A tabela mostra que o soft prompt tuning melhorou significativamente a precis√£o, o recall e o F1-score do sistema RAG, indicando um desempenho superior na tarefa de responder a perguntas sobre documentos m√©dicos.
>
> **An√°lise Residual:** A an√°lise dos casos em que o sistema ainda falha pode revelar que o soft prompt precisa ser aprimorado para lidar com perguntas mais complexas que envolvem racioc√≠nio sobre m√∫ltiplos documentos ou informa√ß√µes impl√≠citas.

**Teorema 2:** *Ao aplicar soft prompt tuning no contexto do RAG, o modelo converge para uma representa√ß√£o que equilibra a informa√ß√£o recuperada com o conhecimento pr√©-existente no LLM.*

*Demonstra√ß√£o (Esbo√ßo):* O soft prompt atua como um "filtro" ou "adaptador" entre a informa√ß√£o recuperada e a gera√ß√£o de texto. O processo de otimiza√ß√£o ajusta os par√¢metros do soft prompt de forma a minimizar a perda, que depende tanto da relev√¢ncia da informa√ß√£o recuperada quanto da coer√™ncia da resposta gerada com o conhecimento pr√©vio do LLM. O ponto de converg√™ncia representa um equil√≠brio entre esses dois fatores.





![Compara√ß√£o entre 'Fine-tuning' e 'Prefix-tuning' em modelos Transformer, mostrando a otimiza√ß√£o de par√¢metros em cada abordagem.](./../images/image18.jpg)





![RAG architecture: Enhancing language models with external knowledge retrieval for improved answer generation.](./../images/image17.jpg)

### Conclus√£o

O *soft prompt tuning* √© uma t√©cnica de ajuste fino promissora para LLMs, oferecendo efici√™ncia param√©trica, flexibilidade e preserva√ß√£o do conhecimento pr√©vio. Sua aplicabilidade no contexto do RAG permite otimizar a intera√ß√£o entre os componentes de recupera√ß√£o e gera√ß√£o, melhorando o desempenho do modelo em tarefas espec√≠ficas. Ao adicionar um tensor trein√°vel √†s *input embeddings* [^1], o soft prompt permite a incorpora√ß√£o de sinais de um n√∫mero arbitr√°rio de exemplos rotulados [^1].

### Refer√™ncias
[^1]: Soft prompt tuning is a fine-tuning technique that adds a trainable tensor to the input embeddings of the model, creating a soft prompt that can be learned via backpropagation. This allows the incorporation of signals from any number of labeled examples.
<!-- END -->