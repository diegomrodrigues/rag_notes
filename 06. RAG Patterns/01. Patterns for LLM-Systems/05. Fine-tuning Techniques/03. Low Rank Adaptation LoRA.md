## Low-Rank Adaptation (LoRA) para Fine-tuning Eficiente em RAG

### Introdu√ß√£o

A adapta√ß√£o de modelos de linguagem pr√©-treinados (PLMs) para tarefas espec√≠ficas em sistemas de Neural Information Retrieval (NIR) e Retrieval-Augmented Generation (RAG) √© um desafio constante. O fine-tuning completo, que envolve ajustar todos os par√¢metros do modelo, pode ser computacionalmente caro e propenso a overfitting, especialmente com modelos de bilh√µes de par√¢metros. A t√©cnica Low-Rank Adaptation (LoRA) [^3] surge como uma alternativa eficiente, explorando a hip√≥tese de que as atualiza√ß√µes de pesos necess√°rias para adaptar um PLM a uma nova tarefa residem em um espa√ßo de baixa dimens√£o. Este cap√≠tulo explora os fundamentos te√≥ricos de LoRA, suas vantagens e desvantagens, e sua aplica√ß√£o em contextos de RAG.

### Conceitos Fundamentais

LoRA se baseia na observa√ß√£o de que PLMs pr√©-treinados possuem uma dimens√£o intr√≠nseca baixa quando adaptados a uma tarefa espec√≠fica [^3]. Em outras palavras, as atualiza√ß√µes de pesos necess√°rias para que um modelo pr√©-treinado desempenhe bem em uma nova tarefa podem ser representadas de forma eficiente em um espa√ßo de dimens√£o muito menor do que o espa√ßo original dos pesos do modelo.

Formalmente, LoRA prop√µe aproximar a matriz de atualiza√ß√£o de pesos $\Delta W \in \mathbb{R}^{d \times k}$ como um produto de duas matrizes menores, $A \in \mathbb{R}^{d \times r}$ e $B \in \mathbb{R}^{r \times k}$, onde $r \ll \min(d, k)$. Assim, temos:

$$\Delta W \approx BA$$

Aqui, $r$ √© o *rank* da adapta√ß√£o LoRA e controla a dimens√£o do espa√ßo de baixa dimens√£o. Durante o fine-tuning, apenas as matrizes $A$ e $B$ s√£o treinadas, enquanto os pesos originais $W$ do PLM s√£o mantidos fixos. A sa√≠da do modelo √© ent√£o calculada como:

$$h = Wx + BAx$$

onde $x$ √© a entrada e $h$ √© a sa√≠da.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma camada linear em um modelo de linguagem com uma matriz de pesos $W$ de dimens√£o $d \times k = 1024 \times 2048$. O fine-tuning completo dessa camada envolveria otimizar todos $1024 \times 2048 = 2,097,152$ par√¢metros. Com LoRA, podemos escolher um rank $r = 16$.  Ent√£o, criamos duas matrizes, $A$ de dimens√£o $1024 \times 16$ e $B$ de dimens√£o $16 \times 2048$. O n√∫mero total de par√¢metros trein√°veis com LoRA seria $(1024 \times 16) + (16 \times 2048) = 16,384 + 32,768 = 49,152$. Isso representa uma redu√ß√£o de aproximadamente 97.65% no n√∫mero de par√¢metros trein√°veis, tornando o fine-tuning muito mais eficiente.
>
> $\text{Redu√ß√£o de par√¢metros} = \frac{2097152 - 49152}{2097152} \approx 0.9765$
>
> ```python
> import numpy as np
>
> d = 1024
> k = 2048
> r = 16
>
> # Fine-tuning completo
> num_params_full = d * k
> print(f"N√∫mero de par√¢metros no fine-tuning completo: {num_params_full}")
>
> # LoRA
> num_params_A = d * r
> num_params_B = r * k
> num_params_lora = num_params_A + num_params_B
> print(f"N√∫mero de par√¢metros com LoRA: {num_params_lora}")
>
> # Redu√ß√£o
> reduction = (num_params_full - num_params_lora) / num_params_full
> print(f"Redu√ß√£o na quantidade de par√¢metros: {reduction:.4f}")
> ```

**Teorema 1:** *Converg√™ncia de LoRA*. Assumindo que a fun√ß√£o de perda √© Lipschitz cont√≠nua e suave, e que o otimizador utilizado garante uma diminui√ß√£o suficiente da fun√ß√£o de perda a cada itera√ß√£o, o treinamento com LoRA converge para um m√≠nimo local da fun√ß√£o de perda.

*Prova (Esbo√ßo):* A prova se baseia na an√°lise da din√¢mica do gradiente descendente aplicado √†s matrizes $A$ e $B$. Como o n√∫mero de par√¢metros trein√°veis √© significativamente menor do que no fine-tuning completo, a converg√™ncia tende a ser mais est√°vel e menos sujeita a overfitting. A Lipschitz continuidade e a suavidade garantem que o gradiente seja bem-comportado, e a diminui√ß√£o suficiente da fun√ß√£o de perda assegura a progress√£o em dire√ß√£o a um m√≠nimo local. Uma an√°lise mais detalhada envolveria a aplica√ß√£o de teoremas de converg√™ncia para otimiza√ß√£o n√£o-convexa.

**Vantagens de LoRA:**

*   **Efici√™ncia computacional:** LoRA reduz significativamente o n√∫mero de par√¢metros trein√°veis, economizando mem√≥ria e tempo de computa√ß√£o durante o fine-tuning.  Isto √© crucial ao lidar com modelos extremamente grandes.
*   **Redu√ß√£o do risco de overfitting:** Ao restringir as atualiza√ß√µes de pesos a um espa√ßo de baixa dimens√£o, LoRA atua como uma forma de regulariza√ß√£o impl√≠cita, ajudando a evitar o overfitting em conjuntos de dados menores.
*   **Facilidade de implanta√ß√£o:**  Como LoRA n√£o altera os pesos originais do modelo, √© poss√≠vel armazenar e trocar diferentes adapta√ß√µes LoRA (pares de matrizes A e B) de forma eficiente, permitindo a adapta√ß√£o r√°pida a diferentes tarefas sem a necessidade de replicar o modelo base completo.

**Desvantagens de LoRA:**

*   **Necessidade de escolha do rank:** A escolha do rank $r$ √© um hiperpar√¢metro cr√≠tico que afeta o desempenho do modelo. Um rank muito baixo pode limitar a capacidade do modelo de aprender adapta√ß√µes complexas, enquanto um rank muito alto pode reduzir os benef√≠cios de efici√™ncia computacional e regulariza√ß√£o.
*   **Performance:** Em alguns casos, LoRA pode n√£o atingir o mesmo n√≠vel de performance do fine-tuning completo, especialmente em tarefas que requerem adapta√ß√µes mais significativas dos pesos do modelo.

Para mitigar essa desvantagem, podemos considerar uma abordagem hier√°rquica, onde diferentes ranks s√£o usados para diferentes camadas do modelo.

**Teorema 1.1:** *LoRA Hier√°rquico*. Seja $r_i$ o rank utilizado para a camada $i$ de um modelo com $n$ camadas. A escolha de $r_i$ pode ser adaptada para cada camada, permitindo maior flexibilidade na adapta√ß√£o do modelo.

*Prova (Esbo√ßo):* A prova consiste em demonstrar que a otimiza√ß√£o dos ranks $r_i$ para cada camada pode ser formulada como um problema de otimiza√ß√£o multi-objetivo, onde o objetivo √© maximizar a performance do modelo enquanto minimiza o n√∫mero total de par√¢metros trein√°veis.  T√©cnicas de busca de hiperpar√¢metros, como Bayesian Optimization, podem ser utilizadas para encontrar a configura√ß√£o √≥tima dos $r_i$.

> üí° **Exemplo Num√©rico:**
>
> Imagine um modelo com 3 camadas. Em vez de usar um rank fixo $r=16$ para todas as camadas, podemos experimentar ranks diferentes: $r_1=8$ para a primeira camada, $r_2=16$ para a segunda e $r_3=32$ para a terceira.  A escolha desses valores pode depender da import√¢ncia de cada camada para a tarefa espec√≠fica.  Por exemplo, se a terceira camada lida com o racioc√≠nio de alto n√≠vel, podemos alocar um rank maior para permitir uma adapta√ß√£o mais flex√≠vel.
>
> A tabela abaixo ilustra essa abordagem:
>
> | Camada | Dimens√£o da Matriz de Peso Original ($d_i \times k_i$) | Rank LoRA ($r_i$) | N√∫mero de Par√¢metros LoRA |
> |---|---|---|---|
> | 1 | 512 x 1024 | 8 | (512 x 8) + (8 x 1024) = 12288 |
> | 2 | 1024 x 2048 | 16 | (1024 x 16) + (16 x 2048) = 49152 |
> | 3 | 2048 x 4096 | 32 | (2048 x 32) + (32 x 4096) = 196608 |
> | **Total** |  |  | **257,048** |

**Aplica√ß√£o de LoRA em RAG:**

Em sistemas RAG, LoRA pode ser aplicada para fine-tuning tanto o modelo de recupera√ß√£o quanto o modelo de gera√ß√£o.

*   **Fine-tuning do modelo de recupera√ß√£o:** Ao adaptar o modelo de recupera√ß√£o (por exemplo, um modelo de embedding como SentenceBERT) com LoRA, podemos melhorar a relev√¢ncia dos documentos recuperados para uma determinada consulta. Isso √© feito treinando as matrizes $A$ e $B$ para otimizar a similaridade entre consultas e documentos relevantes.

![RAG architecture: Enhancing language models with external knowledge retrieval for improved answer generation.](./../images/image17.jpg)

*   **Fine-tuning do modelo de gera√ß√£o:** LoRA pode ser usado para adaptar o modelo de gera√ß√£o (por exemplo, um modelo de linguagem como GPT-3) para gerar respostas mais precisas, concisas e relevantes com base nos documentos recuperados.  Neste caso, as matrizes $A$ e $B$ s√£o treinadas para otimizar a probabilidade de gerar a resposta correta, dadas a consulta e os documentos relevantes.

> üí° **Exemplo Num√©rico:**
>
> Considere um cen√°rio onde o modelo de recupera√ß√£o inicial retorna os seguintes documentos para uma consulta:
>
> | Documento | Pontua√ß√£o de Similaridade | Relevante? |
> |---|---|---|
> | Doc 1 | 0.75 | Sim |
> | Doc 2 | 0.70 | N√£o |
> | Doc 3 | 0.65 | Sim |
> | Doc 4 | 0.60 | N√£o |
>
> Ap√≥s fine-tuning o modelo de recupera√ß√£o com LoRA, as pontua√ß√µes podem mudar:
>
> | Documento | Pontua√ß√£o de Similaridade (Ap√≥s LoRA) | Relevante? |
> |---|---|---|
> | Doc 1 | 0.85 | Sim |
> | Doc 3 | 0.78 | Sim |
> | Doc 2 | 0.62 | N√£o |
> | Doc 4 | 0.55 | N√£o |
>
> A principal mudan√ßa √© que o Doc 3, que era relevante, agora tem uma pontua√ß√£o maior do que o Doc 2, que n√£o era relevante. Isso indica que o fine-tuning com LoRA melhorou a capacidade do modelo de recupera√ß√£o de priorizar documentos relevantes.  As pontua√ß√µes dos documentos relevantes (Doc 1 e Doc 3) aumentaram, enquanto as pontua√ß√µes dos irrelevantes (Doc 2 e Doc 4) diminu√≠ram ou permaneceram baixas.





![Diagrama do m√©todo Low-Rank Adaptation (LoRA) para ajuste fino de modelos de linguagem.](./../images/image25.jpg)

**Considera√ß√µes pr√°ticas:**

*   **Escolha do rank:** A escolha do rank $r$ geralmente requer experimenta√ß√£o. Come√ßar com valores pequenos (por exemplo, 8, 16) e aument√°-los gradualmente at√© que a performance comece a se estabilizar √© uma abordagem comum.
*   **Localiza√ß√£o dos m√≥dulos LoRA:** √â importante decidir quais m√≥dulos do modelo ser√£o adaptados com LoRA.  Geralmente, adaptar as camadas de aten√ß√£o e as camadas feedforward √© uma boa pr√°tica.

Uma outra considera√ß√£o importante √© a inicializa√ß√£o das matrizes A e B.

**Proposi√ß√£o 2:** *Inicializa√ß√£o de Matrizes LoRA*. A inicializa√ß√£o das matrizes A e B afeta a estabilidade e a velocidade de converg√™ncia do treinamento com LoRA. Inicializar A com uma matriz aleat√≥ria com desvio padr√£o baixo e B com zeros tende a melhorar a converg√™ncia inicial.

*Prova (Esbo√ßo):* A prova se baseia na an√°lise da magnitude do gradiente inicial. Inicializar B com zeros garante que a atualiza√ß√£o inicial dos pesos seja pequena, evitando grandes saltos no espa√ßo de par√¢metros que podem desestabilizar o treinamento. A inicializa√ß√£o de A com uma matriz aleat√≥ria com desvio padr√£o baixo garante que a dire√ß√£o inicial da atualiza√ß√£o seja aleat√≥ria, explorando diferentes regi√µes do espa√ßo de par√¢metros.

> üí° **Exemplo Num√©rico:**
>
> Seja $A$ uma matriz $2 \times 2$ inicializada aleatoriamente com desvio padr√£o baixo (por exemplo, 0.01) e $B$ uma matriz $2 \times 2$ inicializada com zeros:
>
> $$A = \begin{bmatrix} 0.005 & -0.002 \\ 0.001 & 0.003 \end{bmatrix}$$
>
> $$B = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix}$$
>
> Inicializar $B$ com zeros garante que a atualiza√ß√£o inicial $\Delta W = BA$ seja tamb√©m uma matriz de zeros, preservando o estado pr√©-treinado do modelo inicialmente. A pequena magnitude dos elementos em $A$ garante que, √† medida que $B$ come√ßa a ser atualizado, as mudan√ßas em $\Delta W$ s√£o graduais e est√°veis.

*   **Combina√ß√£o com outras t√©cnicas:** LoRA pode ser combinada com outras t√©cnicas de fine-tuning, como o uso de learning rates ajustados ou o uso de data augmentation.

### Conclus√£o
<!-- END -->