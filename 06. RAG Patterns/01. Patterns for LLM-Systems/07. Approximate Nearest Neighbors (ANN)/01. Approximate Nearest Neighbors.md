## Approximate Nearest Neighbors (ANN) em Recupera√ß√£o de Informa√ß√£o Neural

### Introdu√ß√£o

Em sistemas de Recupera√ß√£o de Informa√ß√£o Neural (NIR) e na arquitetura Retrieval-Augmented Generation (RAG) com Large Language Models (LLMs), a capacidade de encontrar rapidamente os vizinhos mais pr√≥ximos de um dado vetor em um espa√ßo de alta dimensionalidade √© crucial. No entanto, a busca exaustiva pelos *$k$* vizinhos mais pr√≥ximos (k-NN) torna-se computacionalmente proibitiva √† medida que a dimens√£o dos vetores e o tamanho do conjunto de dados aumentam. Para mitigar este problema, t√©cnicas de Approximate Nearest Neighbors (ANN) [^1] s√£o empregadas. Este cap√≠tulo explora em detalhe o conceito de ANN, destacando suas vantagens, desvantagens e aplica√ß√µes no contexto de NIR e RAG.

### Conceitos Fundamentais

**Defini√ß√£o de Approximate Nearest Neighbors (ANN)**

ANN √© uma t√©cnica que visa encontrar os vizinhos mais pr√≥ximos de um ponto de dados em um espa√ßo vetorial de alta dimens√£o, priorizando a velocidade de recupera√ß√£o em detrimento da precis√£o exata [^1]. Em outras palavras, ao inv√©s de retornar os *$k$* vizinhos mais pr√≥ximos verdadeiros, ANN retorna um conjunto de *$k$* vizinhos que s√£o *aproximadamente* os mais pr√≥ximos. Essa aproxima√ß√£o √© fundamental para otimizar o desempenho em cen√°rios onde a lat√™ncia √© cr√≠tica, como em sistemas de busca e recomenda√ß√£o em tempo real.

**Trade-off entre Precis√£o e Velocidade**

O core do ANN reside no equil√≠brio entre a precis√£o dos resultados e a velocidade da busca. A busca exata por k-NN garante que os *$k$* pontos mais similares sejam sempre retornados, mas o custo computacional aumenta linearmente com o tamanho do conjunto de dados, tornando-se impratic√°vel para grandes volumes de dados [^1]. ANN introduz uma pequena margem de erro para obter uma acelera√ß√£o significativa na velocidade de busca. Este trade-off √© controlado por par√¢metros espec√≠ficos de cada algoritmo ANN, permitindo que o desenvolvedor ajuste a precis√£o de acordo com os requisitos da aplica√ß√£o.

> üí° **Exemplo Num√©rico:** Imagine que voc√™ tem um banco de dados com 1 milh√£o de documentos (*n* = 1,000,000) e cada documento √© representado por um vetor de 1000 dimens√µes (*d* = 1000). Para encontrar os 10 vizinhos mais pr√≥ximos usando busca exaustiva, seria necess√°rio calcular a dist√¢ncia entre a consulta e cada um dos 1 milh√£o de documentos. Isso significa 1,000,000 * 1000 = 1 bilh√£o de opera√ß√µes de dist√¢ncia. Se cada opera√ß√£o de dist√¢ncia leva 1 microssegundo, a busca demoraria aproximadamente 1000 segundos, o que √© inaceit√°vel para uma busca em tempo real. Usando ANN, podemos reduzir esse tempo para, digamos, 100 milissegundos, com uma pequena perda de precis√£o (por exemplo, retornar 9 dos 10 vizinhos mais pr√≥ximos verdadeiros).

**Lema 1:** *Complexidade da Busca K-NN.* A busca exata por k-NN em um conjunto de dados de tamanho *$n$* e dimens√£o *$d$* possui complexidade de tempo O(*n* *d*).

*Demonstra√ß√£o:* Para cada ponto de consulta, √© necess√°rio calcular a dist√¢ncia at√© todos os *$n$* pontos no conjunto de dados. Calcular a dist√¢ncia entre dois vetores de dimens√£o *$d$* requer O(*d*) opera√ß√µes. Portanto, a complexidade total √© O(*n* *d*).

**Algoritmos Comuns de ANN**

V√°rios algoritmos de ANN foram desenvolvidos para diferentes tipos de dados e requisitos de desempenho. Alguns dos mais comuns incluem:

*   **Locality Sensitive Hashing (LSH):** LSH utiliza fun√ß√µes hash que mapeiam pontos similares em um espa√ßo de alta dimens√£o para o mesmo "bucket" com alta probabilidade [^1]. A busca ent√£o se restringe aos pontos dentro do mesmo bucket, reduzindo drasticamente o n√∫mero de compara√ß√µes necess√°rias.

**Teorema 1:** *Fam√≠lia de Fun√ß√µes LSH.* Uma fam√≠lia de fun√ß√µes *$H$* √© (r, cr, p1, p2)-sensitive se para quaisquer pontos *$p$*, *$q$*:
    *   Se dist(*p*, *q*) <= *$r$*, ent√£o Pr[h(*p*) = h(*q*)] >= *$p1$*.
    *   Se dist(*p*, *q*) >= *$cr$*, ent√£o Pr[h(*p*) = h(*q*)] <= *$p2$*.
    onde *$h$* √© escolhida uniformemente de *$H$* e *$p1$* > *$p2$*.

> üí° **Exemplo Num√©rico (LSH):** Suponha que temos uma fam√≠lia de fun√ß√µes LSH onde *$r$* = 0.8 (dois vetores s√£o considerados similares se a dist√¢ncia entre eles for menor ou igual a 0.8), *$cr$* = 1.2 (dois vetores s√£o considerados diferentes se a dist√¢ncia entre eles for maior ou igual a 1.2), *$p1$* = 0.9 (probabilidade de vetores similares serem hasheados para o mesmo bucket √© 90%) e *$p2$* = 0.1 (probabilidade de vetores diferentes serem hasheados para o mesmo bucket √© 10%). Se dois vetores t√™m uma dist√¢ncia de 0.7, a probabilidade de eles serem colocados no mesmo bucket √© alta (90%). Se a dist√¢ncia for 1.5, a probabilidade cai para apenas 10%. Isso significa que, com alta probabilidade, LSH ir√° agrupar vetores similares e separar vetores diferentes.

*   **Hierarchical Navigable Small World (HNSW):** HNSW constr√≥i um grafo hier√°rquico multi-camadas, onde cada camada representa um n√≠vel diferente de granularidade [^1]. A busca come√ßa na camada superior e navega para as camadas inferiores, refinando progressivamente a busca at√© encontrar os vizinhos aproximados mais pr√≥ximos.

**Teorema 2:** *Constru√ß√£o do Grafo HNSW.* A constru√ß√£o do grafo HNSW envolve a inser√ß√£o iterativa de pontos, conectando cada ponto a seus *$M$* vizinhos mais pr√≥ximos na camada atual. A probabilidade de um n√≥ ser promovido para a pr√≥xima camada superior √© controlada por um par√¢metro probabil√≠stico.

> üí° **Exemplo Num√©rico (HNSW):** Imagine que voc√™ est√° construindo um grafo HNSW. Voc√™ insere um novo ponto e define *$M$* = 16 (cada ponto se conecta a 16 vizinhos na camada atual). A probabilidade de um n√≥ ser promovido para a camada superior √© de 0.3. Isso significa que, em m√©dia, 30% dos n√≥s ser√£o promovidos para a camada superior, criando uma estrutura hier√°rquica que permite uma busca r√°pida. A busca come√ßa na camada mais alta (a mais "grosseira") e, em seguida, se move para camadas inferiores mais detalhadas, restringindo rapidamente o espa√ßo de busca.

*   **Product Quantization (PQ):** PQ divide o espa√ßo vetorial em subespa√ßos menores e quantiza cada subespa√ßo usando um algoritmo de clustering [^1]. A dist√¢ncia entre os vetores √© ent√£o estimada com base nos c√≥digos de quantiza√ß√£o, permitindo uma busca r√°pida e eficiente.

**Lema 2:** *Erro de Quantiza√ß√£o em PQ.* O erro de quantiza√ß√£o em Product Quantization √© proporcional √† vari√¢ncia dos dados em cada subespa√ßo e inversamente proporcional ao n√∫mero de clusters utilizados para quantizar cada subespa√ßo. Reduzir a dimensionalidade dos subespa√ßos e aumentar o n√∫mero de clusters diminui o erro.

> üí° **Exemplo Num√©rico (PQ):** Suponha que voc√™ tem vetores de 128 dimens√µes e decide usar Product Quantization. Voc√™ divide o vetor em 8 subespa√ßos de 16 dimens√µes cada. Para cada subespa√ßo, voc√™ usa k-means clustering com 256 clusters. Cada subvetor de 16 dimens√µes √© ent√£o representado pelo ID do cluster mais pr√≥ximo. Durante a busca, a dist√¢ncia entre a consulta e os vetores no banco de dados √© estimada usando a dist√¢ncia entre os IDs dos clusters correspondentes. Isso reduz drasticamente o custo computacional, pois a compara√ß√£o √© feita no espa√ßo dos IDs dos clusters (256 valores) em vez do espa√ßo original de 16 dimens√µes. O erro de quantiza√ß√£o surge porque a representa√ß√£o pelo ID do cluster √© uma aproxima√ß√£o do vetor original.

*   **k-d Trees e varia√ß√µes:** Embora os k-d trees sejam mais adequados para espa√ßos de dimens√µes moderadas, varia√ß√µes como randomized k-d trees s√£o utilizadas em cen√°rios de ANN [^1]. Essas estruturas particionam o espa√ßo recursivamente, permitindo a elimina√ß√£o r√°pida de regi√µes irrelevantes durante a busca.

**Observa√ß√£o:** A escolha do algoritmo ANN mais apropriado depende fortemente da dimensionalidade dos dados, do tamanho do conjunto de dados, e dos requisitos espec√≠ficos de precis√£o e velocidade da aplica√ß√£o. Em geral, LSH √© eficaz para dados de alta dimensionalidade, enquanto HNSW oferece um bom balanceamento entre precis√£o e velocidade para uma ampla gama de conjuntos de dados. Product Quantization √© particularmente √∫til quando a compress√£o dos dados √© uma prioridade.

**M√©tricas de Avalia√ß√£o para ANN**

A avalia√ß√£o de algoritmos ANN envolve a utiliza√ß√£o de m√©tricas que quantificam tanto a precis√£o quanto a efici√™ncia. M√©tricas comuns incluem:

*   **Recall@K:** Mede a propor√ß√£o de vizinhos verdadeiros mais pr√≥ximos que s√£o recuperados nos *$k$* resultados retornados pelo algoritmo ANN.
*   **Precis√£o@K:** Mede a propor√ß√£o de vizinhos retornados que s√£o verdadeiramente os *$k$* vizinhos mais pr√≥ximos.
*   **Queries por Segundo (QPS):** Mede o n√∫mero de consultas que o sistema pode processar por segundo, refletindo a velocidade da busca.
*   **Tempo de Indexa√ß√£o:** Mede o tempo necess√°rio para construir o √≠ndice ANN a partir do conjunto de dados.

> üí° **Exemplo Num√©rico (M√©tricas):** Imagine que voc√™ est√° avaliando um sistema ANN para *$k$* = 10. Voc√™ executa 100 consultas. Para cada consulta, voc√™ verifica se os 10 resultados retornados pelo ANN est√£o entre os 10 vizinhos mais pr√≥ximos verdadeiros (calculados por busca exaustiva). Se, em m√©dia, 8 dos 10 resultados retornados forem vizinhos verdadeiros, ent√£o a Precis√£o@10 √© 0.8 ou 80%. Se, em m√©dia, o ANN recupera 9 dos 10 vizinhos verdadeiros, ent√£o o Recall@10 √© 0.9 ou 90%. Se o sistema consegue processar 500 consultas por segundo, ent√£o o QPS √© 500. Se o tempo para construir o √≠ndice ANN √© de 1 hora, ent√£o o Tempo de Indexa√ß√£o √© 1 hora.

**Aplica√ß√µes em NIR e RAG**

Em sistemas NIR, ANN √© fundamental para acelerar a busca por documentos relevantes em grandes cole√ß√µes de texto [^1]. Ao inv√©s de comparar a consulta com cada documento individualmente, a consulta √© convertida em um vetor e utilizada para buscar os documentos mais similares usando um √≠ndice ANN pr√©-constru√≠do.

Na arquitetura RAG, ANN desempenha um papel crucial na etapa de *retrieval*. Dado um prompt do usu√°rio, o sistema utiliza ANN para encontrar os documentos ou fragmentos de texto mais relevantes para o prompt [^1]. Estes documentos s√£o ent√£o concatenados com o prompt e alimentados a um LLM para gerar uma resposta informada e contextualizada. A velocidade e precis√£o do retrieval impactam diretamente a qualidade da resposta gerada pelo LLM.

> üí° **Exemplo Num√©rico (RAG):** Um usu√°rio pergunta: "Quais s√£o os principais sintomas da gripe?". O sistema RAG usa ANN para encontrar os 5 fragmentos de texto mais relevantes em uma base de conhecimento m√©dica (*k*=5). Suponha que o ANN retorna os seguintes fragmentos: 1) "A gripe causa febre alta", 2) "Tosse seca √© um sintoma comum", 3) "Dor de garganta pode ocorrer", 4) "N√°useas e v√¥mitos s√£o raros em adultos", 5) "Vacina√ß√£o √© a melhor preven√ß√£o". Esses fragmentos s√£o ent√£o combinados com o prompt do usu√°rio e enviados a um LLM, que gera a resposta: "Os principais sintomas da gripe s√£o febre alta, tosse seca e dor de garganta. N√°useas e v√¥mitos s√£o raros em adultos. A vacina√ß√£o √© a melhor forma de preven√ß√£o." A qualidade da resposta depende diretamente da relev√¢ncia dos fragmentos recuperados pelo ANN.



![RAG architecture: Enhancing language models with external knowledge retrieval for improved answer generation.](./../images/image17.jpg)

**Corol√°rio 1:** *Impacto do Recall@K na RAG.* Em sistemas RAG, um alto Recall@K na etapa de retrieval ANN garante que a maioria dos documentos relevantes para o prompt do usu√°rio seja recuperada, aumentando a probabilidade de o LLM gerar uma resposta precisa e completa.

**Extens√µes e T√≥picos Avan√ßados**

Al√©m dos algoritmos e m√©tricas j√° mencionados, a √°rea de ANN continua a evoluir. T√≥picos como a busca ANN em grafos, a adapta√ß√£o de algoritmos ANN para dados n√£o-vetoriais, e a utiliza√ß√£o de aprendizado de m√°quina para otimizar os par√¢metros dos algoritmos ANN representam √°reas de pesquisa ativa. Al√©m disso, a combina√ß√£o de ANN com t√©cnicas de compress√£o e quantiza√ß√£o de vetores permite a constru√ß√£o de sistemas de busca ainda mais eficientes e escal√°veis.

### Conclus√£o

A t√©cnica de Approximate Nearest Neighbors (ANN) [^1] representa uma ferramenta essencial para otimizar a busca por vizinhos mais pr√≥ximos em espa√ßos vetoriais de alta dimens√£o. Sua aplica√ß√£o em sistemas de Recupera√ß√£o de Informa√ß√£o Neural (NIR) e na arquitetura Retrieval-Augmented Generation (RAG) com Large Language Models (LLMs) √© fundamental para garantir a efici√™ncia e escalabilidade desses sistemas. A escolha do algoritmo ANN adequado depende das caracter√≠sticas dos dados, dos requisitos de desempenho e do trade-off desejado entre precis√£o e velocidade. O desenvolvimento e otimiza√ß√£o cont√≠nuos de algoritmos ANN continuam sendo uma √°rea ativa de pesquisa, impulsionada pela crescente demanda por sistemas de busca e recomenda√ß√£o mais r√°pidos e eficientes.

### Refer√™ncias

[^1]: Approximate Nearest Neighbors (ANN) is a technique used to find the nearest neighbors of a data point in a high-dimensional vector space. It optimizes retrieval speed at the expense of exact precision, returning the approximate (rather than exact) top k most similar neighbors.
<!-- END -->