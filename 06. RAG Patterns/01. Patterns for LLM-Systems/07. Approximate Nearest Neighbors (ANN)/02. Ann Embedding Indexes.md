## √çndices de Embeddings ANN: Estruturas de Dados para Busca Eficiente

### Introdu√ß√£o

A busca por vizinhos mais pr√≥ximos aproximados (ANN) √© uma etapa crucial em muitos sistemas de recupera√ß√£o de informa√ß√£o neural e RAG (Retrieval-Augmented Generation), especialmente quando lidamos com grandes volumes de dados e embeddings de alta dimensionalidade. Conforme mencionado anteriormente, a busca exata pelo vizinho mais pr√≥ximo pode se tornar computacionalmente proibitiva em tais cen√°rios. Os √≠ndices de embeddings ANN surgem como uma solu√ß√£o eficaz, oferecendo um compromisso entre precis√£o e efici√™ncia. Este cap√≠tulo se aprofunda nas estruturas de dados que permitem realizar buscas ANN de forma eficiente [^2].

### Conceitos Fundamentais

Os √≠ndices de embeddings ANN s√£o essencialmente estruturas de dados projetadas para particionar o espa√ßo de embeddings de forma a permitir uma r√°pida identifica√ß√£o das regi√µes onde o vetor de consulta (query vector) est√° localizado [^2]. Ao inv√©s de comparar o vetor de consulta com todos os vetores no √≠ndice, estas estruturas permitem focar a busca em um subconjunto significativamente menor, reduzindo o tempo de busca drasticamente.

V√°rias t√©cnicas populares s√£o utilizadas para construir esses √≠ndices, cada uma com suas pr√≥prias caracter√≠sticas e tradeoffs em termos de precis√£o, velocidade e requisitos de mem√≥ria [^2]. Vamos explorar algumas das mais relevantes:

1.  **Locality Sensitive Hashing (LSH):** LSH √© uma fam√≠lia de t√©cnicas de hashing que visa mapear vetores semelhantes para o mesmo "bucket" com alta probabilidade. A ideia central √© que, se dois vetores s√£o pr√≥ximos no espa√ßo original, eles ter√£o uma alta probabilidade de colidir no mesmo bucket ap√≥s a aplica√ß√£o da fun√ß√£o de hash.

    *   **Funcionamento:** LSH utiliza m√∫ltiplas fun√ß√µes de hash que s√£o sens√≠veis √† localidade. Cada fun√ß√£o de hash mapeia os vetores para buckets. Ao realizar uma busca, o vetor de consulta √© passado por essas fun√ß√µes de hash, e apenas os vetores nos buckets correspondentes s√£o considerados como candidatos.
    *   **Vantagens:** Simplicidade conceitual e implementa√ß√£o relativamente f√°cil.
    *   **Desvantagens:** Pode exigir m√∫ltiplas tabelas de hash para alcan√ßar boa precis√£o, resultando em maior consumo de mem√≥ria. A performance pode ser sens√≠vel √† escolha das fun√ß√µes de hash.

    > üí° **Exemplo Num√©rico:** Imagine temos 3 documentos representados por vetores bidimensionais: $d_1 = [1, 2]$, $d_2 = [1.5, 2.3]$, $d_3 = [5, 8]$. Usamos uma fun√ß√£o LSH que projeta esses vetores em uma linha definida por um vetor aleat√≥rio $r = [0.8, 0.6]$. O hash √© 1 se o produto escalar entre o documento e $r$ for positivo e 0 caso contr√°rio.
    >
    > $\text{Hash}(d, r) = \begin{cases} 1, & \text{se } d \cdot r > 0 \\ 0, & \text{se } d \cdot r \le 0 \end{cases}$
    >
    > Vamos calcular os hashes:
    >
    > $d_1 \cdot r = (1 \times 0.8) + (2 \times 0.6) = 0.8 + 1.2 = 2.0 > 0 \rightarrow \text{Hash}(d_1, r) = 1$
    > $d_2 \cdot r = (1.5 \times 0.8) + (2.3 \times 0.6) = 1.2 + 1.38 = 2.58 > 0 \rightarrow \text{Hash}(d_2, r) = 1$
    > $d_3 \cdot r = (5 \times 0.8) + (8 \times 0.6) = 4.0 + 4.8 = 8.8 > 0 \rightarrow \text{Hash}(d_3, r) = 1$
    >
    > Neste exemplo simplificado, $d_1$ e $d_2$ (que s√£o mais similares entre si) colidem no mesmo bucket (hash 1) junto com $d_3$, mesmo sendo menos similar. Para melhorar a precis√£o, poder√≠amos usar m√∫ltiplas fun√ß√µes de hash.
    >
    > Se usarmos outra fun√ß√£o LSH com $r' = [-0.6, 0.8]$:
    >
    > $d_1 \cdot r' = (1 \times -0.6) + (2 \times 0.8) = -0.6 + 1.6 = 1.0 > 0 \rightarrow \text{Hash}(d_1, r') = 1$
    > $d_2 \cdot r' = (1.5 \times -0.6) + (2.3 \times 0.8) = -0.9 + 1.84 = 0.94 > 0 \rightarrow \text{Hash}(d_2, r') = 1$
    > $d_3 \cdot r' = (5 \times -0.6) + (8 \times 0.8) = -3.0 + 6.4 = 3.4 > 0 \rightarrow \text{Hash}(d_3, r') = 1$
    >
    > Neste caso, todos ainda colidem. √â preciso combinar m√∫ltiplos hashes para refinar a busca. Por exemplo, concatenando os hashes ter√≠amos "11" para todos os documentos, mas com mais fun√ß√µes de hash, a probabilidade de documentos similares terem hashes significantemente diferentes diminui.

    Para complementar a discuss√£o sobre LSH, podemos analisar brevemente como diferentes fun√ß√µes de hash podem ser aplicadas e suas propriedades.

    **Proposi√ß√£o 1:** Existem diferentes fam√≠lias de fun√ß√µes LSH, como aquelas baseadas em proje√ß√µes aleat√≥rias (e.g., random hyperplane hashing) ou em quantiza√ß√£o vetorial. A escolha da fam√≠lia de fun√ß√µes LSH influencia diretamente a probabilidade de colis√£o e, portanto, a precis√£o da busca ANN.

    *   **Random Hyperplane Hashing:** Projeta os vetores em hiperplanos aleat√≥rios e usa o lado do hiperplano em que o vetor se encontra como o hash.
    *   **Quantiza√ß√£o Vetorial LSH:** Usa quantiza√ß√£o vetorial para agrupar vetores pr√≥ximos em clusters e usa o ID do cluster como o hash.

    A escolha da fun√ß√£o de hash ideal depende da distribui√ß√£o dos dados e dos requisitos de precis√£o da aplica√ß√£o.

2.  **Facebook AI Similarity Search (FAISS):** FAISS √© uma biblioteca desenvolvida pelo Facebook AI Research para busca eficiente de similaridade em grandes conjuntos de dados. Ela oferece uma ampla gama de algoritmos ANN, incluindo abordagens baseadas em quantiza√ß√£o, particionamento do espa√ßo e grafos.

    *   **Quantiza√ß√£o:** FAISS utiliza t√©cnicas de quantiza√ß√£o para comprimir os vetores de embeddings, reduzindo o tamanho do √≠ndice e acelerando a busca. A quantiza√ß√£o consiste em mapear os vetores originais para um conjunto menor de vetores "c√≥digo" (code vectors) ou centr√≥ides.
    *   **Particionamento do Espa√ßo:** FAISS tamb√©m implementa algoritmos que particionam o espa√ßo de embeddings em c√©lulas, permitindo que a busca seja restrita √†s c√©lulas mais relevantes.
    *   **Vantagens:** Alto desempenho e escalabilidade. Oferece diversas op√ß√µes de algoritmos, permitindo otimizar para diferentes cen√°rios.
    *   **Desvantagens:** Pode ser mais complexo de configurar e otimizar em compara√ß√£o com LSH.

    > üí° **Exemplo Num√©rico:** Suponha que temos um conjunto de embeddings bidimensionais. Aplicamos k-means com k=3 para criar 3 clusters (centr√≥ides): $c_1 = [1, 1]$, $c_2 = [5, 5]$, $c_3 = [8, 2]$. Agora, um novo vetor $v = [4, 4.2]$ chega. Calculamos a dist√¢ncia de $v$ para cada centr√≥ide:
    >
    > $\text{Dist}(v, c_1) = \sqrt{(4-1)^2 + (4.2-1)^2} = \sqrt{9 + 10.24} = \sqrt{19.24} \approx 4.39$
    > $\text{Dist}(v, c_2) = \sqrt{(4-5)^2 + (4.2-5)^2} = \sqrt{1 + 0.64} = \sqrt{1.64} \approx 1.28$
    > $\text{Dist}(v, c_3) = \sqrt{(4-8)^2 + (4.2-2)^2} = \sqrt{16 + 4.84} = \sqrt{20.84} \approx 4.57$
    >
    > O vetor $v$ √© mais pr√≥ximo de $c_2$. Portanto, na quantiza√ß√£o vetorial, $v$ seria representado por $c_2$. Isso reduz drasticamente a quantidade de mem√≥ria necess√°ria para armazenar os embeddings. Durante a busca, apenas os vetores pertencentes ao cluster de $c_2$ precisam ser comparados com $v$.

    Para ilustrar melhor o conceito de quantiza√ß√£o em FAISS, podemos introduzir a quantiza√ß√£o vetorial de produto (PQ).

    **Teorema 2:** A quantiza√ß√£o vetorial de produto (PQ) decomp√µe o espa√ßo de embeddings em subespa√ßos e quantiza cada subespa√ßo independentemente. Isso permite reduzir a distor√ß√£o da quantiza√ß√£o em compara√ß√£o com a quantiza√ß√£o vetorial direta, especialmente em dimens√µes elevadas.

    *   **Esbo√ßo da Prova:** Seja $x \in \mathbb{R}^D$ um vetor de embedding. A PQ decomp√µe $x$ em $m$ subvetores $x_1, \dots, x_m$, onde cada $x_i \in \mathbb{R}^{D/m}$. Cada subvetor $x_i$ √© quantizado para um centr√≥ide $c_i$ de um codebook. A representa√ß√£o quantizada de $x$ √© ent√£o dada por $\hat{x} = [c_1, \dots, c_m]$. A dist√¢ncia entre dois vetores quantizados pode ser calculada eficientemente usando tabelas de lookup pr√©-computadas.

3.  **Hierarchical Navigable Small Worlds (HNSW):** HNSW √© um algoritmo baseado em grafos que constr√≥i um grafo hier√°rquico onde cada n√≥ representa um vetor de embedding. A estrutura do grafo permite que a busca seja realizada de forma eficiente, navegando pelos vizinhos mais pr√≥ximos em cada camada da hierarquia.

    *   **Constru√ß√£o:** O grafo HNSW √© constru√≠do inserindo os vetores de embeddings incrementalmente. Cada vetor √© conectado a seus vizinhos mais pr√≥ximos em diferentes camadas da hierarquia. As camadas superiores cont√™m menos n√≥s e representam uma vis√£o mais geral do espa√ßo de embeddings, enquanto as camadas inferiores cont√™m mais n√≥s e representam uma vis√£o mais detalhada.
    *   **Busca:** A busca come√ßa na camada superior e navega pelo grafo, selecionando os vizinhos mais pr√≥ximos do vetor de consulta em cada camada. A busca continua at√© que a camada inferior seja alcan√ßada, onde os vizinhos mais pr√≥ximos finais s√£o identificados.
    *   **Vantagens:** Alta precis√£o e velocidade, especialmente em dados de alta dimensionalidade. Boa robustez em rela√ß√£o √† escolha dos par√¢metros.
    *   **Desvantagens:** Requer mais mem√≥ria do que outras t√©cnicas. A constru√ß√£o do √≠ndice pode ser relativamente lenta.

    > üí° **Exemplo Num√©rico:** Imagine um grafo HNSW com duas camadas. Na camada superior, temos tr√™s n√≥s representando clusters amplos de documentos: $C_1, C_2, C_3$. Um novo vetor de consulta $q$ chega. Come√ßamos a busca na camada superior. Digamos que $q$ est√° mais pr√≥ximo de $C_2$. Na camada inferior, exploramos apenas os n√≥s que s√£o vizinhos de $C_2$ (ou seus descendentes). Isso evita a necessidade de comparar $q$ com todos os n√≥s do grafo, acelerando a busca.

    Para entender melhor a constru√ß√£o do HNSW, podemos detalhar o processo de inser√ß√£o de um novo n√≥ no grafo.

    **Lema 3:** Ao inserir um novo n√≥ no grafo HNSW, ele √© conectado a seus $M$ vizinhos mais pr√≥ximos em cada camada da hierarquia. O valor de $M$ influencia o grau do grafo e, portanto, o tradeoff entre precis√£o e velocidade de busca.

    *   **Esbo√ßo da Prova:** O processo de inser√ß√£o come√ßa na camada superior e seleciona um conjunto de n√≥s candidatos. A busca pelos vizinhos mais pr√≥ximos √© realizada em cada camada, restringindo a busca √† vizinhan√ßa dos n√≥s candidatos. Os $M$ vizinhos mais pr√≥ximos s√£o selecionados e o novo n√≥ √© conectado a eles. Este processo √© repetido para cada camada da hierarquia.

4.  **Scalable Nearest Neighbors (ScaNN):** ScaNN √© um algoritmo desenvolvido pelo Google Research que combina quantiza√ß√£o e busca em √°rvore para alcan√ßar alta precis√£o e escalabilidade.

    *   **Quantiza√ß√£o:** ScaNN utiliza t√©cnicas de quantiza√ß√£o anisotropic para comprimir os vetores de embeddings, preservando a dire√ß√£o dos vetores.
    *   **Busca em √Årvore:** ScaNN constr√≥i uma √°rvore sobre os vetores quantizados, permitindo que a busca seja realizada de forma eficiente.
    *   **Vantagens:** Excelente desempenho em termos de precis√£o e velocidade, especialmente em grandes conjuntos de dados.
    *   **Desvantagens:** Pode ser mais complexo de implementar do zero.

A escolha do algoritmo ANN mais adequado depende das caracter√≠sticas espec√≠ficas do conjunto de dados, dos requisitos de precis√£o e velocidade, e das restri√ß√µes de recursos computacionais.

Para auxiliar na escolha do algoritmo mais adequado, podemos apresentar uma tabela comparativa simplificada.

**Tabela 1:** Compara√ß√£o Simplificada de Algoritmos ANN

| Algoritmo | Precis√£o | Velocidade | Mem√≥ria | Complexidade de Implementa√ß√£o |
| :-------- | :------- | :--------- | :------ | :-------------------------- |
| LSH       | Baixa    | Alta       | Baixa   | Baixa                       |
| FAISS     | M√©dia    | M√©dia      | M√©dia   | M√©dia                       |
| HNSW      | Alta     | Alta       | Alta    | M√©dia                       |
| ScaNN     | Alta     | Alta       | M√©dia   | Alta                        |

Esta tabela oferece uma vis√£o geral dos tradeoffs entre os diferentes algoritmos. √â importante notar que o desempenho real pode variar dependendo das caracter√≠sticas do conjunto de dados e dos par√¢metros de configura√ß√£o.

### Conclus√£o

Os √≠ndices de embeddings ANN s√£o componentes essenciais para a constru√ß√£o de sistemas de recupera√ß√£o de informa√ß√£o neural e RAG eficientes. Ao permitir a busca r√°pida e aproximada de vizinhos mais pr√≥ximos, eles viabilizam a recupera√ß√£o de informa√ß√µes relevantes em grandes conjuntos de dados, abrindo caminho para aplica√ß√µes como busca sem√¢ntica, sistemas de recomenda√ß√£o e gera√ß√£o de texto aprimorada. A escolha da t√©cnica ANN mais apropriada requer uma an√°lise cuidadosa das caracter√≠sticas do problema e dos tradeoffs entre precis√£o, velocidade e recursos computacionais.

### Refer√™ncias
[^2]: ANN embedding indexes are data structures that allow us to perform ANN searches efficiently. At a high level, they build partitions over the embedding space so we can quickly zoom in on the specific space where the query vector is. Popular techniques include Locality Sensitive Hashing (LSH), Facebook AI Similarity Search (FAISS), Hierarchical Navigable Small Worlds (HNSW) and Scalable Nearest Neighbors (ScaNN).
<!-- END -->