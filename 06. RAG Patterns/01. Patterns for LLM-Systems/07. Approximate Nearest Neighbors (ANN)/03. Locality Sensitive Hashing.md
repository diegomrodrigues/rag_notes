## Locality Sensitive Hashing (LSH) para Busca Aproximada de Vizinhos Mais Pr√≥ximos (ANN)

### Introdu√ß√£o

A busca por vizinhos mais pr√≥ximos (Nearest Neighbors - NN) √© uma tarefa fundamental em diversas √°reas, incluindo recupera√ß√£o de informa√ß√£o, reconhecimento de padr√µes e minera√ß√£o de dados. Em muitos cen√°rios pr√°ticos, lidamos com grandes volumes de dados de alta dimensionalidade, tornando a busca exata de NN computacionalmente proibitiva. A busca aproximada de vizinhos mais pr√≥ximos (Approximate Nearest Neighbors - ANN) surge como uma alternativa vi√°vel, sacrificando a exatid√£o em prol da efici√™ncia computacional. Uma das t√©cnicas mais populares para realizar ANN √© o Locality Sensitive Hashing (LSH), que ser√° o foco deste cap√≠tulo. [^3] O LSH cria fun√ß√µes hash que mapeiam itens similares para o mesmo "bucket" hash com alta probabilidade. Ao restringir a busca apenas aos buckets relevantes, o LSH permite realizar consultas ANN de forma eficiente. [^3]

### Conceitos Fundamentais

**Defini√ß√£o de Locality Sensitive Hashing (LSH)**

Uma fam√≠lia de fun√ß√µes hash $\mathcal{H}$ √© dita *locality sensitive* se, para duas fun√ß√µes hash $h$ selecionadas aleatoriamente de $\mathcal{H}$, e dois pontos $p$ e $q$, satisfizer as seguintes propriedades:

1.  Se $p$ e $q$ s√£o "pr√≥ximos" (i.e., $d(p, q) \leq r_1$), ent√£o $P_{\mathcal{H}}[h(p) = h(q)] \geq P_1$
2.  Se $p$ e $q$ s√£o "distantes" (i.e., $d(p, q) \geq r_2$), ent√£o $P_{\mathcal{H}}[h(p) = h(q)] \leq P_2$

Onde $d(p, q)$ √© uma fun√ß√£o de dist√¢ncia entre $p$ e $q$, $r_1$ e $r_2$ s√£o os raios definindo a proximidade, e $P_1 > P_2$ s√£o probabilidades. Em outras palavras, a probabilidade de colis√£o (i.e., que $p$ e $q$ sejam mapeados para o mesmo bucket) √© maior quando $p$ e $q$ s√£o similares do que quando s√£o dissimilares.

**Observa√ß√£o:** √â importante notar que a defini√ß√£o de LSH depende crucialmente da escolha da fun√ß√£o de dist√¢ncia $d(p, q)$. A qualidade do LSH, e portanto a efici√™ncia do ANN, est√° diretamente ligada √† capacidade de encontrar uma fam√≠lia de fun√ß√µes hash $\mathcal{H}$ que satisfa√ßa as propriedades definidas para a dist√¢ncia escolhida.

**Constru√ß√£o de Fun√ß√µes Hash LSH**

A constru√ß√£o de fun√ß√µes hash LSH depende da m√©trica de dist√¢ncia utilizada. Diferentes fam√≠lias de fun√ß√µes LSH s√£o projetadas para diferentes m√©tricas. Alguns exemplos comuns incluem:

*   **LSH para Dist√¢ncia Euclidiana:** Uma fam√≠lia de fun√ß√µes LSH para dist√¢ncia Euclidiana pode ser constru√≠da projetando os pontos em vetores aleat√≥rios. Dado um vetor aleat√≥rio $\mathbf{a}$ e um escalar aleat√≥rio $b$ uniformemente amostrado no intervalo $[0, w]$, onde $w$ √© um par√¢metro, a fun√ß√£o hash √© definida como:
    $$h_{\mathbf{a}, b}(\mathbf{p}) = \left\lfloor \frac{\mathbf{a} \cdot \mathbf{p} + b}{w} \right\rfloor$$
    Onde $\mathbf{a} \cdot \mathbf{p}$ representa o produto escalar entre os vetores $\mathbf{a}$ e $\mathbf{p}$. A ideia √© que pontos pr√≥ximos ter√£o proje√ß√µes similares em $\mathbf{a}$, e a quantiza√ß√£o pelo par√¢metro $w$ agrupa pontos com proje√ß√µes similares no mesmo bucket.

> üí° **Exemplo Num√©rico:**
>
> Considere dois pontos $\mathbf{p} = [1.0, 2.0]$ e $\mathbf{q} = [1.5, 2.5]$. Seja $\mathbf{a} = [0.5, 0.5]$ um vetor aleat√≥rio e $b = 0.3$ um escalar aleat√≥rio, com $w = 1$.
>
> $\text{Step 1: Calcular } \mathbf{a} \cdot \mathbf{p}$:
>
> $\mathbf{a} \cdot \mathbf{p} = (0.5 \times 1.0) + (0.5 \times 2.0) = 0.5 + 1.0 = 1.5$
>
> $\text{Step 2: Calcular } \mathbf{a} \cdot \mathbf{q}$:
>
> $\mathbf{a} \cdot \mathbf{q} = (0.5 \times 1.5) + (0.5 \times 2.5) = 0.75 + 1.25 = 2.0$
>
> $\text{Step 3: Calcular } h_{\mathbf{a}, b}(\mathbf{p})$:
>
> $h_{\mathbf{a}, b}(\mathbf{p}) = \left\lfloor \frac{1.5 + 0.3}{1} \right\rfloor = \left\lfloor 1.8 \right\rfloor = 1$
>
> $\text{Step 4: Calcular } h_{\mathbf{a}, b}(\mathbf{q})$:
>
> $h_{\mathbf{a}, b}(\mathbf{q}) = \left\lfloor \frac{2.0 + 0.3}{1} \right\rfloor = \left\lfloor 2.3 \right\rfloor = 2$
>
> Neste exemplo, $\mathbf{p}$ e $\mathbf{q}$ s√£o mapeados para buckets diferentes (1 e 2, respectivamente).  Repetindo esse processo com diferentes vetores $\mathbf{a}$ e escalares $b$, a probabilidade de colis√£o (i.e., ca√≠rem no mesmo bucket) refletir√° a proximidade entre $\mathbf{p}$ e $\mathbf{q}$.  Se $w$ fosse menor, digamos 0.5, ambos poderiam cair no mesmo bucket com maior probabilidade.

*   **LSH para Dist√¢ncia de Hamming:** Para a dist√¢ncia de Hamming, uma fun√ß√£o hash LSH simples pode ser constru√≠da selecionando um bit aleat√≥rio do vetor bin√°rio. Seja $i$ um √≠ndice aleat√≥rio entre 1 e o comprimento do vetor bin√°rio $\mathbf{p}$, a fun√ß√£o hash √© definida como:
    $$h_i(\mathbf{p}) = p_i$$
    Onde $p_i$ √© o $i$-√©simo bit de $\mathbf{p}$. Pontos com pequena dist√¢ncia de Hamming ter√£o alta probabilidade de ter o mesmo bit no √≠ndice $i$.

> üí° **Exemplo Num√©rico:**
>
> Sejam $\mathbf{p} = [1, 0, 1, 0]$ e $\mathbf{q} = [1, 0, 0, 0]$ dois vetores bin√°rios. A dist√¢ncia de Hamming entre $\mathbf{p}$ e $\mathbf{q}$ √© 1 (eles diferem em um bit).
>
> $\text{Step 1: Selecionar um √≠ndice aleat√≥rio } i$. Suponha que $i = 3$.
>
> $\text{Step 2: Calcular } h_i(\mathbf{p})$:
>
> $h_3(\mathbf{p}) = p_3 = 1$
>
> $\text{Step 3: Calcular } h_i(\mathbf{q})$:
>
> $h_3(\mathbf{q}) = q_3 = 0$
>
> Neste caso, $h_3(\mathbf{p}) \neq h_3(\mathbf{q})$.  Se tiv√©ssemos selecionado $i = 1$ ou $i = 2$ ou $i = 4$, ter√≠amos $h_i(\mathbf{p}) = h_i(\mathbf{q})$. A probabilidade de que $h_i(\mathbf{p}) = h_i(\mathbf{q})$ √© alta quando a dist√¢ncia de Hamming √© pequena.
>
> ```python
> import numpy as np
>
> # Vetores bin√°rios
> p = np.array([1, 0, 1, 0])
> q = np.array([1, 0, 0, 0])
>
> # Calcular dist√¢ncia de Hamming
> hamming_distance = np.sum(p != q)
> print(f"Dist√¢ncia de Hamming: {hamming_distance}")
>
> # Selecionar um √≠ndice aleat√≥rio
> i = np.random.randint(0, len(p))
> print(f"√çndice aleat√≥rio selecionado: {i}")
>
> # Calcular as fun√ß√µes hash
> h_p = p[i]
> h_q = q[i]
>
> print(f"h_i(p) = {h_p}")
> print(f"h_i(q) = {h_q}")
>
> # Verificar se houve colis√£o
> collision = h_p == h_q
> print(f"Colis√£o: {collision}")
> ```

**Lema 1:** A fun√ß√£o hash $h_i(\mathbf{p}) = p_i$ √© uma fun√ß√£o LSH para a dist√¢ncia de Hamming.
*Proof.* Sejam $\mathbf{p}$ e $\mathbf{q}$ dois vetores bin√°rios. Seja $d_H(\mathbf{p}, \mathbf{q})$ a dist√¢ncia de Hamming entre $\mathbf{p}$ e $\mathbf{q}$. A probabilidade de que $h_i(\mathbf{p}) = h_i(\mathbf{q})$ √© a probabilidade de que o $i$-√©simo bit de $\mathbf{p}$ e $\mathbf{q}$ sejam iguais. Isso acontece com probabilidade $1 - \frac{d_H(\mathbf{p}, \mathbf{q})}{n}$, onde $n$ √© o comprimento dos vetores. Se $d_H(\mathbf{p}, \mathbf{q}) \leq r_1$, ent√£o $P[h_i(\mathbf{p}) = h_i(\mathbf{q})] \geq 1 - \frac{r_1}{n} = P_1$. Se $d_H(\mathbf{p}, \mathbf{q}) \geq r_2$, ent√£o $P[h_i(\mathbf{p}) = h_i(\mathbf{q})] \leq 1 - \frac{r_2}{n} = P_2$. Escolhendo $r_1 < r_2$, temos $P_1 > P_2$, satisfazendo a defini√ß√£o de LSH. $\blacksquare$

*   **LSH para Dist√¢ncia de Jaccard:** Para a similaridade de Jaccard entre dois conjuntos $A$ e $B$, definida como $J(A, B) = \frac{|A \cap B|}{|A \cup B|}$, uma fun√ß√£o LSH pode ser constru√≠da usando a t√©cnica de MinHash.

    **Defini√ß√£o de MinHash:** Seja $\pi$ uma permuta√ß√£o aleat√≥ria do conjunto universal de elementos. A fun√ß√£o MinHash $h_{\pi}(A)$ √© definida como o elemento m√≠nimo de $A$ sob a permuta√ß√£o $\pi$:
    $$h_{\pi}(A) = \min_{\pi}(A)$$

    A probabilidade de que duas fun√ß√µes MinHash colidam (i.e., $h_{\pi}(A) = h_{\pi}(B)$) √© igual √† similaridade de Jaccard entre os conjuntos $A$ e $B$:
    $$P[h_{\pi}(A) = h_{\pi}(B)] = J(A, B)$$

    Portanto, MinHash √© uma fun√ß√£o LSH para a similaridade de Jaccard.

> üí° **Exemplo Num√©rico:**
>
> Sejam $A = \{1, 2, 3, 4, 5\}$ e $B = \{3, 4, 5, 6, 7\}$. A similaridade de Jaccard entre $A$ e $B$ √©:
>
> $J(A, B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|\{3, 4, 5\}|}{|\{1, 2, 3, 4, 5, 6, 7\}|} = \frac{3}{7} \approx 0.4286$
>
> Suponha que a permuta√ß√£o aleat√≥ria $\pi$ mapeie os elementos da seguinte forma:
>
> $\pi(1) = 5, \pi(2) = 2, \pi(3) = 7, \pi(4) = 1, \pi(5) = 3, \pi(6) = 6, \pi(7) = 4$
>
> $\text{Step 1: Calcular } h_{\pi}(A)$:
>
> $h_{\pi}(A) = \min_{\pi}(A) = \min(\pi(1), \pi(2), \pi(3), \pi(4), \pi(5)) = \min(5, 2, 7, 1, 3) = 1$ (correspondente ao elemento 4)
>
> $\text{Step 2: Calcular } h_{\pi}(B)$:
>
> $h_{\pi}(B) = \min_{\pi}(B) = \min(\pi(3), \pi(4), \pi(5), \pi(6), \pi(7)) = \min(7, 1, 3, 6, 4) = 1$ (correspondente ao elemento 4)
>
> Neste caso, $h_{\pi}(A) = h_{\pi}(B) = 1$.  Portanto, a fun√ß√£o MinHash colidiu.  A probabilidade de colis√£o, repetida com muitas permuta√ß√µes aleat√≥rias, se aproximar√° da similaridade de Jaccard.
>
> ```python
> import numpy as np
>
> # Conjuntos
> A = {1, 2, 3, 4, 5}
> B = {3, 4, 5, 6, 7}
>
> # Calcular similaridade de Jaccard
> jaccard_similarity = len(A.intersection(B)) / len(A.union(B))
> print(f"Similaridade de Jaccard: {jaccard_similarity}")
>
> # Permuta√ß√£o aleat√≥ria
> elementos = list(range(1, 8))
> np.random.shuffle(elementos)
> permutacao = {i: elementos[i-1] for i in range(1, 8)}
> print(f"Permuta√ß√£o aleat√≥ria: {permutacao}")
>
> # Fun√ß√£o MinHash
> def minhash(conjunto, permutacao):
>     min_hash = min([permutacao[x] for x in conjunto])
>     return min_hash
>
> # Calcular MinHash para A e B
> h_A = minhash(A, permutacao)
> h_B = minhash(B, permutacao)
>
> print(f"h(A) = {h_A}")
> print(f"h(B) = {h_B}")
>
> # Verificar se houve colis√£o
> collision = h_A == h_B
> print(f"Colis√£o: {collision}")
> ```

**Amplifica√ß√£o da Probabilidade de Colis√£o**

Para aumentar a probabilidade de encontrar vizinhos pr√≥ximos e reduzir a probabilidade de falsos positivos, v√°rias fun√ß√µes hash LSH s√£o combinadas. Duas t√©cnicas comuns s√£o:

1.  **AND-construction:** $k$ fun√ß√µes hash LSH s√£o concatenadas em uma √∫nica fun√ß√£o hash:
    $$g(\mathbf{p}) = (h_1(\mathbf{p}), h_2(\mathbf{p}), \dots, h_k(\mathbf{p}))$$
    Dois pontos $\mathbf{p}$ e $\mathbf{q}$ colidem somente se $h_i(\mathbf{p}) = h_i(\mathbf{q})$ para *todos* os $i = 1, 2, \dots, k$. Isso reduz a probabilidade de colis√£o, tornando a busca mais precisa.

> üí° **Exemplo Num√©rico:**
>
> Considere dois pontos $\mathbf{p} = [1, 0, 1, 0]$ e $\mathbf{q} = [1, 0, 0, 1]$. Vamos usar a fun√ß√£o hash para dist√¢ncia de Hamming com $k=2$.  Selecione dois √≠ndices aleat√≥rios: $i_1 = 1$ e $i_2 = 3$.
>
> $\text{Step 1: Calcular } h_1(\mathbf{p})$ e $h_1(\mathbf{q})$:
>
> $h_1(\mathbf{p}) = p_1 = 1$
>
> $h_1(\mathbf{q}) = q_1 = 1$
>
> $\text{Step 2: Calcular } h_2(\mathbf{p})$ e $h_2(\mathbf{q})$:
>
> $h_2(\mathbf{p}) = p_3 = 1$
>
> $h_2(\mathbf{q}) = q_3 = 0$
>
> $\text{Step 3: Aplicar AND-construction:}$
>
> $g(\mathbf{p}) = (h_1(\mathbf{p}), h_2(\mathbf{p})) = (1, 1)$
>
> $g(\mathbf{q}) = (h_1(\mathbf{q}), h_2(\mathbf{q})) = (1, 0)$
>
> Como $g(\mathbf{p}) \neq g(\mathbf{q})$, $\mathbf{p}$ e $\mathbf{q}$ *n√£o* colidem.  Com $k=1$, eles teriam colidido baseado no primeiro bit. A AND-construction torna a colis√£o mais restritiva.

2.  **OR-construction:** $L$ tabelas hash s√£o criadas, cada uma usando uma fun√ß√£o hash $g$ constru√≠da com a AND-construction. Para realizar uma busca, o ponto de consulta √© hasheado em cada uma das $L$ tabelas, e todos os pontos encontrados nos buckets correspondentes s√£o candidatos a vizinhos pr√≥ximos. Isso aumenta a probabilidade de encontrar todos os vizinhos pr√≥ximos.

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior, vamos criar $L=2$ tabelas hash.  Na primeira tabela, usamos os √≠ndices $i_1 = 1$ e $i_2 = 3$ (como antes). Na segunda tabela, usamos os √≠ndices $i_3 = 2$ e $i_4 = 4$.
>
> $\text{Tabela 1:}$ $g_1(\mathbf{p}) = (1, 1)$, $g_1(\mathbf{q}) = (1, 0)$
>
> $\text{Tabela 2:}$
>
> $h_3(\mathbf{p}) = p_2 = 0$
>
> $h_3(\mathbf{q}) = q_2 = 0$
>
> $h_4(\mathbf{p}) = p_4 = 0$
>
> $h_4(\mathbf{q}) = q_4 = 1$
>
> $g_2(\mathbf{p}) = (0, 0)$
>
> $g_2(\mathbf{q}) = (0, 1)$
>
> Durante a busca, $\mathbf{q}$ ser√° comparado com os pontos nos buckets correspondentes *em ambas as tabelas*.  Mesmo que $\mathbf{p}$ e $\mathbf{q}$ n√£o colidam na tabela 1, eles ainda ser√£o considerados como candidatos a vizinhos pr√≥ximos se ca√≠rem em buckets pr√≥ximos em pelo menos uma das $L$ tabelas.

**Teorema 1:** Combinando AND-construction e OR-construction, √© poss√≠vel controlar o balan√ßo entre precis√£o e recall no LSH.

*Proof Sketch:* A AND-construction reduz a probabilidade de colis√£o, aumentando a precis√£o (menos falsos positivos), mas diminuindo o recall (pode perder vizinhos pr√≥ximos). A OR-construction aumenta a probabilidade de colis√£o, aumentando o recall (encontra mais vizinhos pr√≥ximos), mas diminuindo a precis√£o (mais falsos positivos). Ajustando os par√¢metros $k$ (n√∫mero de fun√ß√µes hash na AND-construction) e $L$ (n√∫mero de tabelas hash na OR-construction), √© poss√≠vel controlar este balan√ßo.

**Algoritmo LSH para ANN**

O algoritmo LSH para ANN consiste em duas fases principais:

1.  **Pr√©-processamento:**
    *   Escolha par√¢metros $k$ e $L$ para a AND-construction e OR-construction, respectivamente.
    *   Construa $L$ tabelas hash. Para cada tabela:
        *   Selecione $k$ fun√ß√µes hash LSH aleatoriamente.
        *   Use a AND-construction para combinar as $k$ fun√ß√µes em uma √∫nica fun√ß√£o $g$.
        *   Hasheie todos os pontos do conjunto de dados na tabela usando a fun√ß√£o $g$.

2.  **Consulta:**
    *   Dado um ponto de consulta $\mathbf{q}$:
        *   Para cada uma das $L$ tabelas hash:
            *   Calcule $g(\mathbf{q})$.
            *   Recupere todos os pontos no bucket $g(\mathbf{q})$.
        *   Calcule a dist√¢ncia entre $\mathbf{q}$ e todos os pontos recuperados.
        *   Retorne os $k$ pontos mais pr√≥ximos de $\mathbf{q}$.

**Proposi√ß√£o 1:** A complexidade da fase de consulta do algoritmo LSH para ANN √© sublinear em rela√ß√£o ao tamanho do conjunto de dados.

*Proof Sketch:* Na fase de consulta, em vez de calcular a dist√¢ncia entre o ponto de consulta e todos os pontos do conjunto de dados, calculamos a dist√¢ncia apenas entre o ponto de consulta e os pontos recuperados dos buckets hash. O n√∫mero de pontos recuperados dos buckets hash √© geralmente muito menor do que o tamanho do conjunto de dados, resultando em uma complexidade sublinear.

**Teorema 2:** (Sensibilidade ao par√¢metro *w* na dist√¢ncia Euclidiana). A escolha do par√¢metro *w* na fun√ß√£o hash para Dist√¢ncia Euclidiana influencia diretamente na performance do LSH. Se *w* for muito pequeno, muitos pontos ser√£o mapeados para o mesmo bucket, aumentando os falsos positivos. Se *w* for muito grande, pontos pr√≥ximos podem ser mapeados para buckets diferentes, diminuindo o recall.

**Corol√°rio 2.1:** A otimiza√ß√£o do par√¢metro *w* pode ser realizada atrav√©s de valida√ß√£o cruzada, buscando o valor que maximize uma m√©trica de avalia√ß√£o apropriada, como a raz√£o entre precis√£o e recall.

> üí° **Exemplo Num√©rico:**
>
> Suponha que, ap√≥s a valida√ß√£o cruzada, os seguintes resultados foram obtidos para diferentes valores de *w*, com *k* e *L* fixos:
>
> | *w*   | Precis√£o | Recall | F1-Score |
> |-------|----------|--------|----------|
> | 0.25  | 0.3      | 0.9    | 0.45     |
> | 0.5   | 0.6      | 0.7    | 0.65     |
> | 0.75  | 0.75     | 0.6    | 0.67     |
> | 1.0   | 0.8      | 0.5    | 0.62     |
> | 1.25  | 0.85     | 0.4    | 0.54     |
>
> Neste caso, o valor de *w* = 0.5 resulta no maior F1-Score, indicando o melhor balan√ßo entre precis√£o e recall. Esse seria o valor √≥timo para este conjunto de dados e esses par√¢metros *k* e *L*.

### Conclus√£o

O Locality Sensitive Hashing (LSH) √© uma t√©cnica poderosa para realizar busca aproximada de vizinhos mais pr√≥ximos (ANN) em conjuntos de dados de alta dimensionalidade. [^3] Ao criar fun√ß√µes hash que mapeiam pontos similares para o mesmo bucket com alta probabilidade, o LSH permite restringir a busca apenas aos buckets relevantes, resultando em uma melhoria significativa na efici√™ncia computacional. As t√©cnicas de AND-construction e OR-construction permitem ajustar o equil√≠brio entre precis√£o e recall, adaptando o algoritmo LSH √†s necessidades espec√≠ficas de cada aplica√ß√£o. Embora o LSH introduza uma aproxima√ß√£o na busca, a sua efici√™ncia o torna uma ferramenta indispens√°vel em cen√°rios onde a exatid√£o precisa ser trocada por velocidade.

### Refer√™ncias
[^3]: Locality Sensitive Hashing (LSH) creates hash functions so that similar items are more likely to end up in the same hash bucket. By needing to only check the relevant buckets, we can perform ANN queries efficiently.
<!-- END -->