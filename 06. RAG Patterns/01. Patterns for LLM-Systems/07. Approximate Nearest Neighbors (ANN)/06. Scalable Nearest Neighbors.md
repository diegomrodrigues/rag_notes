## Scalable Nearest Neighbors (ScaNN) para Recupera√ß√£o Eficiente

### Introdu√ß√£o

Em sistemas de Neural Information Retrieval (NIR) e Retrieval-Augmented Generation (RAG) que utilizam Large Language Models (LLMs), a busca eficiente por vizinhos mais pr√≥ximos (Nearest Neighbors - NN) √© crucial para recuperar informa√ß√µes relevantes de grandes bases de dados. Algoritmos Approximate Nearest Neighbors (ANN) s√£o empregados para acelerar essa busca, sacrificando um pouco da precis√£o em favor da velocidade. Dentro do leque de algoritmos ANN, o Scalable Nearest Neighbors (ScaNN) se destaca por oferecer um excelente compromisso entre *recall* e lat√™ncia, tornando-o uma escolha atrativa para aplica√ß√µes em larga escala. Este cap√≠tulo explorar√° o funcionamento interno do ScaNN, com √™nfase em seu processo de duas etapas: quantiza√ß√£o grosseira e busca refinada.

### Conceitos Fundamentais

O ScaNN √© projetado para lidar com grandes conjuntos de dados de vetores de alta dimensionalidade, caracter√≠sticos de embeddings gerados por LLMs. A efici√™ncia do ScaNN reside em sua abordagem de duas etapas [^6]:

1.  **Quantiza√ß√£o Grosseira (Coarse Quantization):** O objetivo desta etapa √© reduzir drasticamente o espa√ßo de busca. Em vez de comparar a query com todos os vetores da base de dados, o ScaNN identifica um subconjunto promissor de vetores para uma an√°lise mais detalhada. Isso √© alcan√ßado atrav√©s da aplica√ß√£o de t√©cnicas de quantiza√ß√£o vetorial.

    A quantiza√ß√£o vetorial envolve a divis√£o do espa√ßo vetorial em *clusters* e a representa√ß√£o de cada *cluster* por um **centroide**. Cada vetor na base de dados √© ent√£o associado ao centroide mais pr√≥ximo. A busca por vizinhos mais pr√≥ximos come√ßa identificando os centroides mais pr√≥ximos da query.  Posteriormente, apenas os vetores associados a esses centroides selecionados s√£o considerados na pr√≥xima etapa. Essa abordagem reduz significativamente o n√∫mero de compara√ß√µes necess√°rias, resultando em ganhos substanciais de velocidade.

    > üí° **Exemplo Num√©rico:**
    >
    > Considere um conjunto de dados com 1000 vetores. Ap√≥s a quantiza√ß√£o grosseira, o espa√ßo √© dividido em 100 clusters (k=100). Na fase de busca, os 10 centroides mais pr√≥ximos da query s√£o selecionados (m=10). Em m√©dia, cada cluster cont√©m 10 vetores (1000 vetores / 100 clusters). Portanto, a busca refinada √© realizada em apenas 100 vetores (10 clusters * 10 vetores/cluster) em vez de 1000.
    >
    > $\text{Taxa de Redu√ß√£o} = 1 - \frac{100}{1000} = 0.9$
    >
    > Isso representa uma redu√ß√£o de 90% no espa√ßo de busca, ilustrando o potencial de otimiza√ß√£o da quantiza√ß√£o grosseira.

2.  **Busca Refinada (Fine-Grained Searching):**  Ap√≥s a etapa de quantiza√ß√£o grosseira, o ScaNN realiza uma busca mais precisa dentro do subconjunto de vetores selecionados. Esta etapa utiliza m√©todos de busca exata ou ANN mais refinados para identificar os vizinhos mais pr√≥ximos da query.

    A escolha do m√©todo de busca refinada pode variar dependendo dos requisitos espec√≠ficos da aplica√ß√£o. Algumas op√ß√µes incluem busca exaustiva (para maior precis√£o, mas menor velocidade), ou outros algoritmos ANN como Hierarchical Navigable Small World (HNSW) ou variantes otimizadas de quantiza√ß√£o de produto. O objetivo √© equilibrar a precis√£o da busca com a lat√™ncia desejada.

    > üí° **Exemplo Num√©rico:**
    >
    > Suponha que, ap√≥s a quantiza√ß√£o grosseira (como no exemplo acima), temos 100 vetores para busca refinada. Uma busca exaustiva calcularia a dist√¢ncia da query para cada um desses 100 vetores. Alternativamente, usar HNSW poderia reduzir o n√∫mero de compara√ß√µes para, digamos, 20, com uma pequena perda de precis√£o. Essa escolha impacta diretamente o *trade-off* entre *recall* e lat√™ncia.

A combina√ß√£o dessas duas etapas permite que o ScaNN alcance um excelente *recall* (a propor√ß√£o de vizinhos verdadeiros recuperados) com baixa lat√™ncia (o tempo necess√°rio para realizar a busca). A escolha dos par√¢metros de quantiza√ß√£o (n√∫mero de centroides, m√©todo de quantiza√ß√£o) e do algoritmo de busca refinada influencia diretamente esse *trade-off*, e deve ser ajustada de acordo com as caracter√≠sticas do conjunto de dados e os requisitos da aplica√ß√£o.

Para complementar a discuss√£o sobre os algoritmos de busca refinada, podemos introduzir uma alternativa que se beneficia de estruturas de grafos:

**Teorema 1** *Grafos de vizinhan√ßa aproximada podem ser utilizados na etapa de busca refinada para acelerar a identifica√ß√£o dos vizinhos mais pr√≥ximos.*

*Prova (Esbo√ßo)*: A constru√ß√£o de um grafo onde cada n√≥ representa um vetor e as arestas conectam vizinhos pr√≥ximos permite que a busca se restrinja √† vizinhan√ßa de cada n√≥. Algoritmos de busca em grafos, como a busca gulosa ou varia√ß√µes do algoritmo A*, podem ser empregados para explorar o grafo e encontrar os vizinhos mais pr√≥ximos da query de forma eficiente. A qualidade da aproxima√ß√£o depende da constru√ß√£o do grafo e da escolha dos par√¢metros do algoritmo de busca.

### An√°lise Matem√°tica da Quantiza√ß√£o Grosseira

Para entender o impacto da quantiza√ß√£o grosseira, considere um conjunto de dados $X = \{x_1, x_2, \ldots, x_N\}$ de $N$ vetores em um espa√ßo $d$-dimensional, $\mathbb{R}^d$. A quantiza√ß√£o vetorial divide este espa√ßo em $k$ *clusters*, representados por seus centroides $c_1, c_2, \ldots, c_k$.

A atribui√ß√£o de um vetor $x_i$ ao centroide $c_j$ √© geralmente baseada na dist√¢ncia Euclidiana:

$$
\text{assign}(x_i) = \arg\min_{j} ||x_i - c_j||_2
$$

O conjunto de vetores associados a cada centroide $c_j$ define um *cluster* $S_j$:

$$
S_j = \{x_i \in X \mid \text{assign}(x_i) = j\}
$$

Durante a busca, dada uma query $q \in \mathbb{R}^d$, o ScaNN primeiro identifica os $m$ centroides mais pr√≥ximos de $q$:

$$
C = \{c_{j_1}, c_{j_2}, \ldots, c_{j_m}\} = \arg\min_{j} ||q - c_j||_2
$$

onde $m < k$. Em seguida, a busca refinada √© realizada apenas nos vetores pertencentes aos *clusters* correspondentes a esses centroides:

$$
X' = \bigcup_{c_j \in C} S_j
$$

A cardinalidade de $X'$ √© significativamente menor que a de $X$, resultando em uma busca muito mais r√°pida.

Para quantificar a redu√ß√£o do espa√ßo de busca, podemos definir a taxa de redu√ß√£o como:

$$
\text{Taxa de Redu√ß√£o} = 1 - \frac{|X'|}{|X|}
$$

**Proposi√ß√£o 1** *A taxa de redu√ß√£o aumenta com a diminui√ß√£o de $m$ e com uma distribui√ß√£o mais uniforme dos vetores entre os clusters.*

*Prova*: Se $m$ diminui, menos clusters s√£o selecionados para a busca refinada, resultando em um menor $|X'|$ e, portanto, uma maior taxa de redu√ß√£o. Uma distribui√ß√£o mais uniforme significa que cada cluster cont√©m aproximadamente o mesmo n√∫mero de vetores. Se a distribui√ß√£o for muito enviesada, com alguns clusters contendo a maioria dos vetores, a sele√ß√£o de mesmo poucos desses clusters pode resultar em um $|X'|$ pr√≥ximo de $|X|$, diminuindo a taxa de redu√ß√£o.

> üí° **Exemplo Num√©rico:**
>
> Assumindo que temos $N = 10000$ vetores e dividimos em $k=256$ clusters usando k-means.
>
> Caso 1: Distribui√ß√£o uniforme. Cada cluster tem aproximadamente $10000/256 \approx 39$ vetores. Se selecionarmos $m=16$ clusters, a busca refinada ser√° feita em $16 \times 39 = 624$ vetores.
> $\text{Taxa de Redu√ß√£o} = 1 - \frac{624}{10000} = 0.9376 = 93.76\%$
>
> Caso 2: Distribui√ß√£o n√£o uniforme. Alguns clusters s√£o densos, outros esparsos. Suponha que os 16 clusters mais pr√≥ximos da query contenham, em m√©dia, 200 vetores cada (devido √† distribui√ß√£o n√£o uniforme). Ent√£o, a busca refinada ser√° feita em $16 \times 200 = 3200$ vetores.
> $\text{Taxa de Redu√ß√£o} = 1 - \frac{3200}{10000} = 0.68 = 68\%$
>
> Isso ilustra como a distribui√ß√£o dos dados afeta a taxa de redu√ß√£o. A escolha de *k* e *m* deve levar em conta essa distribui√ß√£o.

### Otimiza√ß√£o e Trade-off Recall/Lat√™ncia

A efic√°cia do ScaNN depende crucialmente da escolha do n√∫mero de centroides $k$ e do n√∫mero de centroides $m$ a serem considerados na etapa de busca refinada. Aumentar $k$ geralmente leva a uma melhor representa√ß√£o do espa√ßo vetorial, mas tamb√©m aumenta o custo computacional da etapa de quantiza√ß√£o grosseira. Aumentar $m$ melhora o *recall*, mas aumenta a lat√™ncia da busca refinada.

A otimiza√ß√£o desses par√¢metros requer uma an√°lise cuidadosa do conjunto de dados e dos requisitos de desempenho da aplica√ß√£o. T√©cnicas como *cross-validation* podem ser utilizadas para encontrar os valores √≥timos de $k$ e $m$ que maximizam o *recall* para um determinado limite de lat√™ncia. Al√©m disso, a escolha do algoritmo de quantiza√ß√£o vetorial (e.g., k-means, produto quantiza√ß√£o) e do algoritmo de busca refinada tamb√©m impacta o *trade-off* *recall*/lat√™ncia.

> üí° **Exemplo Num√©rico:**
>
> Considere um sistema RAG onde a lat√™ncia m√°xima aceit√°vel √© de 50ms. Podemos testar diferentes configura√ß√µes de ScaNN e medir o *recall* e a lat√™ncia.
>
> | Configura√ß√£o | k    | m  | Busca Refinada | Lat√™ncia (ms) | Recall |
> |--------------|------|----|----------------|----------------|--------|
> | A            | 64   | 8  | Exaustiva      | 40             | 0.80   |
> | B            | 128  | 8  | Exaustiva      | 60             | 0.85   |
> | C            | 64   | 16 | Exaustiva      | 70             | 0.88   |
> | D            | 64   | 8  | HNSW           | 30             | 0.75   |
> | E            | 128  | 8  | HNSW           | 45             | 0.82   |
>
> Neste exemplo, as configura√ß√µes A e E est√£o dentro do limite de lat√™ncia de 50ms. A configura√ß√£o E (k=128, m=8, HNSW) oferece um *recall* ligeiramente melhor (0.82) do que A (0.80) e √©, portanto, a melhor escolha dentro da restri√ß√£o de lat√™ncia. A configura√ß√£o D, apesar de ter a menor lat√™ncia, sacrifica muito o *recall*. A an√°lise deste tipo de tabela auxilia na escolha dos melhores par√¢metros.

Podemos formalizar o *trade-off* entre *recall* e lat√™ncia atrav√©s da seguinte defini√ß√£o:

**Defini√ß√£o 1** *Define-se a fronteira de Pareto Recall-Lat√™ncia como o conjunto de pares (Recall, Lat√™ncia) para os quais n√£o existe nenhuma outra configura√ß√£o de par√¢metros do ScaNN que ofere√ßa tanto um Recall maior quanto uma Lat√™ncia menor.*

A otimiza√ß√£o do ScaNN consiste em encontrar pontos pr√≥ximos √† fronteira de Pareto, representando configura√ß√µes de par√¢metros que oferecem o melhor *recall* poss√≠vel para uma dada lat√™ncia ou a menor lat√™ncia poss√≠vel para um dado *recall*.

**Teorema 1.1** *A fronteira de Pareto Recall-Lat√™ncia √© monotonicamente n√£o-crescente.*

*Prova (Esbo√ßo)*: Suponha que a fronteira de Pareto n√£o seja monotonicamente n√£o-crescente. Ent√£o, existiriam dois pontos (Recall1, Lat√™ncia1) e (Recall2, Lat√™ncia2) na fronteira tais que Recall2 > Recall1 e Lat√™ncia2 < Lat√™ncia1. Isso significaria que a configura√ß√£o associada a (Recall2, Lat√™ncia2) √© estritamente melhor que a configura√ß√£o associada a (Recall1, Lat√™ncia1), contradizendo a defini√ß√£o da fronteira de Pareto. Portanto, a fronteira deve ser monotonicamente n√£o-crescente.

### Conclus√£o

O Scalable Nearest Neighbors (ScaNN) oferece uma solu√ß√£o poderosa e eficiente para busca aproximada de vizinhos mais pr√≥ximos em grandes conjuntos de dados. Sua abordagem de duas etapas, com quantiza√ß√£o grosseira seguida de busca refinada, permite alcan√ßar um excelente compromisso entre *recall* e lat√™ncia. A escolha cuidadosa dos par√¢metros de quantiza√ß√£o e do algoritmo de busca refinada √© fundamental para otimizar o desempenho do ScaNN para uma aplica√ß√£o espec√≠fica. Ao entender os princ√≠pios fundamentais do ScaNN e os fatores que influenciam seu desempenho, os engenheiros e pesquisadores podem aproveitar ao m√°ximo essa ferramenta para construir sistemas de NIR e RAG altamente eficientes.

### Refer√™ncias
[^6]: Detalhes sobre o processo de duas etapas do ScaNN.
<!-- END -->