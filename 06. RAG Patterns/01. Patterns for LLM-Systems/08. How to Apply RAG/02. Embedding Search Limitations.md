## LimitacÃßoÃÉes da Busca Exclusiva por Embeddings e a Complementaridade da Busca por Palavras-Chave

### Introdu√ß√£o
A busca baseada exclusivamente em embeddings tem se mostrado eficaz em diversos cen√°rios de Recupera√ß√£o de Informa√ß√£o Neural (NIR) e RAG (Retrieval-Augmented Generation) com LLMs. No entanto, existem situa√ß√µes espec√≠ficas onde essa abordagem apresenta limita√ß√µes significativas [^1]. Este cap√≠tulo explora essas limita√ß√µes e demonstra como a combina√ß√£o da busca sem√¢ntica (embedding-based search) com a busca por palavras-chave pode complementar e aprimorar os resultados da recupera√ß√£o.

### Conceitos Fundamentais

#### Limita√ß√µes da Busca Exclusiva por Embeddings
A busca por embeddings, apesar de sua capacidade de capturar nuances sem√¢nticas e rela√ß√µes complexas entre os termos, pode falhar em cen√°rios espec√≠ficos [^1]. Exemplos t√≠picos incluem:

*   **Busca por nomes pr√≥prios:** A busca por nomes de pessoas (e.g., Eugene) ou objetos (e.g., Kaptir 2.0) pode ser problem√°tica se esses nomes n√£o estiverem bem representados no espa√ßo de embeddings ou se houver varia√ß√µes na forma como s√£o mencionados [^1].
*   **Busca por acr√¥nimos e frases:** Acr√¥nimos (e.g., RAG, RLHF) e frases espec√≠ficas podem n√£o ter representa√ß√µes de embeddings robustas, especialmente se forem raros ou novos [^1].
*   **Busca por identificadores (IDs):** A busca por IDs (e.g., gpt-3.5-turbo, titan-xlarge-v1.01) depende da presen√ßa exata desses identificadores nos documentos indexados e em suas representa√ß√µes de embeddings. Varia√ß√µes m√≠nimas ou erros de digita√ß√£o podem levar a falhas na recupera√ß√£o [^1].

> üí° **Exemplo Num√©rico:** Imagine uma busca por "titan-xlarge-v1.01". Se o documento contiver apenas "titan-xlarge" ou "titan xlarge v1.01", a busca por embeddings pode falhar em identificar o documento como relevante, mesmo que semanticamente pr√≥ximo. Uma busca por palavras-chave com correspond√™ncia exata teria maior probabilidade de sucesso se o termo exato estivesse presente.

Para complementar a discuss√£o sobre as limita√ß√µes da busca por embeddings, podemos analisar como a dimensionalidade do espa√ßo de embeddings afeta o desempenho.

**Teorema 1** (Maldi√ß√£o da Dimensionalidade em Embeddings): Em espa√ßos de embeddings de alta dimensionalidade, a dist√¢ncia entre vetores tende a se tornar mais uniforme, dificultando a distin√ß√£o entre documentos relevantes e irrelevantes.

*Prova (Esbo√ßo):* Em espa√ßos de alta dimensionalidade, a concentra√ß√£o de medidas faz com que a maioria dos pontos esteja localizada na "casca" de uma esfera. A dist√¢ncia entre um ponto fixo e seus vizinhos mais pr√≥ximos e mais distantes converge √† medida que a dimensionalidade aumenta. Isso torna a similaridade por cosseno menos eficaz, pois a diferen√ßa entre os cossenos dos √¢ngulos entre vetores relevantes e irrelevantes diminui.

> üí° **Exemplo Num√©rico:** Suponha que temos embeddings em um espa√ßo de 2 dimens√µes e outro de 1000 dimens√µes. Em 2D, os vetores [0.1, 0.2] e [0.8, 0.9] s√£o claramente distintos. Em 1000D, mesmo que os vetores sejam [0.1, 0.2, ..., 0.1] e [0.8, 0.9, ..., 0.8], a dist√¢ncia euclidiana entre eles pode ser similar a dist√¢ncia de outros vetores aleat√≥rios, tornando a distin√ß√£o mais dif√≠cil. Isso significa que a dimensionalidade precisa ser cuidadosamente escolhida e ajustada para o dataset.

#### Busca por Palavras-Chave: Uma Abordagem Complementar
A busca por palavras-chave, em contraste com a busca por embeddings, se baseia na an√°lise da frequ√™ncia de palavras e na presen√ßa de termos espec√≠ficos nos documentos [^1]. Embora essa abordagem seja mais simples e n√£o capture a sem√¢ntica subjacente, ela oferece vantagens em certos cen√°rios:

*   **Precis√£o na correspond√™ncia exata:** A busca por palavras-chave √© eficaz na identifica√ß√£o de documentos que cont√™m os termos de busca exatos, o que √© crucial para a busca por nomes pr√≥prios, acr√¥nimos e IDs [^1].
*   **Robustez a varia√ß√µes ortogr√°ficas:** Embora n√£o seja inerente, t√©cnicas de stemming, lemmatization, e fuzzy matching podem ser incorporadas na busca por palavras-chave para mitigar problemas relacionados a varia√ß√µes ortogr√°ficas e erros de digita√ß√£o.

Para formalizar a ideia de robustez a varia√ß√µes ortogr√°ficas, podemos definir uma m√©trica de similaridade textual.

**Defini√ß√£o 1** (Similaridade de Levenshtein): A similaridade de Levenshtein entre duas strings $s_1$ e $s_2$ √© definida como o n√∫mero m√≠nimo de edi√ß√µes (inser√ß√µes, remo√ß√µes ou substitui√ß√µes) necess√°rias para transformar $s_1$ em $s_2$. A similaridade normalizada √© dada por $1 - \frac{\text{Levenshtein}(s_1, s_2)}{\max(|s_1|, |s_2|)}$.

Com essa defini√ß√£o, podemos estabelecer um limiar m√≠nimo de similaridade para considerar uma palavra-chave como correspondente a um termo na consulta, mesmo que haja pequenas diferen√ßas ortogr√°ficas.

> üí° **Exemplo Num√©rico:** Consideremos as strings "Kaptir 2.0" e "Kaptir2.0". A dist√¢ncia de Levenshtein entre elas √© 1 (a remo√ß√£o do espa√ßo). A similaridade normalizada √© $1 - \frac{1}{10} = 0.9$. Se definirmos um limiar de similaridade de 0.85, considerar√≠amos essas duas strings como correspondentes.

> üí° **Exemplo Num√©rico:** **TF-IDF Calculation**
>
> Let's consider a corpus of two documents:
>
> *   Document 1: "This is a story about dogs. Dogs are cute."
> *   Document 2: "This story is about cats. Cats are not dogs."
>
> And the query: "dogs cats".
>
> **Step 1: Calculate Term Frequency (TF)**
>
> | Term    | Document 1 (TF) | Document 2 (TF) |
> | ------- | --------------- | --------------- |
> | dogs    | 2/8 = 0.25      | 1/9 ‚âà 0.11      |
> | cats    | 0/8 = 0         | 2/9 ‚âà 0.22      |
> | story   | 1/8 ‚âà 0.125     | 1/9 ‚âà 0.11      |
> | is      | 1/8 ‚âà 0.125     | 1/9 ‚âà 0.11      |
> | about   | 1/8 ‚âà 0.125     | 1/9 ‚âà 0.11      |
> | are     | 1/8 ‚âà 0.125     | 1/9 ‚âà 0.11      |
> | cute    | 1/8 ‚âà 0.125     | 0/9 = 0         |
> | not     | 0/8 = 0         | 1/9 ‚âà 0.11      |
>
> **Step 2: Calculate Inverse Document Frequency (IDF)**
>
> First, calculate Document Frequency (DF):
>
> | Term    | Document Frequency (DF) |
> | ------- | ----------------------- |
> | dogs    | 2                       |
> | cats    | 1                       |
> | story   | 2                       |
> | is      | 2                       |
> | about   | 2                       |
> | are     | 2                       |
> | cute    | 1                       |
> | not     | 1                       |
>
> Then, calculate IDF using the formula: $\text{IDF}(t) = \log \frac{N}{DF(t)}$, where $N$ is the total number of documents (2 in this case).
>
> | Term    | IDF            |
> | ------- | -------------- |
> | dogs    | log(2/2) = 0   |
> | cats    | log(2/1) ‚âà 0.3 |
> | story   | log(2/2) = 0   |
> | is      | log(2/2) = 0   |
> | about   | log(2/2) = 0   |
> | are     | log(2/2) = 0   |
> | cute    | log(2/1) ‚âà 0.3 |
> | not     | log(2/1) ‚âà 0.3 |
>
> **Step 3: Calculate TF-IDF**
>
> Multiply TF by IDF for each term in each document:
>
> | Term    | Document 1 (TF-IDF) | Document 2 (TF-IDF) |
> | ------- | ------------------- | ------------------- |
> | dogs    | 0.25 * 0 = 0        | 0.11 * 0 = 0        |
> | cats    | 0 * 0.3 = 0         | 0.22 * 0.3 ‚âà 0.066  |
> | story   | 0.125 * 0 = 0       | 0.11 * 0 = 0        |
> | is      | 0.125 * 0 = 0       | 0.11 * 0 = 0        |
> | about   | 0.125 * 0 = 0       | 0.11 * 0 = 0        |
> | are     | 0.125 * 0 = 0       | 0.11 * 0 = 0        |
> | cute    | 0.125 * 0.3 ‚âà 0.0375| 0 * 0.3 = 0         |
> | not     | 0 * 0.3 = 0         | 0.11 * 0.3 ‚âà 0.033  |
>
> **Step 4: Query TF-IDF**
>
> | Term    | Query (TF-IDF) |
> | ------- | -------------- |
> | dogs    | 0.7           |
> | cats    | 0.7           |
>
> **Step 5: Calculate Cosine Similarity**
>
> We only consider "dogs" and "cats"
>
> Document 1 Vector: \[0, 0]
>
> Document 2 Vector: \[0, 0.066]
>
> Query vector: \[0.7, 0.7]
>
> Cosine similarity between Query and Document 1: 0
>
> Cosine similarity between Query and Document 2: $\frac{0.7 * 0 + 0.7*0.066}{\sqrt{0.7^2+0.7^2} * \sqrt{0^2+0.066^2}} = \frac{0.0462}{0.99 * 0.066} = 0.7045$
>
> **Interpretation:** Document 2 is ranked higher than Document 1, which aligns with our intuition since Document 2 contains both "dogs" and "cats".

#### A Complementaridade entre Busca Sem√¢ntica e Busca por Palavras-Chave
A principal limita√ß√£o da busca por palavras-chave √© sua incapacidade de lidar com sin√¥nimos e hiper√¥nimos [^1]. Um modelo de busca baseado somente em frequ√™ncias de palavras n√£o captura informa√ß√µes sem√¢nticas ou de correla√ß√£o. Por exemplo, uma busca por "carro" pode n√£o retornar documentos que mencionam "autom√≥vel", mesmo que os dois termos se refiram ao mesmo conceito. Similarmente, a busca por "animal de estima√ß√£o" pode n√£o retornar documentos que mencionam "gato" ou "cachorro" [^1].

> üí° **Exemplo Num√©rico:** Considere uma consulta "melhor carro el√©trico". A busca por palavras-chave pode n√£o retornar documentos que mencionam "autom√≥vel el√©trico de alta performance" porque as palavras n√£o correspondem exatamente. A busca sem√¢ntica, por outro lado, pode identificar esses documentos como relevantes devido √† similaridade sem√¢ntica.

A busca sem√¢ntica, por outro lado, √© capaz de superar essas limita√ß√µes ao capturar a sem√¢ntica subjacente aos termos de busca e dos documentos. No entanto, como discutido anteriormente, ela pode falhar em cen√°rios que exigem correspond√™ncia exata [^1].

A combina√ß√£o estrat√©gica das duas abordagens permite mitigar as limita√ß√µes individuais e aproveitar as vantagens de cada uma. Por exemplo, √© poss√≠vel utilizar a busca por palavras-chave para identificar documentos que contenham os termos de busca exatos (e.g., nomes pr√≥prios, acr√¥nimos, IDs) e, em seguida, utilizar a busca sem√¢ntica para refinar os resultados, priorizando documentos que sejam semanticamente relevantes para a consulta [^1].

Podemos formalizar essa combina√ß√£o com uma fun√ß√£o de pontua√ß√£o h√≠brida.

**Defini√ß√£o 2** (Fun√ß√£o de Pontua√ß√£o H√≠brida): Seja $S(q, d)$ a pontua√ß√£o de similaridade sem√¢ntica entre a consulta $q$ e o documento $d$, e seja $K(q, d)$ a pontua√ß√£o de similaridade por palavras-chave entre $q$ e $d$. Uma fun√ß√£o de pontua√ß√£o h√≠brida pode ser definida como:

$H(q, d) = \alpha S(q, d) + (1 - \alpha) K(q, d)$

onde $\alpha \in [0, 1]$ √© um par√¢metro que controla o peso relativo da busca sem√¢ntica e da busca por palavras-chave. A otimiza√ß√£o de $\alpha$ pode ser realizada atrav√©s de t√©cnicas de aprendizado supervisionado ou por ajuste manual baseado em dados de avalia√ß√£o.

> üí° **Exemplo Num√©rico:** Suponha que, para uma consulta espec√≠fica, a busca sem√¢ntica atribui uma pontua√ß√£o de 0.8 a um documento e a busca por palavras-chave atribui uma pontua√ß√£o de 0.6. Se definirmos $\alpha = 0.7$, a pontua√ß√£o h√≠brida seria $H(q, d) = 0.7 \times 0.8 + 0.3 \times 0.6 = 0.56 + 0.18 = 0.74$. Ao ajustar $\alpha$, podemos priorizar a busca sem√¢ntica (aumentando $\alpha$) ou a busca por palavras-chave (diminuindo $\alpha$), dependendo do tipo de consulta e das caracter√≠sticas dos documentos.

> üí° **Exemplo Num√©rico:**
>
> Consider a scenario where we have the following:
>
> *   **Query:** "RLHF explanation"
> *   **Document 1:** "Reinforcement Learning from Human Feedback (RLHF) is a technique..." (Semantic Score: 0.7, Keyword Score: 0.9)
> *   **Document 2:** "This document discusses various Reinforcement Learning methods." (Semantic Score: 0.8, Keyword Score: 0.2)
>
> Let's use $\alpha = 0.6$.
>
> *   Hybrid Score (Document 1): $0.6 * 0.7 + 0.4 * 0.9 = 0.42 + 0.36 = 0.78$
> *   Hybrid Score (Document 2): $0.6 * 0.8 + 0.4 * 0.2 = 0.48 + 0.08 = 0.56$
>
> In this case, Document 1 would be ranked higher using the hybrid approach, correctly prioritizing the document that contains the full acronym and its explanation.

### Conclus√£o
A busca exclusiva por embeddings √© uma ferramenta poderosa para NIR e RAG, mas possui limita√ß√µes inerentes [^1]. A busca por palavras-chave, apesar de sua simplicidade, oferece vantagens complementares em cen√°rios espec√≠ficos. A combina√ß√£o inteligente das duas abordagens √© fundamental para construir sistemas de recupera√ß√£o de informa√ß√£o mais robustos e eficazes, capazes de lidar com uma ampla gama de consultas e cen√°rios de uso [^1].

### Refer√™ncias
[^1]: Informa√ß√£o extra√≠da do contexto fornecido.
<!-- END -->