## Embeddings Cl√°ssicos para RAG: Word2vec e fastText

### Introdu√ß√£o
No contexto de Retrieval-Augmented Generation (RAG) com Large Language Models (LLMs), a escolha de um modelo de *embedding* adequado √© crucial para a efic√°cia do processo de recupera√ß√£o. Os modelos de *embedding* cl√°ssicos, como Word2vec e fastText, representam uma op√ß√£o interessante, especialmente para prototipagem r√°pida e cen√°rios com recursos computacionais limitados. Este cap√≠tulo se concentrar√° no fastText, detalhando suas caracter√≠sticas, vantagens e aplicabilidade no contexto de RAG.

### Conceitos Fundamentais

#### Word2vec e fastText: Uma Breve Compara√ß√£o
Word2vec foi um dos primeiros modelos de *embedding* a ganhar destaque devido √† sua capacidade de capturar rela√ß√µes sem√¢nticas entre palavras [^4]. No entanto, Word2vec tem limita√ß√µes ao lidar com palavras raras ou fora do vocabul√°rio (OOV). √â neste ponto que fastText se destaca.

#### fastText: Uma Abordagem Baseada em Subpalavras
fastText √© uma biblioteca *open-source* leve que permite aos usu√°rios aproveitar *embeddings* pr√©-treinados ou treinar novos modelos de *embedding* [^4]. A principal diferen√ßa entre fastText e Word2vec reside na sua abordagem para representar palavras. Enquanto Word2vec trata cada palavra como uma unidade at√¥mica, fastText decomp√µe as palavras em *n-grams* de caracteres.

Por exemplo, a palavra "comer" poderia ser representada pelos *n-grams* "co", "com", "ome", "mer", "er". Essa decomposi√ß√£o permite que o fastText capture informa√ß√µes morfol√≥gicas e lide melhor com palavras raras e OOV. Ao encontrar uma palavra desconhecida, o fastText pode gerar um *embedding* combinando os *embeddings* de seus *n-grams* constituintes.

> üí° **Exemplo Num√©rico:** Considere a palavra "computador" e suponha que estamos usando *n-grams* de tamanho 3. O fastText dividiria a palavra em: "com", "omp", "mpu", "put", "uta", "tad", "ado", "dor".  Se o modelo fastText j√° tiver visto esses *n-grams* durante o treinamento, ele ter√° *embeddings* para cada um deles. O *embedding* final para "computador" seria a soma desses *embeddings* de *n-grams*.
Para complementar essa descri√ß√£o, podemos formalizar a representa√ß√£o de uma palavra no fastText.

**Defini√ß√£o:** Seja $w$ uma palavra e $G_w$ o conjunto de seus *n-grams*. O *embedding* da palavra $w$ no fastText, denotado por $v_w$, √© dado por:

$$v_w = \sum_{g \in G_w} v_g$$

onde $v_g$ √© o *embedding* do *n-gram* $g$.

#### Vantagens do fastText para RAG
1.  **Suporte a M√∫ltiplas L√≠nguas:** fastText vem com *embeddings* pr√©-treinados para 157 l√≠nguas [^4], facilitando a implementa√ß√£o de RAG em cen√°rios multilingues.
2.  **Velocidade:** fastText √© extremamente r√°pido, mesmo sem GPU [^4]. Isso o torna uma excelente op√ß√£o para prototipagem r√°pida e para aplica√ß√µes onde a lat√™ncia √© uma preocupa√ß√£o.
3.  **Robustez a Palavras Raras:** A capacidade de lidar com palavras raras e OOV √© uma grande vantagem em cen√°rios de RAG, onde o vocabul√°rio dos documentos pode ser vasto e variado.
4.  **Leveza:** Por ser uma biblioteca leve, fastText pode ser facilmente integrado em diferentes plataformas e ambientes, minimizando a sobrecarga computacional.

**Teorema 1:** A decomposi√ß√£o de palavras em *n-grams* no fastText permite uma melhor generaliza√ß√£o para palavras OOV em compara√ß√£o com Word2vec, desde que os *n-grams* da palavra OOV tenham sido observados durante o treinamento.

*Prova (Esbo√ßo):* Word2vec atribui um vetor aleat√≥rio a palavras OOV, sem qualquer rela√ß√£o com outras palavras no vocabul√°rio. fastText, por outro lado, decomp√µe a palavra OOV em seus *n-grams*. Se esses *n-grams* foram vistos durante o treinamento, seus *embeddings* estar√£o dispon√≠veis, permitindo que fastText construa um *embedding* razo√°vel para a palavra OOV atrav√©s da soma dos *embeddings* dos *n-grams*. A qualidade desse *embedding* depender√° da frequ√™ncia e da relev√¢ncia dos *n-grams* na l√≠ngua.

#### Utilizando fastText em um Pipeline RAG

A utiliza√ß√£o do fastText em um pipeline RAG envolve os seguintes passos:

1.  **Indexa√ß√£o:** Os documentos da base de conhecimento s√£o processados e seus *embeddings* s√£o gerados utilizando o fastText. Esses *embeddings* s√£o ent√£o armazenados em um √≠ndice de vetores, como FAISS ou Annoy, para busca eficiente.
2.  **Consulta:** Quando um usu√°rio faz uma pergunta, ela √© convertida em um *embedding* utilizando o mesmo modelo fastText usado na indexa√ß√£o.
3.  **Recupera√ß√£o:** O *embedding* da consulta √© usado para buscar os documentos mais relevantes no √≠ndice de vetores. A similaridade entre o *embedding* da consulta e os *embeddings* dos documentos √© geralmente medida usando a similaridade do coseno.
4.  **Gera√ß√£o:** Os documentos recuperados s√£o combinados com a pergunta original e alimentados em um LLM para gerar a resposta final.

> üí° **Exemplo Num√©rico:** Suponha que temos os seguintes *embeddings* (simplificados para 2 dimens√µes para facilitar a visualiza√ß√£o) para uma consulta e dois documentos:
>
> *   Consulta: $q = [0.8, 0.6]$
> *   Documento 1: $d_1 = [0.7, 0.7]$
> *   Documento 2: $d_2 = [-0.9, 0.1]$
>
> A similaridade do coseno √© calculada como:
>
> $$\text{Cosine Similarity}(q, d) = \frac{q \cdot d}{||q|| \cdot ||d||}$$
>
> Para o Documento 1:
>
> $q \cdot d_1 = (0.8 * 0.7) + (0.6 * 0.7) = 0.56 + 0.42 = 0.98$
>
> $||q|| = \sqrt{0.8^2 + 0.6^2} = \sqrt{0.64 + 0.36} = \sqrt{1} = 1$
>
> $||d_1|| = \sqrt{0.7^2 + 0.7^2} = \sqrt{0.49 + 0.49} = \sqrt{0.98} \approx 0.99$
>
> $$\text{Cosine Similarity}(q, d_1) = \frac{0.98}{1 * 0.99} \approx 0.99$$
>
> Para o Documento 2:
>
> $q \cdot d_2 = (0.8 * -0.9) + (0.6 * 0.1) = -0.72 + 0.06 = -0.66$
>
> $||d_2|| = \sqrt{(-0.9)^2 + 0.1^2} = \sqrt{0.81 + 0.01} = \sqrt{0.82} \approx 0.90$
>
> $$\text{Cosine Similarity}(q, d_2) = \frac{-0.66}{1 * 0.90} \approx -0.73$$
>
> Neste caso, o Documento 1 seria considerado mais relevante que o Documento 2 porque tem uma similaridade do coseno muito maior (0.99 vs -0.73). A similaridade do coseno varia de -1 a 1, onde 1 significa vetores id√™nticos, 0 significa vetores ortogonais (n√£o relacionados) e -1 significa vetores opostos.
>
> **Interpreta√ß√£o:**  A similaridade do coseno quantifica o √¢ngulo entre os vetores. Vetores apontando na mesma dire√ß√£o t√™m uma similaridade pr√≥xima de 1, enquanto vetores ortogonais (√¢ngulo de 90 graus) t√™m similaridade 0. Valores negativos indicam que os vetores apontam em dire√ß√µes opostas.

**Proposi√ß√£o 1:** O uso da similaridade do coseno para medir a relev√¢ncia entre *embeddings* no passo de recupera√ß√£o √© eficaz para capturar rela√ß√µes sem√¢nticas, mas pode ser complementado com outras m√©tricas de similaridade para melhorar a precis√£o.

*Exemplos de m√©tricas alternativas incluem a dist√¢ncia euclidiana normalizada, que pode ser mais sens√≠vel a diferen√ßas de magnitude entre os vetores, e a similaridade de Jaccard, que pode ser √∫til quando os *embeddings* s√£o esparsos.*

#### Exemplo Simplificado (Pseudo-c√≥digo)

```python
# Supondo que temos um modelo fastText carregado e um √≠ndice de vetores

def rag_fasttext(query, fasttext_model, vector_index, top_k=5):
  """
  Executa RAG utilizando fastText para embeddings.

  Args:
    query: A pergunta do usu√°rio.
    fasttext_model: O modelo fastText carregado.
    vector_index: O √≠ndice de vetores contendo os embeddings dos documentos.
    top_k: O n√∫mero de documentos a serem recuperados.

  Returns:
    Uma string contendo a resposta gerada pelo LLM.
  """

  query_embedding = fasttext_model.get_sentence_vector(query)
  relevant_docs = vector_index.search(query_embedding, top_k)
  context = " ".join([doc.text for doc in relevant_docs])

  # Integrar com LLM (exemplo simplificado)
  prompt = f"Context: {context}\nQuestion: {query}\nAnswer:"
  answer = LLM(prompt) # Supondo que LLM √© uma fun√ß√£o que interage com um LLM
  return answer
```

Para otimizar ainda mais o processo de recupera√ß√£o, podemos considerar a pondera√ß√£o dos *n-grams* no fastText.

**Teorema 2:** Atribuir pesos diferentes aos *n-grams* com base em sua frequ√™ncia ou import√¢ncia sem√¢ntica pode melhorar a qualidade dos *embeddings* gerados pelo fastText e, consequentemente, o desempenho do pipeline RAG.

*Prova (Esbo√ßo):* Nem todos os *n-grams* contribuem igualmente para o significado de uma palavra. *N-grams* mais frequentes podem ser menos informativos do que *n-grams* menos comuns, e alguns *n-grams* podem ter maior relev√¢ncia sem√¢ntica. Ao ponderar os *n-grams* de acordo com sua frequ√™ncia inversa (TF-IDF) ou outras medidas de import√¢ncia, podemos dar mais peso aos *n-grams* mais relevantes, resultando em *embeddings* mais precisos e uma melhor recupera√ß√£o de documentos relevantes. A escolha da fun√ß√£o de pondera√ß√£o ideal depender√° das caracter√≠sticas do corpus e da tarefa espec√≠fica.

> üí° **Exemplo Num√©rico:**  Suponha que estamos ponderando os *n-grams* usando TF-IDF. Temos a palavra "banana" e os seguintes *n-grams* (tamanho 2): "ba", "an", "na", "an", "na". Observe a repeti√ß√£o de "an" e "na".
>
> Digamos que, ap√≥s calcular o TF-IDF para cada *n-gram* em um corpus, obtivemos os seguintes valores:
>
> *   TF-IDF("ba") = 0.8
> *   TF-IDF("an") = 0.2
> *   TF-IDF("na") = 0.3
>
> Para calcular o *embedding* ponderado da palavra "banana", multiplicamos cada *embedding* do *n-gram* pelo seu respectivo valor de TF-IDF e somamos:
>
> $$v_{\text{banana}} = 0.8 * v_{\text{"ba"}} + 0.2 * v_{\text{"an"}} + 0.3 * v_{\text{"na"}} +  0.2 * v_{\text{"an"}} + 0.3 * v_{\text{"na"}}$$
>
> $$v_{\text{banana}} = 0.8 * v_{\text{"ba"}} + 0.4 * v_{\text{"an"}} + 0.6 * v_{\text{"na"}}$$
>
> Aqui, $v_{\text{"ba"}}, v_{\text{"an"}}, v_{\text{"na"}}$ s√£o os *embeddings* originais dos *n-grams* obtidos do modelo fastText. Este c√°lculo d√° mais import√¢ncia ao *n-gram* "ba" porque ele √© considerado mais distintivo no corpus (TF-IDF alto).

### Conclus√£o
fastText oferece uma solu√ß√£o pr√°tica e eficiente para a gera√ß√£o de *embeddings* em pipelines RAG, especialmente em cen√°rios com recursos limitados e requisitos de velocidade. Sua capacidade de lidar com m√∫ltiplas l√≠nguas e palavras raras o torna uma ferramenta valiosa para construir sistemas RAG robustos e adapt√°veis. Apesar de modelos mais avan√ßados como transformers terem ganho popularidade, fastText continua sendo uma op√ß√£o vi√°vel para prototipagem r√°pida e para aplica√ß√µes espec√≠ficas onde a complexidade e o custo computacional s√£o fatores cr√≠ticos. A escolha entre fastText e modelos mais complexos deve ser baseada em uma an√°lise cuidadosa dos requisitos da aplica√ß√£o e dos recursos dispon√≠veis.
### Refer√™ncias
[^4]: fastText is a lightweight open-source library that allows users to leverage pre-trained embeddings or train new embedding models. It comes with pre-trained embeddings for 157 languages and is extremely fast, even without a GPU. It's a good go-to for early-stage proof of concept.
<!-- END -->