## Caching em Retrieval-Augmented Generation: ReduÃ§Ã£o de LatÃªncia e Custo

### IntroduÃ§Ã£o
O uso de **caching** Ã© uma tÃ©cnica fundamental para otimizar sistemas de *Retrieval-Augmented Generation* (RAG) que utilizam *Large Language Models* (LLMs). Em essÃªncia, o caching visa armazenar dados previamente computados ou recuperados, permitindo que solicitaÃ§Ãµes futuras pelos mesmos dados sejam atendidas de forma mais rÃ¡pida e eficiente [^1]. Este capÃ­tulo explorarÃ¡ os princÃ­pios do caching, suas aplicaÃ§Ãµes especÃ­ficas em sistemas RAG e os benefÃ­cios que ele oferece em termos de reduÃ§Ã£o de latÃªncia e custo.

### Conceitos Fundamentais
O **caching** Ã© uma estratÃ©gia amplamente utilizada em diversas Ã¡reas da computaÃ§Ã£o, desde sistemas de hardware atÃ© aplicaÃ§Ãµes de software complexas. O princÃ­pio bÃ¡sico Ã© simples: em vez de recalcular ou recuperar dados repetidamente, armazene os resultados em um local de acesso rÃ¡pido (o *cache*) e, quando a mesma solicitaÃ§Ã£o for feita novamente, sirva os dados diretamente do cache.

Em sistemas RAG, o caching pode ser aplicado em vÃ¡rias etapas do processo, incluindo:

*   **Caching de Resultados de RecuperaÃ§Ã£o:** A fase de *retrieval* em RAG envolve a busca por documentos relevantes em um Ã­ndice de conhecimento. Se a mesma consulta for feita repetidamente, os resultados da busca podem ser armazenados em cache. Isso evita a necessidade de realizar a busca novamente, reduzindo a latÃªncia e o custo computacional.
*   **Caching de Embeddings:** O processo de transformar texto em representaÃ§Ãµes vetoriais (embeddings) Ã© computacionalmente intensivo. Se os embeddings para determinados trechos de texto jÃ¡ foram calculados, eles podem ser armazenados em cache para uso futuro.
*   **Caching de Respostas Geradas:** As respostas geradas pelos LLMs tambÃ©m podem ser armazenadas em cache. Se a mesma pergunta for feita repetidamente, a resposta armazenada em cache pode ser retornada instantaneamente, sem a necessidade de consultar o LLM novamente.

> ğŸ’¡ **Exemplo NumÃ©rico:** Imagine um sistema RAG que responde perguntas sobre documentos de texto. Se a pergunta "Qual a capital da FranÃ§a?" Ã© feita, o sistema consulta os documentos, gera a resposta "Paris" e armazena a pergunta e a resposta no cache. Se a mesma pergunta for feita novamente, o sistema retorna "Paris" diretamente do cache, sem precisar consultar os documentos ou o LLM. Isso economiza tempo e recursos computacionais.

**BenefÃ­cios do Caching:**

*   **ReduÃ§Ã£o de LatÃªncia:** O principal benefÃ­cio do caching Ã© a reduÃ§Ã£o da latÃªncia. Servir dados do cache Ã© significativamente mais rÃ¡pido do que recalcular ou recuperar os dados originais. Isso Ã© especialmente importante em aplicaÃ§Ãµes interativas, onde os usuÃ¡rios esperam respostas rÃ¡pidas.
*   **ReduÃ§Ã£o de Custo:** A reduÃ§Ã£o da latÃªncia tambÃ©m leva Ã  reduÃ§Ã£o do custo. Ao evitar a necessidade de realizar cÃ¡lculos ou buscas repetidas, o caching reduz o uso de recursos computacionais, como CPU, memÃ³ria e largura de banda. Em sistemas baseados em nuvem, isso pode se traduzir em economias significativas.
*   **Escalabilidade:** O caching pode melhorar a escalabilidade de sistemas RAG. Ao reduzir a carga nos componentes mais lentos do sistema, o caching permite que o sistema suporte um nÃºmero maior de usuÃ¡rios e solicitaÃ§Ãµes.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que uma consulta ao LLM custe \$0.01 e leve 1 segundo. Se o caching reduzir o nÃºmero de consultas ao LLM em 50%, o custo total serÃ¡ reduzido em 50%, e a latÃªncia mÃ©dia serÃ¡ reduzida. Se 1000 consultas sÃ£o feitas, o custo original seria \$10 e 1000 segundos. Com caching de 50%, o custo seria \$5 e a latÃªncia *efetiva* cairia.

**EstratÃ©gias de Caching:**

Existem diversas estratÃ©gias de caching que podem ser utilizadas em sistemas RAG. Algumas das mais comuns incluem:

*   **Cache baseado em chave:** A estratÃ©gia mais simples Ã© usar a consulta (ou um hash da consulta) como a chave do cache. Quando uma consulta Ã© feita, o sistema verifica se a chave correspondente existe no cache. Se existir, os dados armazenados sÃ£o retornados. Caso contrÃ¡rio, os dados sÃ£o calculados ou recuperados, armazenados no cache e, em seguida, retornados.
*   **Cache baseado em tempo:** Os dados armazenados no cache podem ser invalidados apÃ³s um determinado perÃ­odo de tempo. Isso garante que o cache nÃ£o fique obsoleto e que os usuÃ¡rios recebam informaÃ§Ãµes atualizadas.
*   **Cache baseado em frequÃªncia:** Os dados que sÃ£o acessados com mais frequÃªncia sÃ£o mantidos no cache por mais tempo. Isso garante que os dados mais relevantes estejam sempre disponÃ­veis de forma rÃ¡pida.
*   **Cache distribuÃ­do:** Em sistemas RAG de grande escala, o cache pode ser distribuÃ­do em vÃ¡rios servidores. Isso permite que o sistema lide com um nÃºmero maior de solicitaÃ§Ãµes e garante que o cache esteja sempre disponÃ­vel, mesmo se um servidor falhar.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um cache baseado em tempo com um tempo de vida (TTL) de 1 hora. Se uma pergunta for feita Ã s 9:00, a resposta serÃ¡ armazenada no cache atÃ© as 10:00. Se a mesma pergunta for feita novamente Ã s 9:30, a resposta serÃ¡ retornada do cache. Se for feita novamente Ã s 10:30, a resposta terÃ¡ que ser recalculada, pois o TTL expirou.

**Lema 1:** *Cache baseado em frequÃªncia com limiar.* Uma estratÃ©gia de cache baseada em frequÃªncia pode ser aprimorada com um limiar mÃ­nimo de acessos. Dados sÃ³ sÃ£o adicionados ao cache se sua frequÃªncia de acesso exceder este limiar.

*Proof:* Esta estratÃ©gia combina os benefÃ­cios do cache baseado em frequÃªncia, evitando que dados raramente acessados ocupem espaÃ§o no cache. Ao exigir um nÃºmero mÃ­nimo de acessos antes de adicionar um item ao cache, asseguramos que apenas dados com alta probabilidade de serem reutilizados sejam armazenados.

> ğŸ’¡ **Exemplo NumÃ©rico:** Imagine que um sistema registra a frequÃªncia de acesso a diferentes consultas. Definimos um limiar de 5 acessos. Se uma consulta for feita 3 vezes em um dia, ela nÃ£o serÃ¡ adicionada ao cache. Se outra consulta for feita 7 vezes no mesmo dia, ela serÃ¡ adicionada ao cache.

**ImplementaÃ§Ã£o do Caching:**

A implementaÃ§Ã£o do caching pode ser feita de diversas formas, dependendo da arquitetura do sistema RAG e dos requisitos especÃ­ficos da aplicaÃ§Ã£o. Algumas opÃ§Ãµes comuns incluem:

*   **Caches em memÃ³ria:** O cache Ã© armazenado na memÃ³ria do servidor. Essa opÃ§Ã£o Ã© a mais rÃ¡pida, mas tambÃ©m a mais cara, pois a memÃ³ria Ã© um recurso limitado. Exemplos incluem o uso de dicionÃ¡rios em Python ou bibliotecas de caching como `cachetools`.
*   **Caches em disco:** O cache Ã© armazenado no disco rÃ­gido do servidor. Essa opÃ§Ã£o Ã© mais barata do que o caching em memÃ³ria, mas tambÃ©m mais lenta. Exemplos incluem o uso de bancos de dados NoSQL como Redis ou Memcached.
*   **Caches distribuÃ­dos:** O cache Ã© distribuÃ­do em vÃ¡rios servidores. Essa opÃ§Ã£o Ã© a mais escalÃ¡vel, mas tambÃ©m a mais complexa de implementar. Exemplos incluem o uso de serviÃ§os de caching como Amazon ElastiCache ou Google Cloud Memorystore.



![A caching system architecture for LLM-based applications using embedding similarity.](./../images/image3.jpg)

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um sistema que usa um cache em memÃ³ria (RAM) com capacidade de 10GB e um cache em disco (SSD) com capacidade de 100GB. O cache em memÃ³ria Ã© usado para armazenar as respostas mais recentes e mais frequentemente acessadas, enquanto o cache em disco Ã© usado para armazenar um histÃ³rico maior de respostas.

**Teorema 1:** *Impacto do tamanho do cache na taxa de acerto (hit rate).* A taxa de acerto do cache (proporÃ§Ã£o de requisiÃ§Ãµes atendidas pelo cache) aumenta monotonicamente com o tamanho do cache, atÃ© um ponto de saturaÃ§Ã£o.

*Proof (EstratÃ©gia):* Inicialmente, Ã  medida que o tamanho do cache aumenta, mais dados podem ser armazenados, elevando a probabilidade de encontrar a informaÃ§Ã£o desejada no cache. No entanto, apÃ³s um determinado ponto, o cache comeÃ§a a armazenar dados menos frequentemente acessados, o que contribui menos para a taxa de acerto e eventualmente leva a um ganho marginal decrescente no desempenho. Formalmente, podemos modelar a taxa de acerto $H(C)$ como uma funÃ§Ã£o do tamanho do cache $C$.  A derivada $\frac{dH}{dC}$ Ã© positiva, indicando que aumentar $C$ aumenta $H$.  Contudo, $\frac{d^2H}{dC^2}$ Ã© negativa, indicando que o aumento de $H$ com $C$ diminui Ã  medida que $C$ aumenta, ilustrando a lei dos retornos decrescentes.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que um sistema RAG tenha um cache com diferentes tamanhos.
>
> | Tamanho do Cache (GB) | Taxa de Acerto (%) |
> | ---------------------- | ------------------- |
> | 1                      | 20                  |
> | 5                      | 50                  |
> | 10                     | 70                  |
> | 20                     | 75                  |
> | 50                     | 78                  |
>
> Como podemos ver, aumentar o tamanho do cache de 1GB para 10GB aumenta significativamente a taxa de acerto. No entanto, aumentar o tamanho do cache de 20GB para 50GB tem um impacto muito menor. Isso demonstra o ponto de saturaÃ§Ã£o mencionado no teorema.

**Exemplo:**

Considere um sistema RAG para responder a perguntas sobre artigos cientÃ­ficos. O sistema recebe uma pergunta do usuÃ¡rio, recupera os artigos relevantes usando um Ã­ndice de busca e, em seguida, usa um LLM para gerar uma resposta.

Para implementar o caching neste sistema, podemos usar uma estratÃ©gia baseada em chave. A chave do cache pode ser um hash da pergunta do usuÃ¡rio. Quando uma pergunta Ã© feita, o sistema verifica se a chave correspondente existe no cache. Se existir, a resposta armazenada em cache Ã© retornada. Caso contrÃ¡rio, o sistema realiza a busca, gera a resposta e armazena a resposta no cache, juntamente com a chave correspondente.

AlÃ©m disso, podemos implementar um cache baseado em tempo para garantir que as respostas armazenadas em cache nÃ£o fiquem obsoletas. Por exemplo, podemos invalidar as respostas armazenadas em cache apÃ³s 24 horas.

**Teorema 1.1:** *Cache HÃ­brido: CombinaÃ§Ã£o de Cache em MemÃ³ria e em Disco.* Um sistema de caching hÃ­brido que utiliza tanto cache em memÃ³ria quanto em disco oferece um compromisso entre velocidade e capacidade.

*Proof (EstratÃ©gia):* O cache em memÃ³ria Ã© usado para armazenar os dados acessados com mais frequÃªncia (seguindo uma polÃ­tica de substituiÃ§Ã£o como LRU - Least Recently Used), enquanto o cache em disco Ã© usado para dados acessados com menos frequÃªncia. Quando uma requisiÃ§Ã£o chega, o cache em memÃ³ria Ã© consultado primeiro. Se a informaÃ§Ã£o estiver presente, ela Ã© retornada imediatamente. Caso contrÃ¡rio, o cache em disco Ã© consultado. Se a informaÃ§Ã£o for encontrada no cache em disco, ela Ã© retornada e movida para o cache em memÃ³ria, substituindo o item menos recentemente usado. Se a informaÃ§Ã£o nÃ£o for encontrada em nenhum dos caches, ela Ã© calculada/recuperada e armazenada tanto no cache em memÃ³ria quanto no cache em disco. Este esquema aproveita a velocidade do cache em memÃ³ria para acessos frequentes e a capacidade do cache em disco para reter um conjunto maior de dados, otimizando o desempenho geral.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que o tempo de acesso ao cache em memÃ³ria seja de 1ms, ao cache em disco seja de 10ms, e o tempo para gerar uma resposta do LLM seja de 1000ms. Se a taxa de acerto no cache em memÃ³ria for de 60% e a taxa de acerto no cache em disco (considerando os acessos que nÃ£o foram encontrados na memÃ³ria) for de 30%, a latÃªncia mÃ©dia serÃ¡:
>
> $$\text{LatÃªncia MÃ©dia} = 0.60 \times 1\text{ms} + 0.40 \times (0.30 \times 10\text{ms} + 0.70 \times 1000\text{ms}) = 0.6 + 0.4(3 + 700) = 0.6 + 0.4(703) = 0.6 + 281.2 = 281.8\text{ms}$$
>
> Sem caching, a latÃªncia seria de 1000ms. O cache hÃ­brido reduz a latÃªncia significativamente.

### ConclusÃ£o
O **caching** Ã© uma tÃ©cnica poderosa para otimizar sistemas RAG, reduzindo a latÃªncia e o custo. Ao armazenar dados previamente computados ou recuperados, o caching permite que solicitaÃ§Ãµes futuras sejam atendidas de forma mais rÃ¡pida e eficiente. A escolha da estratÃ©gia de caching e da implementaÃ§Ã£o dependerÃ¡ da arquitetura do sistema RAG e dos requisitos especÃ­ficos da aplicaÃ§Ã£o. A implementaÃ§Ã£o cuidadosa do caching pode levar a melhorias significativas no desempenho e na escalabilidade de sistemas RAG.

### ReferÃªncias
[^1]: Caching is a technique for storing previously retrieved or computed data, allowing future requests for the same data to be served faster.
<!-- END -->