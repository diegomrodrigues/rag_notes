## RAG e Aplica√ß√µes de Question Answering e Chat

### Introdu√ß√£o
A arquitetura Retrieval-Augmented Generation (RAG) emergiu como uma solu√ß√£o eficaz para aprimorar modelos de linguagem grandes (LLMs) com conhecimento externo e atualizado. Este cap√≠tulo explora como RAG capacita aplica√ß√µes de *question answering* e *chat*, integrando motores de busca da web com LLMs e habilitando aplica√ß√µes de *chat-with-your-data* [^2]. Al√©m disso, discutiremos o papel crucial de bibliotecas de c√≥digo aberto como LangChain e LlamaIndex, que fornecem as ferramentas necess√°rias para a constru√ß√£o de pipelines e aplica√ß√µes baseadas em LLMs, facilitando a implementa√ß√£o de RAG [^2].

### Conceitos Fundamentais
O RAG aborda a limita√ß√£o dos LLMs em rela√ß√£o ao conhecimento factual, permitindo que acessem e integrem informa√ß√µes externas no processo de gera√ß√£o de respostas [^2]. Isso √© particularmente √∫til em cen√°rios onde as informa√ß√µes necess√°rias n√£o est√£o presentes nos dados de treinamento do LLM ou quando se deseja fornecer respostas baseadas em dados atualizados [^2].

![Diagram of a Naive RAG architecture showcasing the basic workflow from query to answer generation.](./../images/image4.png)

**1. Question Answering (QA) com RAG:**
Em aplica√ß√µes de *question answering*, o RAG permite que o LLM responda perguntas com base em um conjunto de documentos relevantes recuperados dinamicamente [^2]. O processo geralmente envolve as seguintes etapas:

1.  **Consulta:** O usu√°rio faz uma pergunta.
2.  **Recupera√ß√£o:** Um motor de busca (como um √≠ndice vetorial) recupera os documentos mais relevantes para a pergunta.
3.  **Aumento:** A pergunta original √© aumentada com os documentos recuperados.
4.  **Gera√ß√£o:** O LLM usa a pergunta aumentada para gerar uma resposta.

Formalmente, seja $q$ a pergunta do usu√°rio e $D = \{d_1, d_2, ..., d_n\}$ um conjunto de documentos. O objetivo √© encontrar um subconjunto $D' \subseteq D$ de documentos relevantes para $q$. A relev√¢ncia pode ser definida por uma fun√ß√£o de similaridade $s(q, d_i)$ entre a pergunta $q$ e cada documento $d_i$. O conjunto $D'$ √© ent√£o definido como:

$$D' = \{d_i \in D \mid s(q, d_i) > \theta\}$$

Onde $\theta$ √© um limiar de relev√¢ncia. A resposta $r$ √© gerada pelo LLM com base em $q$ e $D'$:

$$r = LLM(q, D')$$

**Teorema 1** (Otimiza√ß√£o do Limiar de Relev√¢ncia): A escolha do limiar de relev√¢ncia $\theta$ impacta diretamente a precis√£o e a revoca√ß√£o da resposta gerada.

*Prova*: Um valor de $\theta$ muito alto pode levar √† exclus√£o de documentos relevantes (baixa revoca√ß√£o), enquanto um valor muito baixo pode incluir documentos irrelevantes, diminuindo a precis√£o. A otimiza√ß√£o de $\theta$ envolve encontrar um equil√≠brio entre esses dois fatores, frequentemente utilizando t√©cnicas de valida√ß√£o cruzada em um conjunto de dados de perguntas e respostas conhecidas.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um conjunto de 5 documentos e uma pergunta. Calculamos a similaridade entre a pergunta e cada documento usando alguma m√©trica (e.g., similaridade de cossenos dos embeddings) obtendo os seguintes scores:
>
> | Documento | Similaridade |
> | --------- | ------------ |
> | $d_1$     | 0.85         |
> | $d_2$     | 0.70         |
> | $d_3$     | 0.92         |
> | $d_4$     | 0.60         |
> | $d_5$     | 0.78         |
>
> Se definirmos o limiar de relev√¢ncia $\theta = 0.75$, ent√£o o conjunto de documentos relevantes $D'$ ser√° {$d_1$, $d_3$, $d_5$}. Se diminuirmos o limiar para $\theta = 0.65$, $D'$ se torna {$d_1$, $d_2$, $d_3$, $d_5$}, aumentando o recall mas possivelmente diminuindo a precis√£o. A escolha √≥tima de $\theta$ depende do trade-off entre precis√£o e recall desejado para a aplica√ß√£o.
>
> Para encontrar o melhor valor para $\theta$, podemos usar um conjunto de valida√ß√£o. Por exemplo, podemos testar diferentes valores de $\theta$ e medir a precis√£o e o recall da resposta gerada pelo LLM. Suponha que obtemos os seguintes resultados:
>
> | $\theta$ | Precis√£o | Recall |
> | -------- | -------- | ------ |
> | 0.60     | 0.70     | 0.90   |
> | 0.70     | 0.80     | 0.85   |
> | 0.80     | 0.90     | 0.75   |
> | 0.90     | 0.95     | 0.60   |
>
> Neste caso, $\theta = 0.80$ parece ser um bom compromisso entre precis√£o e recall.

**1.1 Relev√¢ncia Ponderada:**
√â importante notar que nem todos os documentos em $D'$ contribuem igualmente para a resposta. Introduzimos um peso $w_i$ para cada documento $d_i \in D'$ que reflete sua relev√¢ncia relativa √† pergunta $q$. A resposta $r$ √© ent√£o gerada considerando esses pesos:

$$r = LLM(q, \{(d_i, w_i) \mid d_i \in D' \})$$

A fun√ß√£o de similaridade $s(q, d_i)$ pode ser usada para derivar os pesos $w_i$. Por exemplo, podemos normalizar os valores de similaridade:

$$w_i = \frac{s(q, d_i)}{\sum_{d_j \in D'} s(q, d_j)}$$

Essa abordagem permite que o LLM foque nos documentos mais relevantes ao gerar a resposta.

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior com $D' = \{d_1, d_3, d_5\}$ e as similaridades 0.85, 0.92, e 0.78, podemos calcular os pesos normalizados:
>
> $\text{Total Similaridade} = 0.85 + 0.92 + 0.78 = 2.55$
>
> $w_1 = \frac{0.85}{2.55} \approx 0.33$
> $w_3 = \frac{0.92}{2.55} \approx 0.36$
> $w_5 = \frac{0.78}{2.55} \approx 0.31$
>
> O LLM agora recebe $q$ e $\{(d_1, 0.33), (d_3, 0.36), (d_5, 0.31)\}$.  Isso instrui o LLM a dar mais import√¢ncia a $d_3$ (ligeiramente), pois ele tem a maior relev√¢ncia ponderada.

**2. Chat Applications com RAG:**
Em aplica√ß√µes de *chat*, o RAG permite que o LLM mantenha conversas informativas e contextuais, acessando e integrando informa√ß√µes externas em cada turno da conversa [^2]. Similar ao QA, o processo envolve a recupera√ß√£o de informa√ß√µes relevantes, mas com a adi√ß√£o do hist√≥rico da conversa para manter o contexto.

Seja $H = \{h_1, h_2, ..., h_t\}$ o hist√≥rico da conversa, onde $h_i$ representa o i-√©simo turno da conversa. A pergunta atual $q_t$ √© combinada com o hist√≥rico $H$ para formar uma consulta contextual $q'_t$:

$$q'_t = f(q_t, H)$$

Onde $f$ √© uma fun√ß√£o que combina a pergunta atual com o hist√≥rico da conversa. Os documentos relevantes $D'$ s√£o recuperados com base em $q'_t$, e a resposta $r_t$ √© gerada pelo LLM:

$$r_t = LLM(q'_t, D')$$

**Lema 2** (Preserva√ß√£o do Contexto): A fun√ß√£o $f(q_t, H)$ deve preservar o contexto relevante do hist√≥rico da conversa $H$ para garantir a coer√™ncia e a relev√¢ncia da resposta $r_t$.

*Prova*: A perda de contexto pode levar a respostas irrelevantes ou contradit√≥rias. A fun√ß√£o $f$ pode empregar t√©cnicas como sumariza√ß√£o do hist√≥rico, sele√ß√£o de turnos relevantes ou uso de vetores de embedding para representar o contexto. A escolha da t√©cnica depende da complexidade da conversa e dos recursos computacionais dispon√≠veis.

> üí° **Exemplo Num√©rico:**
>
> Suponha o seguinte hist√≥rico de conversa:
>
> $h_1$: Usu√°rio: "Qual √© a capital da Fran√ßa?"
> $h_2$: LLM: "A capital da Fran√ßa √© Paris."
> $h_3$: Usu√°rio: "E qual √© a popula√ß√£o dessa cidade?" ($q_t$)
>
> Aqui, $q_t =$ "E qual √© a popula√ß√£o dessa cidade?". Se usarmos concatena√ß√£o simples, $q'_t =$ "Qual √© a capital da Fran√ßa? A capital da Fran√ßa √© Paris. E qual √© a popula√ß√£o dessa cidade?". O motor de busca ir√° procurar por documentos relevantes para essa consulta completa. Se usarmos uma sumariza√ß√£o simples, podemos resumir o hist√≥rico para "Pergunta sobre Paris". Ent√£o $q'_t =$ "Qual √© a popula√ß√£o de Paris?". Essa √∫ltima consulta √© mais concisa e direcionada.

![Popular Chat Engine types within RAG architectures: context-augmented and condense-plus-context.](./../images/image6.png)

**2.1 Estrat√©gias para Gerenciamento do Hist√≥rico:**
A fun√ß√£o $f(q_t, H)$ pode ser implementada utilizando diferentes estrat√©gias, cada uma com suas vantagens e desvantagens:

*   **Concatena√ß√£o Simples:** Concatena a pergunta atual $q_t$ com todo o hist√≥rico $H$. Essa abordagem √© simples, mas pode levar a consultas muito longas, afetando o desempenho do motor de busca e do LLM.

*   **Sumariza√ß√£o do Hist√≥rico:** Resume o hist√≥rico $H$ para um tamanho fixo antes de combin√°-lo com a pergunta atual $q_t$. Isso reduz o tamanho da consulta, mas pode levar √† perda de informa√ß√µes importantes.

*   **Sele√ß√£o de Turnos Relevantes:** Seleciona apenas os turnos do hist√≥rico $H$ que s√£o mais relevantes para a pergunta atual $q_t$. Isso pode ser feito utilizando t√©cnicas de similaridade sem√¢ntica entre a pergunta e cada turno do hist√≥rico.

A escolha da estrat√©gia ideal depende das caracter√≠sticas da aplica√ß√£o e dos recursos computacionais dispon√≠veis.

> üí° **Exemplo Num√©rico:**
>
> Considere uma conversa sobre diferentes modelos de carros.
>
> | Turno | Usu√°rio/LLM                                   |
> | ----- | --------------------------------------------- |
> | 1     | Usu√°rio: Fale sobre o Toyota Corolla.        |
> | 2     | LLM: O Toyota Corolla √© um carro confi√°vel... |
> | 3     | Usu√°rio: E o Honda Civic?                   |
> | 4     | LLM: O Honda Civic √© um carro popular...     |
> | 5     | Usu√°rio: Qual deles √© mais econ√¥mico?         |
>
> *   **Concatena√ß√£o:** A consulta para o turno 5 seria "Fale sobre o Toyota Corolla. O Toyota Corolla √© um carro confi√°vel... E o Honda Civic? O Honda Civic √© um carro popular... Qual deles √© mais econ√¥mico?".

> *   **Sumariza√ß√£o:** O hist√≥rico poderia ser resumido para "Compara√ß√£o entre Toyota Corolla e Honda Civic". A consulta para o turno 5 seria "Qual dos carros comparados (Toyota Corolla e Honda Civic) √© mais econ√¥mico?".
>
> *   **Sele√ß√£o:** A consulta para o turno 5 poderia manter apenas os turnos 1, 2, 3 e 4, focando nos carros espec√≠ficos.

**3. Integra√ß√£o com Web Search Engines:**
O RAG pode ser integrado com motores de busca da web para acessar informa√ß√µes atualizadas e abrangentes [^2]. Isso permite que o LLM responda perguntas sobre eventos recentes, tend√™ncias atuais e outras informa√ß√µes que podem n√£o estar presentes em seus dados de treinamento.

**4. Chat-with-your-data Applications:**
As aplica√ß√µes de *chat-with-your-data* permitem que os usu√°rios interajam com seus pr√≥prios dados usando linguagem natural [^2]. Isso √© particularmente √∫til para acessar e analisar informa√ß√µes armazenadas em documentos, bancos de dados e outras fontes de dados. O RAG facilita essa intera√ß√£o, permitindo que o LLM recupere e integre informa√ß√µes relevantes dos dados do usu√°rio para responder √†s suas perguntas.

![Basic index retrieval: Document chunks are vectorized and retrieved to inform the LLM's response.](./../images/image1.png)

![Hierarchical index retrieval in RAG, showcasing a multi-stage approach for efficient document retrieval and information synthesis.](./../images/image9.png)

**Proposi√ß√£o 3:** (Indexa√ß√£o Sem√¢ntica para Dados Privados): A efic√°cia das aplica√ß√µes *chat-with-your-data* depende crucialmente da constru√ß√£o de um √≠ndice sem√¢ntico preciso sobre os dados privados do usu√°rio.

*Prova*: Um √≠ndice sem√¢ntico bem constru√≠do permite a recupera√ß√£o eficiente de informa√ß√µes relevantes, mesmo quando as consultas do usu√°rio n√£o correspondem exatamente aos termos nos documentos. T√©cnicas como embeddings de documentos e indexa√ß√£o vetorial s√£o essenciais para capturar o significado sem√¢ntico dos dados e facilitar a busca por similaridade.

> üí° **Exemplo Num√©rico:**
>
> Imagine um sistema de chat com seus documentos financeiros. O usu√°rio pergunta: "Qual foi o meu lucro total no √∫ltimo trimestre?".
>
> O sistema precisa:
> 1.  **Entender a pergunta:** Identificar a inten√ß√£o do usu√°rio (calcular lucro total) e o per√≠odo (√∫ltimo trimestre).
> 2.  **Recuperar documentos relevantes:** Buscar documentos financeiros do √∫ltimo trimestre, como balan√ßos, demonstrativos de resultados e extratos banc√°rios.
> 3.  **Extrair informa√ß√µes:** Analisar os documentos para identificar as receitas e despesas do per√≠odo.
> 4.  **Calcular o lucro:** Subtrair as despesas das receitas.
> 5.  **Gerar a resposta:** "Seu lucro total no √∫ltimo trimestre foi de R\$ 150.000,00."
>
> A qualidade da resposta depende da capacidade do sistema em recuperar os documentos corretos e extrair as informa√ß√µes relevantes.

![Diagrama ilustrativo da transforma√ß√£o de consultas em um sistema RAG, mostrando a decomposi√ß√£o e o enriquecimento da consulta inicial para melhorar a recupera√ß√£o.](./../images/image5.png)

![Sentence Window Retrieval: Diagram illustrating the technique of retrieving a single relevant sentence and expanding context for the LLM.](./../images/image3.png)

![Parent-child chunks retrieval enhances context for LLMs by merging related leaf chunks into a larger parent chunk during retrieval.](./../images/image10.png)

![Diagram illustrating the Fusion Retrieval technique, combining keyword-based and semantic search for enhanced RAG.](./../images/image7.png)

![Multi-document agent architecture for advanced RAG, showcasing query routing and agentic behavior.](./../images/image2.png)

![Diagram of an advanced RAG architecture, showcasing key components like agents, DB storage, and reranking to optimize information retrieval for LLM integration.](./../images/image8.png)

**5. Open-source Libraries: LangChain e LlamaIndex:**
Bibliotecas de c√≥digo aberto como LangChain e LlamaIndex fornecem ferramentas e abstra√ß√µes que simplificam a constru√ß√£o de pipelines e aplica√ß√µes baseadas em LLMs [^2]. Essas bibliotecas oferecem funcionalidades como:

*   **Conectores de dados:** Permitem que o LLM acesse dados de diversas fontes.
*   **√çndices vetoriais:** Facilitam a recupera√ß√£o eficiente de documentos relevantes.
*   **Pipelines de RAG:** Simplificam a implementa√ß√£o de arquiteturas RAG.
*   **Ferramentas de avalia√ß√£o:** Ajudam a avaliar e melhorar o desempenho de aplica√ß√µes RAG.

### Conclus√£o
O RAG representa um avan√ßo significativo na forma como os LLMs podem ser utilizados para responder perguntas, manter conversas e interagir com dados [^2]. Ao integrar motores de busca da web e bibliotecas de c√≥digo aberto, o RAG capacita aplica√ß√µes de *question answering* e *chat* com conhecimento externo e atualizado, abrindo novas possibilidades para a intera√ß√£o humano-computador. A facilidade de implementa√ß√£o proporcionada por bibliotecas como LangChain e LlamaIndex democratiza o acesso a essa tecnologia, permitindo que desenvolvedores de todos os n√≠veis criem aplica√ß√µes inovadoras baseadas em RAG.

### Refer√™ncias
[^2]: Informa√ß√µes fornecidas no contexto.
<!-- END -->