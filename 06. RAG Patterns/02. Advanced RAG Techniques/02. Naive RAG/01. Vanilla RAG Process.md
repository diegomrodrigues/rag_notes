## Vanilla RAG: Processo e Componentes Fundamentais

### Introdu√ß√£o

Este cap√≠tulo detalha o processo de **Retrieval-Augmented Generation (RAG)** em sua forma mais b√°sica, frequentemente referida como *vanilla RAG*. O processo envolve a combina√ß√£o de t√©cnicas de recupera√ß√£o de informa√ß√£o (information retrieval) e gera√ß√£o de texto, utilizando *Large Language Models (LLMs)* para responder a consultas com base em um contexto relevante recuperado. Entender o vanilla RAG √© crucial para compreender as vers√µes mais avan√ßadas e personalizadas do RAG. O vanilla RAG serve como base para a experimenta√ß√£o e aprimoramento de sistemas de RAG, permitindo a explora√ß√£o de diferentes estrat√©gias de indexa√ß√£o, recupera√ß√£o e gera√ß√£o.

### Conceitos Fundamentais

O processo de vanilla RAG pode ser decomposto nas seguintes etapas principais:

1.  **Segmenta√ß√£o de Textos em Chunks**: Inicialmente, os documentos de origem s√£o divididos em segmentos menores, conhecidos como *chunks* [^1]. Esta etapa √© fundamental para otimizar o processo de recupera√ß√£o, pois chunks muito grandes podem conter informa√ß√µes irrelevantes, enquanto chunks muito pequenos podem perder o contexto necess√°rio.

    > üí° **Exemplo Num√©rico:**
    >
    > Suponha um documento de 1000 palavras. Podemos considerar diferentes estrat√©gias de chunking:
    >
    > | Chunking Strategy | Chunk Size (words) | Number of Chunks |
    > | ------------------ | ------------------ | ---------------- |
    > | Small              | 50                 | 20               |
    > | Medium             | 100                | 10               |
    > | Large              | 200                | 5                |
    >
    > A escolha depende do conte√∫do. Textos t√©cnicos podem se beneficiar de chunks menores para focar em informa√ß√µes espec√≠ficas, enquanto textos narrativos podem precisar de chunks maiores para manter o contexto. Por exemplo, um artigo cient√≠fico sobre "aprendizado profundo" pode ser dividido em chunks de 50 palavras para capturar defini√ß√µes e m√©todos precisos.

2.  **Convers√£o em Vetores Usando Transformer Encoders**: Cada chunk de texto √© ent√£o transformado em um vetor num√©rico utilizando modelos *Transformer Encoder* [^1]. Esses modelos, como o *BERT* (Bidirectional Encoder Representations from Transformers) ou variantes, capturam as nuances sem√¢nticas do texto e representam cada chunk em um espa√ßo vetorial de alta dimens√£o. A escolha do modelo Transformer Encoder influencia diretamente a qualidade da representa√ß√£o vetorial e, consequentemente, a efic√°cia da recupera√ß√£o.

    *   A transforma√ß√£o √© realizada atrav√©s de um processo de embedding. Dado um chunk de texto $c$, o modelo Transformer Encoder $E$ gera um vetor $v$:

        $$v = E(c)$$

    *   O vetor $v$ representa o chunk $c$ no espa√ßo vetorial.

    **Proposi√ß√£o 1.** *A escolha do tamanho ideal do chunk depende da densidade informacional do texto fonte. Textos t√©cnicos e cient√≠ficos, por exemplo, podem se beneficiar de chunks menores para evitar dilui√ß√£o do sinal, enquanto textos narrativos podem requerer chunks maiores para preservar o contexto.*

    *Proof Strategy.* A prova desta proposi√ß√£o reside na an√°lise emp√≠rica do desempenho do sistema RAG com diferentes tamanhos de chunk em diversos tipos de documentos. M√©tricas como precis√£o e revoca√ß√£o (precision and recall) podem ser utilizadas para avaliar a qualidade das respostas geradas pelo LLM.

    > üí° **Exemplo Num√©rico:**
    >
    > Considere dois chunks:
    >
    > *   Chunk 1: "O gato est√° no tapete."
    > *   Chunk 2: "O c√£o est√° no jardim."
    >
    > Suponha que, ap√≥s a aplica√ß√£o do Transformer Encoder, obtemos os seguintes vetores (simplificados para 2 dimens√µes para facilitar a visualiza√ß√£o):
    >
    > *   $v_1 = [0.8, 0.6]$ (representa√ß√£o do Chunk 1)
    > *   $v_2 = [0.3, 0.9]$ (representa√ß√£o do Chunk 2)
    >
    > Estes vetores representam a posi√ß√£o de cada chunk no espa√ßo vetorial. A similaridade entre eles pode ser calculada, por exemplo, usando a similaridade do cosseno.

3.  **Indexa√ß√£o de Vetores**: Os vetores gerados na etapa anterior s√£o indexados em uma estrutura de dados especializada para busca vetorial [^1]. Essa estrutura permite a recupera√ß√£o eficiente dos vetores mais similares a um vetor de consulta. Existem diversas op√ß√µes para a indexa√ß√£o, como *√°rvores KD*, *Locality Sensitive Hashing (LSH)* ou *estruturas baseadas em grafos*. A escolha da estrutura de indexa√ß√£o depende do tamanho do corpus, da velocidade de recupera√ß√£o desejada e da precis√£o da busca.

    *   O processo de indexa√ß√£o envolve a cria√ß√£o de uma estrutura de dados $I$ que mapeia vetores a seus respectivos chunks de texto. Dado um conjunto de vetores $V = \{v_1, v_2, \ldots, v_n\}$, a indexa√ß√£o cria $I(V)$.

    **Teorema 1.** *A complexidade da busca na estrutura de √≠ndice $I$ impacta diretamente a lat√™ncia do sistema RAG. Estruturas de √≠ndice mais complexas podem oferecer maior precis√£o na recupera√ß√£o, mas ao custo de maior tempo de resposta.*

    *Proof.* A prova deste teorema decorre da an√°lise da complexidade algor√≠tmica das diferentes estruturas de indexa√ß√£o. Por exemplo, a busca em uma √°rvore KD tem complexidade $O(\log n)$ no caso m√©dio, enquanto a busca exaustiva em um espa√ßo vetorial tem complexidade $O(n)$. A escolha da estrutura de √≠ndice deve, portanto, levar em considera√ß√£o o compromisso entre precis√£o e lat√™ncia.

    **Lema 1.1.** *Para grandes volumes de dados, t√©cnicas de quantiza√ß√£o de vetores podem ser empregadas para reduzir o tamanho dos √≠ndices e acelerar a busca, com um pequeno sacrif√≠cio na precis√£o.*

    *Proof Strategy.* A quantiza√ß√£o de vetores envolve a compress√£o dos vetores utilizando t√©cnicas como *Product Quantization* ou *Scalar Quantization*. Isso reduz o espa√ßo de armazenamento necess√°rio para o √≠ndice e permite buscas mais r√°pidas, pois os c√°lculos de similaridade s√£o realizados em vetores menores. A perda de precis√£o resultante da quantiza√ß√£o pode ser minimizada atrav√©s da escolha cuidadosa dos par√¢metros da quantiza√ß√£o.

    > üí° **Exemplo Num√©rico:**
    >
    > Considere um √≠ndice com 1000 vetores.
    >
    > | Indexing Method | Average Search Time | Memory Footprint |
    > | --------------- | ------------------- | ---------------- |
    > | Exact Search    | 100 ms              | 8 MB             |
    > | LSH             | 10 ms               | 4 MB             |
    > | Quantization    | 5 ms                | 2 MB             |
    >
    > A busca exata garante a recupera√ß√£o dos vetores mais similares, mas √© mais lenta. LSH e Quantization s√£o mais r√°pidas, mas podem perder alguns dos vetores mais relevantes. A escolha do m√©todo depende dos requisitos de desempenho e da toler√¢ncia a falsos negativos.

4.  **Cria√ß√£o de Prompts para LLMs**: A etapa final envolve a cria√ß√£o de prompts para o LLM que instruem o modelo a responder √† consulta com base no contexto recuperado [^1]. O prompt geralmente inclui a consulta do usu√°rio e os chunks de texto mais relevantes recuperados na etapa anterior. O objetivo √© fornecer ao LLM informa√ß√µes suficientes para gerar uma resposta precisa e relevante.

    *   Um exemplo de prompt pode ser:

        `"Responda √† seguinte pergunta com base no contexto fornecido. Pergunta: [Consulta do usu√°rio]. Contexto: [Chunks de texto recuperados]."`

    **Teorema 2.** *A qualidade do prompt influencia significativamente a qualidade da resposta gerada pelo LLM. Prompts bem formulados que explicitam o papel do LLM e fornecem contexto claro tendem a produzir respostas mais precisas e relevantes.*

    *Proof Strategy.* Este teorema pode ser provado empiricamente atrav√©s da experimenta√ß√£o com diferentes tipos de prompts e da avalia√ß√£o da qualidade das respostas geradas pelo LLM. M√©tricas como *BLEU score*, *ROUGE score* ou avalia√ß√£o humana podem ser utilizadas para comparar a qualidade das diferentes respostas.

    > üí° **Exemplo Num√©rico:**
    >
    > Suponha a seguinte pergunta: "Qual a cor do gato?" e o seguinte chunk recuperado: "O gato preto est√° dormindo no tapete."
    >
    > Prompt 1 (B√°sico): "Pergunta: Qual a cor do gato? Contexto: O gato preto est√° dormindo no tapete."
    > Prompt 2 (Melhorado): "Responda √† pergunta com base no seguinte contexto. Pergunta: Qual a cor do gato? Contexto: O gato preto est√° dormindo no tapete. Resposta:"
    >
    > O Prompt 2 √© melhor porque instrui explicitamente o LLM a responder √† pergunta e fornece um marcador para o in√≠cio da resposta.



![Diagram of a Naive RAG architecture showcasing the basic workflow from query to answer generation.](./../images/image4.png)

### Conclus√£o

O vanilla RAG representa um ponto de partida essencial para sistemas mais complexos de RAG [^1]. Ao compreender o processo de segmenta√ß√£o, convers√£o em vetores, indexa√ß√£o e cria√ß√£o de prompts, √© poss√≠vel explorar e aprimorar cada etapa para otimizar a precis√£o, relev√¢ncia e efici√™ncia do sistema. As pr√≥ximas se√ß√µes deste material abordar√£o estrat√©gias avan√ßadas de RAG, incluindo t√©cnicas de otimiza√ß√£o de prompts, m√©todos de recupera√ß√£o mais sofisticados e abordagens para lidar com diferentes tipos de dados.

### Refer√™ncias

[^1]: Informa√ß√£o fornecida na descri√ß√£o do t√≥pico.
<!-- END -->