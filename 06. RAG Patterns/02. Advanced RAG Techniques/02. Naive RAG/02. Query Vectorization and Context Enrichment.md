## Vetoriza√ß√£o de Consultas e Busca de Contexto em RAG Naive

### Introdu√ß√£o

No contexto de **Retrieval-Augmented Generation (RAG)**, a etapa de **vetoriza√ß√£o de consultas** e a **busca de contexto** correspondente s√£o componentes cruciais para enriquecer o prompt do Large Language Model (LLM) com informa√ß√µes relevantes. No padr√£o RAG *Naive*, a consulta do usu√°rio √© transformada em um vetor usando um modelo Encoder, e esse vetor √© comparado com um √≠ndice vetorial pr√©-constru√≠do para identificar os *k* resultados mais relevantes [^2]. A escolha do modelo Encoder Transformer impacta diretamente a qualidade dos embeddings vetoriais e, consequentemente, a efic√°cia da busca [^2]. Este cap√≠tulo explora detalhadamente esse processo, enfatizando a import√¢ncia da sele√ß√£o do modelo Encoder e seus impactos na performance do sistema RAG. Al√©m disso, exploraremos as limita√ß√µes do RAG Naive e como estrat√©gias de re-rank podem mitigar algumas dessas limita√ß√µes.

### Conceitos Fundamentais

A etapa de vetoriza√ß√£o da consulta em tempo de execu√ß√£o envolve os seguintes passos:

1.  **Codifica√ß√£o da Consulta:** A consulta do usu√°rio √© alimentada no modelo Encoder Transformer selecionado [^2]. Este modelo, treinado para mapear texto para um espa√ßo vetorial denso, gera um vetor representativo da consulta. A qualidade dessa representa√ß√£o √© crucial, pois ela determinar√° a similaridade entre a consulta e os chunks de texto indexados.

2.  **Busca no √çndice Vetorial:** O vetor da consulta √© usado para realizar uma busca de similaridade no √≠ndice vetorial [^2]. Este √≠ndice, constru√≠do a partir dos embeddings dos chunks de texto, permite encontrar os *k* vizinhos mais pr√≥ximos do vetor da consulta. Algoritmos de busca de vizinhos mais pr√≥ximos aproximados (ANN), como HNSW (Hierarchical Navigable Small World), s√£o frequentemente utilizados para acelerar esse processo, especialmente em bases de dados de grande escala.

3.  **Recupera√ß√£o dos Chunks de Texto:** Os *k* chunks de texto correspondentes aos vetores mais similares s√£o recuperados do banco de dados [^2]. Esses chunks representar√£o o contexto relevante para a consulta do usu√°rio e ser√£o usados para enriquecer o prompt do LLM.

> üí° **Exemplo Num√©rico:** Suponha que temos um √≠ndice vetorial constru√≠do com embeddings de dimens√£o 128. Uma consulta, ap√≥s ser codificada pelo modelo Encoder, resulta no vetor $q = [0.1, -0.2, 0.3, \ldots, 0.05]$. Este vetor `q` √© ent√£o comparado com os vetores no √≠ndice para encontrar os *k* mais similares, utilizando, por exemplo, a similaridade do cosseno.

![Diagram of a Naive RAG architecture showcasing the basic workflow from query to answer generation.](./../images/image4.png)

**Sele√ß√£o do Modelo Encoder Transformer**

A escolha do modelo Encoder Transformer √© um fator determinante no desempenho do sistema RAG. V√°rios aspectos devem ser considerados:

*   **Qualidade dos Embeddings:** O modelo Encoder deve ser capaz de gerar embeddings vetoriais que capturem com precis√£o a sem√¢ntica do texto. Modelos pr√©-treinados em grandes corpora de texto, como BERT, RoBERTa, e Sentence-BERT, s√£o frequentemente utilizados como base [^2]. Modelos Sentence-BERT s√£o particularmente adequados para tarefas de similaridade sem√¢ntica, pois s√£o treinados para gerar embeddings que representam senten√ßas inteiras, otimizando a compara√ß√£o entre consultas e chunks de texto.

*   **Tamanho do Modelo:** Modelos maiores tendem a gerar embeddings de maior qualidade, mas tamb√©m requerem mais recursos computacionais [^2]. √â importante encontrar um equil√≠brio entre a qualidade dos embeddings e a efici√™ncia computacional, considerando as restri√ß√µes de hardware e os requisitos de lat√™ncia do sistema.

*   **Dom√≠nio do Texto:** Se o sistema RAG for aplicado a um dom√≠nio espec√≠fico, pode ser ben√©fico usar um modelo Encoder treinado ou fine-tuned nesse dom√≠nio [^2]. Isso pode melhorar significativamente a qualidade dos embeddings e, consequentemente, a precis√£o da busca.

*   **Tamanho da Janela de Contexto:** O tamanho m√°ximo da sequ√™ncia de entrada que o modelo Encoder pode processar (janela de contexto) tamb√©m √© um fator importante [^2]. Se os chunks de texto forem maiores do que a janela de contexto do modelo, ser√° necess√°rio trunc√°-los ou dividi-los em segmentos menores, o que pode comprometer a qualidade dos embeddings.

> üí° **Exemplo Num√©rico:** Considere um modelo BERT com uma janela de contexto de 512 tokens. Se um chunk de texto tem 700 tokens, ele precisa ser dividido em pelo menos dois chunks (e.g., 350 e 350, ou 512 e 188) para que possa ser processado pelo modelo. Isso pode resultar na perda de contexto entre as partes separadas.

**Considera√ß√µes T√©cnicas**

*   **Normaliza√ß√£o dos Embeddings:** √â comum normalizar os embeddings vetoriais para que tenham norma unit√°ria [^2]. Isso facilita a compara√ß√£o entre vetores e melhora a precis√£o da busca de similaridade.

*   **M√©tricas de Similaridade:** A m√©trica de similaridade utilizada para comparar vetores (e.g., similaridade do cosseno, dist√¢ncia euclidiana) deve ser escolhida com cuidado, pois ela pode impactar significativamente os resultados da busca [^2]. A similaridade do cosseno √© uma escolha comum, pois ela mede o √¢ngulo entre dois vetores, tornando-a menos sens√≠vel √† magnitude dos vetores.

> üí° **Exemplo Num√©rico:**
>
> Sejam dois vetores, $v_1 = [3, 4]$ e $v_2 = [6, 8]$.
>
> A similaridade do cosseno √© calculada como:
>
> $\text{cosine\_similarity}(v_1, v_2) = \frac{v_1 \cdot v_2}{||v_1|| \cdot ||v_2||}$
>
> $\text{Step 1: Calculate dot product}$
> $v_1 \cdot v_2 = (3 \times 6) + (4 \times 8) = 18 + 32 = 50$
>
> $\text{Step 2: Calculate magnitudes}$
> $||v_1|| = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = \sqrt{25} = 5$
> $||v_2|| = \sqrt{6^2 + 8^2} = \sqrt{36 + 64} = \sqrt{100} = 10$
>
> $\text{Step 3: Calculate cosine similarity}$
> $\text{cosine\_similarity}(v_1, v_2) = \frac{50}{5 \times 10} = \frac{50}{50} = 1$
>
> Nesse caso, a similaridade do cosseno √© 1, indicando que os vetores apontam na mesma dire√ß√£o.  Se tiv√©ssemos $v_3 = [-3, -4]$, a similaridade entre $v_1$ e $v_3$ seria -1.

*   **Indexa√ß√£o Vetorial:** A escolha do algoritmo de indexa√ß√£o vetorial e seus par√¢metros (e.g., n√∫mero de vizinhos, tamanho dos clusters) tamb√©m afeta o desempenho da busca [^2]. √â importante otimizar esses par√¢metros para obter um bom equil√≠brio entre a precis√£o da busca e a velocidade de indexa√ß√£o.

**Limita√ß√µes e Estrat√©gias de Re-Rank**

O RAG Naive, apesar de sua simplicidade, apresenta algumas limita√ß√µes inerentes. Uma delas √© a depend√™ncia exclusiva da similaridade vetorial para determinar a relev√¢ncia do contexto. Isso pode levar √† recupera√ß√£o de chunks de texto que s√£o semanticamente similares √† consulta, mas que n√£o cont√™m as informa√ß√µes mais relevantes para responder √† pergunta do usu√°rio. Al√©m disso, o RAG Naive n√£o considera a ordem ou a rela√ß√£o entre os chunks de texto recuperados, o que pode prejudicar a coer√™ncia e a completude da resposta gerada pelo LLM.

Para mitigar essas limita√ß√µes, estrat√©gias de *re-rank* podem ser empregadas. O re-rank consiste em aplicar um modelo de aprendizado de m√°quina para reordenar os *k* chunks de texto recuperados pelo √≠ndice vetorial, com base em crit√©rios de relev√¢ncia mais sofisticados do que a simples similaridade vetorial.

**Teorema 1** [Re-ranking Melhora a Precis√£o do RAG]
Dado um conjunto de documentos $\mathcal{D}$, uma consulta $q$, um modelo de embedding $E$, um √≠ndice vetorial $I$, e um modelo de linguagem $L$, a aplica√ß√£o de um modelo de re-rank $R$ nos $k$ documentos recuperados de $I$ aumenta a probabilidade de $L$ gerar uma resposta mais precisa em rela√ß√£o √† consulta $q$.

*Proof Strategy:* A prova se baseia no fato de que o modelo de re-rank $R$ √© treinado para discriminar entre documentos relevantes e irrelevantes, considerando m√∫ltiplos fatores al√©m da similaridade vetorial. Ao reordenar os documentos, $R$ prioriza aqueles que s√£o mais propensos a conter as informa√ß√µes necess√°rias para responder √† consulta $q$, aumentando a probabilidade de $L$ gerar uma resposta correta.

**Proposi√ß√£o 1** [Crit√©rios para um Modelo de Re-Rank Eficaz]
Um modelo de re-rank eficaz deve considerar os seguintes crit√©rios:
1.  **Relev√¢ncia Sem√¢ntica:** Similaridade sem√¢ntica entre a consulta e o chunk de texto, similar ao RAG Naive, mas possivelmente utilizando um modelo diferente ou uma m√©trica de similaridade mais refinada.
2.  **Coer√™ncia Contextual:** Rela√ß√£o entre os chunks de texto recuperados, buscando garantir que formem um contexto coerente e completo para responder √† consulta.
3.  **Import√¢ncia da Entidade:** Identifica√ß√£o e prioriza√ß√£o de chunks de texto que mencionam entidades importantes relacionadas √† consulta.
4.  **Novidade:** Prefer√™ncia por chunks de texto que fornecem informa√ß√µes novas ou complementares em rela√ß√£o aos chunks j√° considerados.

**Lema 1** [Modelos de Re-Rank Baseados em Transformers]
Modelos de re-rank baseados em Transformers, como o BERT e seus derivados, s√£o particularmente adequados para essa tarefa, pois podem capturar rela√ß√µes complexas entre a consulta e os chunks de texto [^3]. Esses modelos podem ser treinados com dados de treinamento anotados, onde cada exemplo consiste em uma consulta, um conjunto de chunks de texto recuperados e um r√≥tulo indicando a relev√¢ncia de cada chunk para a consulta.

[^3]: Nogueira, Marco, and Kyunghyun Cho. "Passage re-ranking with bert." *arXiv preprint arXiv:1901.04085* (2019).

> üí° **Exemplo Num√©rico:**
>
> Suponha que a consulta seja: "Quais s√£o os benef√≠cios da vitamina C?".  O RAG Naive recupera os seguintes chunks:
>
> 1.  "A vitamina D √© importante para os ossos." (Similaridade: 0.75)
> 2.  "A vitamina C fortalece o sistema imunol√≥gico." (Similaridade: 0.80)
> 3.  "O √°cido asc√≥rbico, tamb√©m conhecido como vitamina C, √© um antioxidante." (Similaridade: 0.70)
> 4.  "A falta de vitaminas pode causar doen√ßas." (Similaridade: 0.65)
>
> Um modelo de re-rank pode reordenar esses chunks com base na relev√¢ncia:
>
> 1.  "A vitamina C fortalece o sistema imunol√≥gico." (Relev√¢ncia: 0.95)
> 2.  "O √°cido asc√≥rbico, tamb√©m conhecido como vitamina C, √© um antioxidante." (Relev√¢ncia: 0.90)
> 3.  "A falta de vitaminas pode causar doen√ßas." (Relev√¢ncia: 0.60)
> 4.  "A vitamina D √© importante para os ossos." (Relev√¢ncia: 0.50)
>
> A tabela abaixo resume as mudan√ßas:
>
> | Chunk                                                         | Similaridade (RAG Naive) | Relev√¢ncia (Re-Rank) |
> | :------------------------------------------------------------ | :----------------------- | :------------------- |
> | A vitamina D √© importante para os ossos.                      | 0.75                     | 0.50                 |
> | A vitamina C fortalece o sistema imunol√≥gico.                 | 0.80                     | 0.95                 |
> | O √°cido asc√≥rbico, tamb√©m conhecido como vitamina C, √© um antioxidante. | 0.70                     | 0.90                 |
> | A falta de vitaminas pode causar doen√ßas.                      | 0.65                     | 0.60                 |

### Conclus√£o

A vetoriza√ß√£o da consulta e a busca de contexto s√£o etapas cr√≠ticas no padr√£o RAG Naive. A sele√ß√£o cuidadosa do modelo Encoder Transformer, considerando a qualidade dos embeddings, o tamanho do modelo, o dom√≠nio do texto e o tamanho da janela de contexto, √© essencial para garantir a efic√°cia do sistema RAG. Al√©m disso, a normaliza√ß√£o dos embeddings, a escolha da m√©trica de similaridade e a otimiza√ß√£o dos par√¢metros de indexa√ß√£o vetorial s√£o considera√ß√µes t√©cnicas importantes que podem impactar significativamente o desempenho da busca. A introdu√ß√£o de estrat√©gias de re-rank representa um avan√ßo significativo, permitindo refinar a sele√ß√£o de contexto e melhorar a qualidade das respostas geradas pelo LLM.

### Refer√™ncias

[^2]: At runtime, the user's query is vectorized using the same Encoder model, searched against the index to find the top-k results, and the corresponding text chunks from the database are used to enrich the LLM prompt as context. The selection of an appropriate Transformer Encoder model is crucial, as it directly affects the quality of vector embeddings and search results.
<!-- END -->