## Recupera√ß√£o por Fus√£o e Busca H√≠brida com RRF

### Introdu√ß√£o

A recupera√ß√£o de informa√ß√£o moderna, especialmente no contexto de *Retrieval-Augmented Generation* (RAG) com *Large Language Models* (LLMs), frequentemente se beneficia da combina√ß√£o de diferentes m√©todos de busca para otimizar a relev√¢ncia e precis√£o dos resultados. A busca h√≠brida, ou recupera√ß√£o por fus√£o, representa uma dessas abordagens, integrando m√©todos baseados em palavras-chave (como tf-idf e BM25) com t√©cnicas de busca sem√¢ntica ou vetorial [^1]. Este cap√≠tulo explorar√° a fundo a recupera√ß√£o por fus√£o, com foco no algoritmo *Reciprocal Rank Fusion* (RRF) como um m√©todo eficaz para reclassificar e integrar resultados de diferentes sistemas de recupera√ß√£o, superando os desafios inerentes √† combina√ß√£o de scores de similaridade heterog√™neos [^1].

### Conceitos Fundamentais

**Busca H√≠brida e Recupera√ß√£o por Fus√£o:**

A busca h√≠brida visa combinar as vantagens de diferentes abordagens de recupera√ß√£o. M√©todos baseados em palavras-chave, como **tf-idf** (Term Frequency-Inverse Document Frequency) e **BM25** (Best Matching 25), s√£o eficazes na identifica√ß√£o de documentos que cont√™m termos de consulta espec√≠ficos, mas podem falhar em capturar nuances sem√¢nticas ou lidar bem com varia√ß√µes lingu√≠sticas [^1]. Em contrapartida, a busca sem√¢ntica ou vetorial, que utiliza *embeddings* para representar documentos e consultas em um espa√ßo vetorial, pode capturar rela√ß√µes sem√¢nticas e similaridades conceituais, mas pode ser menos precisa na correspond√™ncia exata de termos [^1]. A recupera√ß√£o por fus√£o, portanto, busca integrar essas abordagens complementares para melhorar a qualidade geral da recupera√ß√£o.

> üí° **Exemplo Num√©rico: TF-IDF**
>
> Considere tr√™s documentos simples:
>
> *   Documento 1: "o gato est√° no tapete"
> *   Documento 2: "o cachorro est√° no tapete"
> *   Documento 3: "o gato gosta de brincar"
>
> E a consulta: "gato tapete"
>
> **Passo 1: Calcular a Frequ√™ncia dos Termos (TF)**
>
> | Termo   | Documento 1 | Documento 2 | Documento 3 |
> | ------- | ----------- | ----------- | ----------- |
> | gato    | 1           | 0           | 1           |
> | tapete  | 1           | 1           | 0           |
>
> **Passo 2: Calcular o Inverse Document Frequency (IDF)**
>
> $\text{IDF}(t) = \log \frac{\text{N√∫mero total de documentos}}{\text{N√∫mero de documentos contendo } t}$
>
> *   $\text{IDF(gato)} = \log \frac{3}{2} \approx 0.176$
> *   $\text{IDF(tapete)} = \log \frac{3}{2} \approx 0.176$
>
> **Passo 3: Calcular o TF-IDF**
>
> $\text{TF-IDF}(t, d) = \text{TF}(t, d) * \text{IDF}(t)$
>
> | Termo   | Documento 1 | Documento 2 | Documento 3 |
> | ------- | ----------- | ----------- | ----------- |
> | gato    | 0.176       | 0           | 0.176       |
> | tapete  | 0.176       | 0.176       | 0           |
>
> **Passo 4: Calcular o score para a consulta "gato tapete"**
>
> O score de cada documento √© a soma dos TF-IDF dos termos da consulta no documento.
>
> *   Documento 1: 0.176 + 0.176 = 0.352
> *   Documento 2: 0 + 0.176 = 0.176
> *   Documento 3: 0.176 + 0 = 0.176
>
> Neste exemplo, o Documento 1 teria o maior score TF-IDF para a consulta "gato tapete".  Isso demonstra como TF-IDF favorece documentos que cont√™m os termos da consulta com mais frequ√™ncia, ajustado pela raridade desses termos na cole√ß√£o de documentos.
>
> üí° **Exemplo Num√©rico: BM25**
>
> BM25 (Best Matching 25) √© uma fun√ß√£o de ranking utilizada para estimar a relev√¢ncia de documentos para um determinado conjunto de termos de busca. Ela √© uma melhoria em rela√ß√£o ao TF-IDF.
>
> A f√≥rmula geral do BM25 √©:
>
> $$Score(D, Q) = \sum_{i=1}^{n} IDF(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{avgdl})}$$
>
> Onde:
>
> *   $Q$ √© a consulta.
> *   $D$ √© o documento.
> *   $q_i$ √© um termo da consulta.
> *   $f(q_i, D)$ √© a frequ√™ncia do termo $q_i$ no documento $D$.
> *   $|D|$ √© o comprimento do documento $D$ em palavras.
> *   $avgdl$ √© o comprimento m√©dio dos documentos na cole√ß√£o.
> *   $k_1$ √© um par√¢metro que controla a satura√ß√£o da frequ√™ncia do termo (tipicamente entre 1.2 e 2.0).
> *   $b$ √© um par√¢metro que controla o efeito do comprimento do documento (tipicamente 0.75).
> *   $IDF(q_i)$ √© o Inverse Document Frequency do termo $q_i$.
>
> Usando os mesmos documentos do exemplo anterior:
>
> *   Documento 1: "o gato est√° no tapete" (comprimento = 5)
> *   Documento 2: "o cachorro est√° no tapete" (comprimento = 5)
> *   Documento 3: "o gato gosta de brincar" (comprimento = 5)
>
> Consulta: "gato tapete"
>
> Assumindo $k_1 = 1.2$, $b = 0.75$ e $avgdl = 5$ (j√° que todos os documentos t√™m o mesmo comprimento).
>
> $\text{IDF(gato)} = \log \frac{3}{2} \approx 0.176$
> $\text{IDF(tapete)} = \log \frac{3}{2} \approx 0.176$
>
> **Passo 1: Calcular o score BM25 para o Documento 1**
>
> $Score(D1, Q) = 0.176 \cdot \frac{1 \cdot (1.2 + 1)}{1 + 1.2 \cdot (1 - 0.75 + 0.75 \cdot \frac{5}{5})} + 0.176 \cdot \frac{1 \cdot (1.2 + 1)}{1 + 1.2 \cdot (1 - 0.75 + 0.75 \cdot \frac{5}{5})} $
>
> $Score(D1, Q) = 0.176 \cdot \frac{2.2}{1 + 1.2} + 0.176 \cdot \frac{2.2}{1 + 1.2} = 0.176 \cdot \frac{2.2}{2.2} + 0.176 \cdot \frac{2.2}{2.2} = 0.176 + 0.176 = 0.352$
>
> **Passo 2: Calcular o score BM25 para o Documento 2**
>
> $Score(D2, Q) = 0.176 \cdot \frac{0 \cdot (1.2 + 1)}{0 + 1.2 \cdot (1 - 0.75 + 0.75 \cdot \frac{5}{5})} + 0.176 \cdot \frac{1 \cdot (1.2 + 1)}{1 + 1.2 \cdot (1 - 0.75 + 0.75 \cdot \frac{5}{5})} $
>
> $Score(D2, Q) = 0 + 0.176 \cdot \frac{2.2}{2.2} = 0.176$
>
> **Passo 3: Calcular o score BM25 para o Documento 3**
>
> $Score(D3, Q) = 0.176 \cdot \frac{1 \cdot (1.2 + 1)}{1 + 1.2 \cdot (1 - 0.75 + 0.75 \cdot \frac{5}{5})} + 0.176 \cdot \frac{0 \cdot (1.2 + 1)}{0 + 1.2 \cdot (1 - 0.75 + 0.75 \cdot \frac{5}{5})} $
>
> $Score(D3, Q) = 0.176 \cdot \frac{2.2}{2.2} + 0 = 0.176$
>
> Neste exemplo simples, os scores BM25 s√£o os mesmos que os scores TF-IDF. No entanto, em cole√ß√µes maiores e com documentos de diferentes comprimentos, o BM25 pode oferecer resultados diferentes devido aos seus par√¢metros $k_1$ e $b$, que ajustam a import√¢ncia da frequ√™ncia dos termos e do comprimento dos documentos.
>
>
> üí° **Exemplo Num√©rico: Busca Sem√¢ntica e Similaridade de Cossenos**
>
> Suponha que temos os mesmos tr√™s documentos e a consulta do exemplo TF-IDF. Ap√≥s passar cada documento e a consulta por um modelo de *embedding* (como Sentence Transformers), obtemos os seguintes vetores (simplificados para 2 dimens√µes para facilitar a visualiza√ß√£o):
>
> *   Consulta: `q = [0.2, 0.8]`
> *   Documento 1: `d1 = [0.3, 0.7]`
> *   Documento 2: `d2 = [0.8, 0.2]`
> *   Documento 3: `d3 = [0.1, 0.9]`
>
> **Passo 1: Calcular a Similaridade de Cossenos**
>
> A similaridade de cossenos entre dois vetores $A$ e $B$ √© dada por:
>
> $\text{cosine\_similarity}(A, B) = \frac{A \cdot B}{||A|| \cdot ||B||}$
>
> Onde $A \cdot B$ √© o produto escalar dos vetores $A$ e $B$, e $||A||$ e $||B||$ s√£o as normas (magnitude) dos vetores $A$ e $B$, respectivamente.
>
> **Passo 2: Calcular a Similaridade entre a Consulta e cada Documento**
>
> *   Similaridade(Consulta, Documento 1):
>     *   $q \cdot d1 = (0.2 * 0.3) + (0.8 * 0.7) = 0.06 + 0.56 = 0.62$
>     *   $||q|| = \sqrt{0.2^2 + 0.8^2} = \sqrt{0.04 + 0.64} = \sqrt{0.68} \approx 0.825$
>     *   $||d1|| = \sqrt{0.3^2 + 0.7^2} = \sqrt{0.09 + 0.49} = \sqrt{0.58} \approx 0.762$
>     *   $\text{cosine\_similarity}(q, d1) = \frac{0.62}{0.825 * 0.762} \approx \frac{0.62}{0.628} \approx 0.987$
>
> *   Similaridade(Consulta, Documento 2):
>     *   $q \cdot d2 = (0.2 * 0.8) + (0.8 * 0.2) = 0.16 + 0.16 = 0.32$
>     *   $||d2|| = \sqrt{0.8^2 + 0.2^2} = \sqrt{0.64 + 0.04} = \sqrt{0.68} \approx 0.825$
>     *   $\text{cosine\_similarity}(q, d2) = \frac{0.32}{0.825 * 0.825} \approx \frac{0.32}{0.68} \approx 0.471$
>
> *   Similaridade(Consulta, Documento 3):
>     *   $q \cdot d3 = (0.2 * 0.1) + (0.8 * 0.9) = 0.02 + 0.72 = 0.74$
>     *   $||d3|| = \sqrt{0.1^2 + 0.9^2} = \sqrt{0.01 + 0.81} = \sqrt{0.82} \approx 0.906$
>     *   $\text{cosine\_similarity}(q, d3) = \frac{0.74}{0.825 * 0.906} \approx \frac{0.74}{0.747} \approx 0.990$
>
> **Passo 3: Ordenar os Documentos por Similaridade**
>
> Neste exemplo, a ordem dos documentos por similaridade de cossenos seria: Documento 3 (0.990), Documento 1 (0.987), Documento 2 (0.471).  Note que este ranking √© diferente do ranking obtido pelo TF-IDF, ilustrando como a busca sem√¢ntica captura similaridades diferentes das capturadas pela busca baseada em termos.

**Reciprocal Rank Fusion (RRF):**

O algoritmo *Reciprocal Rank Fusion* (RRF) √© uma t√©cnica popular para reclassificar resultados recuperados de diferentes sistemas de busca [^1]. Ele aborda o desafio de combinar scores de similaridade heterog√™neos, que podem ter escalas e distribui√ß√µes diferentes, tornando a compara√ß√£o direta problem√°tica. Em vez de usar os scores de similaridade diretamente, o RRF se baseia na *rank* (posi√ß√£o) dos documentos nos resultados de cada sistema de busca.

A ideia central do RRF √© que um documento que aparece no topo da lista de resultados de v√°rios sistemas de busca √© mais prov√°vel de ser relevante do que um documento que aparece no final de apenas um sistema. O RRF atribui um score a cada documento, baseado na soma dos inversos de seus ranks em cada lista de resultados, ponderada por uma constante *k*:

$$
RRFScore(d) = \sum_{i=1}^{n} \frac{1}{k + rank_i(d)}
$$

Onde:

*   $RRFScore(d)$ √© o score RRF do documento $d$.
*   $n$ √© o n√∫mero de sistemas de busca.
*   $rank_i(d)$ √© a posi√ß√£o do documento $d$ na lista de resultados do sistema de busca $i$.
*   $k$ √© uma constante, geralmente definida como 60 [^1], que controla a import√¢ncia dos documentos de alto rank.

Ap√≥s calcular o score RRF para cada documento, os documentos s√£o reclassificados em ordem decrescente de score RRF.

**Teorema 1** [Converg√™ncia do RRF para Rank Relevante]
√Ä medida que o n√∫mero de sistemas de busca $n$ aumenta, o RRFScore tende a favorecer documentos com ranks consistentemente altos em todos os sistemas, convergindo para um ranking que reflete uma relev√¢ncia consensual entre os sistemas.

**Prova (Esbo√ßo):**
Considere dois documentos, $d_1$ e $d_2$. Seja $rank_i(d_1)$ e $rank_i(d_2)$ seus respectivos ranks no sistema $i$. Se $d_1$ tiver um rank consistentemente menor (melhor) que $d_2$ em todos os sistemas (i.e., $rank_i(d_1) < rank_i(d_2)$ para todo $i$), ent√£o $RRFScore(d_1) > RRFScore(d_2)$ para qualquer $n$. √Ä medida que $n$ aumenta, a diferen√ßa entre os scores RRF se torna mais pronunciada, a menos que alguns sistemas deem a $d_2$ um rank muito superior, o que seria compensado pela consist√™ncia dos outros sistemas. Este comportamento leva a uma converg√™ncia para um ranking que prioriza documentos com relev√¢ncia consensual.

**Vantagens do RRF:**

*   **Robustez a Scores Heterog√™neos:** O RRF √© robusto a diferentes escalas e distribui√ß√µes de scores de similaridade, pois utiliza ranks em vez de scores brutos.
*   **Simplicidade e Efici√™ncia:** O algoritmo RRF √© simples de implementar e computacionalmente eficiente.
*   **Desempenho Emp√≠rico:** O RRF tem demonstrado bom desempenho emp√≠rico em uma variedade de tarefas de recupera√ß√£o de informa√ß√£o.

**Proposi√ß√£o 1** [Sensibilidade ao Par√¢metro k]
O par√¢metro *k* no RRF modula a sensibilidade do algoritmo a documentos com ranks mais baixos. Aumentar *k* diminui a import√¢ncia dos ranks altos, tornando o RRF mais tolerante a documentos que aparecem em posi√ß√µes mais baixas em algumas listas de resultados.

**Exemplo Ilustrativo:**

Considere que temos dois sistemas de busca: um baseado em tf-idf e outro baseado em *embeddings*. Para uma determinada consulta, os sistemas retornam as seguintes listas de resultados (ordenadas por relev√¢ncia):

*   Sistema tf-idf: \[Doc1, Doc3, Doc5, Doc2, Doc4]
*   Sistema de *embeddings*: \[Doc3, Doc2, Doc1, Doc6, Doc7]

Usando RRF com $k = 60$, calcular√≠amos o score RRF para cada documento da seguinte forma:

*   Doc1: 1/(60+1) + 1/(60+3) = 0.0161 + 0.0161 = 0.0322
*   Doc2: 1/(60+4) + 1/(60+2) = 0.0156 + 0.0164 = 0.0320
*   Doc3: 1/(60+2) + 1/(60+1) = 0.0164 + 0.0161 = 0.0325
*   Doc4: 1/(60+5) + 0 = 0.0154
*   Doc5: 1/(60+3) + 0 = 0.0161
*   Doc6: 0 + 1/(60+4) = 0.0156
*   Doc7: 0 + 1/(60+5) = 0.0154

A lista reclassificada seria ent√£o: \[Doc3, Doc1, Doc2, Doc5, Doc6, Doc4, Doc7]. Observe que Doc3, que apareceu no topo de ambas as listas, agora est√° no topo da lista reclassificada pelo RRF.

> üí° **Exemplo Num√©rico: RRF com diferentes valores de k**
>
> Usando o mesmo exemplo ilustrativo acima, vamos comparar o resultado do RRF com $k=10$ e $k=100$:
>
> **RRF com k = 10**
>
> *   Doc1: 1/(10+1) + 1/(10+3) = 0.0909 + 0.0769 = 0.1678
> *   Doc2: 1/(10+4) + 1/(10+2) = 0.0714 + 0.0833 = 0.1547
> *   Doc3: 1/(10+2) + 1/(10+1) = 0.0833 + 0.0909 = 0.1742
> *   Doc4: 1/(10+5) + 0 = 0.0667
> *   Doc5: 1/(10+3) + 0 = 0.0769
> *   Doc6: 0 + 1/(10+4) = 0.0714
> *   Doc7: 0 + 1/(10+5) = 0.0667
>
> Lista reclassificada (k=10): \[Doc3, Doc1, Doc2, Doc5, Doc6, Doc4, Doc7]
>
> **RRF com k = 100**
>
> *   Doc1: 1/(100+1) + 1/(100+3) = 0.0099 + 0.0097 = 0.0196
> *   Doc2: 1/(100+4) + 1/(100+2) = 0.0096 + 0.0098 = 0.0194
> *   Doc3: 1/(100+2) + 1/(100+1) = 0.0098 + 0.0099 = 0.0197
> *   Doc4: 1/(100+5) + 0 = 0.0095
> *   Doc5: 1/(100+3) + 0 = 0.0097
> *   Doc6: 0 + 1/(100+4) = 0.0096
> *   Doc7: 0 + 1/(100+5) = 0.0095
>
> Lista reclassificada (k=100): \[Doc3, Doc1, Doc2, Doc5, Doc6, Doc4, Doc7]
>
> Neste caso espec√≠fico, a ordem final dos documentos n√£o se alterou entre os diferentes valores de `k`. No entanto, ao aumentar o valor de `k`, as diferen√ßas entre os scores se tornam menores, diminuindo o impacto dos ranks individuais. Isso significa que, com um `k` maior, o RRF se torna mais tolerante a documentos que aparecem em posi√ß√µes mais baixas em uma das listas, dando mais peso √† concord√¢ncia geral entre os sistemas. Um `k` menor torna o ranking mais sens√≠vel √†s primeiras posi√ß√µes de cada lista.

**Corol√°rio 1** [Impacto de *k* no Exemplo Ilustrativo]
Se aumentarmos *k* para, digamos, 120, a diferen√ßa entre os scores RRF dos documentos se tornar√° menor, diminuindo o impacto dos ranks mais altos e tornando o ranking resultante mais uniforme.

**Extens√£o: RRF Ponderado**

Uma extens√£o natural do RRF √© atribuir pesos diferentes a cada sistema de busca, refletindo a confian√ßa ou qualidade percebida de cada um. Isso pode ser √∫til quando um sistema √© conhecido por ser mais preciso ou relevante do que outros em um determinado dom√≠nio. A f√≥rmula para o RRF ponderado seria:

$$
RRFScore(d) = \sum_{i=1}^{n} w_i \cdot \frac{1}{k + rank_i(d)}
$$

Onde $w_i$ √© o peso atribu√≠do ao sistema de busca $i$.

> üí° **Exemplo Num√©rico: RRF Ponderado**
>
> Usando o exemplo anterior, vamos supor que o sistema de *embeddings* √© considerado mais confi√°vel que o sistema tf-idf. Atribuiremos um peso de 0.7 ao sistema de *embeddings* e 0.3 ao sistema tf-idf.  Usaremos $k = 60$.
>
> *   Sistema tf-idf: \[Doc1, Doc3, Doc5, Doc2, Doc4]
> *   Sistema de *embeddings*: \[Doc3, Doc2, Doc1, Doc6, Doc7]
>
> **Calculando os Scores RRF Ponderados:**
>
> *   Doc1: 0.3/(60+1) + 0.7/(60+3) = 0.0049 + 0.0111 = 0.016
> *   Doc2: 0.3/(60+4) + 0.7/(60+2) = 0.0047 + 0.0113 = 0.016
> *   Doc3: 0.3/(60+2) + 0.7/(60+1) = 0.0048 + 0.0115 = 0.0163
> *   Doc4: 0.3/(60+5) + 0 = 0.0046
> *   Doc5: 0.3/(60+3) + 0 = 0.0048
> *   Doc6: 0 + 0.7/(60+4) = 0.0109
> *   Doc7: 0 + 0.7/(60+5) = 0.0108
>
> A lista reclassificada seria ent√£o: \[Doc3, Doc1, Doc2, Doc6, Doc7, Doc5, Doc4]. Observe que Doc3 continua sendo o primeiro, mas a ordem dos outros documentos mudou em rela√ß√£o ao RRF n√£o ponderado. Doc6 e Doc7, que apareciam apenas no sistema de embeddings (com maior peso), subiram na classifica√ß√£o em rela√ß√£o a Doc5 e Doc4, que apareciam apenas no sistema tf-idf (com menor peso).
>
>
> üí° **Exemplo Num√©rico: Avalia√ß√£o de Recupera√ß√£o**
>
> Suponha que temos um sistema de busca e, para uma determinada consulta, ele retorna os seguintes 10 documentos (ordenados por relev√¢ncia):
>
> \[Doc1 (R), Doc2 (N), Doc3 (R), Doc4 (N), Doc5 (N), Doc6 (R), Doc7 (N), Doc8 (N), Doc9 (R), Doc10 (N)]
>
> Onde "R" significa relevante e "N" significa n√£o relevante.  H√° um total de 5 documentos relevantes na cole√ß√£o para essa consulta.
>
> **Passo 1: Calcular Precis√£o@k e Recall@k para diferentes valores de k**
>
> *   Precis√£o@k = (N√∫mero de documentos relevantes nos top k) / k
> *   Recall@k = (N√∫mero de documentos relevantes nos top k) / (N√∫mero total de documentos relevantes na cole√ß√£o)
>
> | k   | Precis√£o@k | Recall@k |
> | --- | ----------- | -------- |
> | 1   | 1/1 = 1.0   | 1/5 = 0.2 |
> | 2   | 1/2 = 0.5   | 1/5 = 0.2 |
> | 3   | 2/3 ‚âà 0.67  | 2/5 = 0.4 |
> | 4   | 2/4 = 0.5   | 2/5 = 0.4 |
> | 5   | 2/5 = 0.4   | 2/5 = 0.4 |
> | 6   | 3/6 = 0.5   | 3/5 = 0.6 |
> | 7   | 3/7 ‚âà 0.43  | 3/5 = 0.6 |
> | 8   | 3/8 = 0.375 | 3/5 = 0.6 |
> | 9   | 4/9 ‚âà 0.44  | 4/5 = 0.8 |
> | 10  | 4/10 = 0.4  | 4/5 = 0.8 |
>
> **Passo 2: Calcular a M√©dia da Precis√£o (MAP)**
>
> A precis√£o √© calculada para cada documento relevante retornado e, em seguida, calcula-se a m√©dia dessas precis√µes.
>
> *   Documentos relevantes retornados: Doc1, Doc3, Doc6, Doc9
> *   Precis√£o para cada documento relevante:
>     *   Doc1: 1/1 = 1.0
>     *   Doc3: 2/3 ‚âà 0.67
>     *   Doc6: 3/6 = 0.5
>     *   Doc9: 4/9 ‚âà 0.44
> *   MAP = (1.0 + 0.67 + 0.5 + 0.44) / 5 = 2.61 / 5 ‚âà 0.52
>
> **Passo 3: Calcular o nDCG (Normalized Discounted Cumulative Gain)**
>
> Primeiro, calcular o DCG (Discounted Cumulative Gain):
>
> $$DCG = \sum_{i=1}^{k} \frac{rel_i}{\log_2(i+1)}$$
>
> Onde $rel_i$ √© a relev√¢ncia do documento na posi√ß√£o i (1 para relevante, 0 para n√£o relevante).
>
> $DCG = \frac{1}{\log_2(1+1)} + \frac{0}{\log_2(2+1)} + \frac{1}{\log_2(3+1)} + \frac{0}{\log_2(4+1)} + \frac{0}{\log_2(5+1)} + \frac{1}{\log_2(6+1)} + \frac{0}{\log_2(7+1)} + \frac{0}{\log_2(8+1)} + \frac{1}{\log_2(9+1)} + \frac{0}{\log_2(10+1)}$
>
> $DCG = \frac{1}{1} + \frac{1}{2} + \frac{1}{log_2(7)} + \frac{1}{log_2(10)}$
>
> $DCG \approx 1 + 0.5 + 0.36 + 0.30 = 2.16$
>
> Em seguida, calcular o IDCG (Ideal Discounted Cumulative Gain), que √© o DCG ideal, onde os documentos relevantes est√£o nas primeiras posi√ß√µes.
>
> $IDCG = \frac{1}{\log_2(1+1)} + \frac{1}{\log_2(2+1)} + \frac{1}{\log_2(3+1)} + \frac{1}{\log_2(4+1)} + \frac{1}{\log_2(5+1)} = 1 + 0.63 + 0.5 + 0.43 + 0.39 = 2.95$
>
> Ent√£o, calcular o nDCG:
>
> $nDCG = \frac{DCG}{IDCG} = \frac{2.16}{2.95} \approx 0.73$
>
> Estes s√£o exemplos de como as m√©tricas de precis√£o, recall, MAP e nDCG s√£o calculadas e usadas para avaliar o desempenho de um sistema de recupera√ß√£o de informa√ß√£o.  Valores mais altos indicam melhor desempenho do sistema.

**Corol√°rio 1.1** [RRF como Caso Especial do RRF Ponderado]
O RRF padr√£o √© um caso especial do RRF ponderado, onde todos os pesos $w_i$ s√£o iguais a 1.

![Diagram illustrating the Fusion Retrieval technique, combining keyword-based and semantic search for enhanced RAG.](./../images/image7.png)

### Conclus√£o

A recupera√ß√£o por fus√£o, implementada atrav√©s de t√©cnicas como o RRF, representa uma estrat√©gia eficaz para aprimorar a qualidade dos resultados de busca em sistemas de *Neural Information Retrieval* e RAG com LLMs [^1]. Ao combinar as for√ßas de diferentes abordagens de recupera√ß√£o, como a busca baseada em palavras-chave e a busca sem√¢ntica, e ao utilizar o RRF para reclassificar e integrar os resultados, √© poss√≠vel obter uma recupera√ß√£o mais robusta, precisa e relevante, melhorando, consequentemente, o desempenho dos LLMs em tarefas que dependem de recupera√ß√£o de informa√ß√£o. A escolha apropriada dos sistemas de busca a serem combinados e o ajuste do par√¢metro *k* no RRF s√£o aspectos importantes a serem considerados para otimizar o desempenho da recupera√ß√£o por fus√£o em aplica√ß√µes espec√≠ficas. Al√©m disso, a pondera√ß√£o dos sistemas de busca no RRF ponderado oferece uma flexibilidade adicional para refinar ainda mais os resultados da recupera√ß√£o.

### Refer√™ncias
[^1]: Informa√ß√£o extra√≠da do contexto fornecido.
<!-- END -->