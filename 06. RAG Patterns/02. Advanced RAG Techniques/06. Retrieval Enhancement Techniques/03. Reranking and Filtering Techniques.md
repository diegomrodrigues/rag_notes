## Reranking e Filtragem para Aprimoramento da Recupera√ß√£o em RAG

### Introdu√ß√£o

Em sistemas de Retrieval-Augmented Generation (RAG), a etapa de recupera√ß√£o √© crucial para fornecer ao Large Language Model (LLM) o contexto relevante para gerar respostas precisas e informativas. No entanto, a recupera√ß√£o inicial pode, por vezes, resultar em documentos ou trechos que n√£o s√£o totalmente relevantes ou que cont√™m informa√ß√µes redundantes. Para refinar os resultados da recupera√ß√£o inicial, t√©cnicas de **reranking** e **filtragem** s√£o aplicadas antes de alimentar o LLM com o contexto final [^3].

### Conceitos Fundamentais

**Filtragem** consiste em remover documentos ou trechos irrelevantes com base em crit√©rios predefinidos. Os crit√©rios podem incluir um limiar de similaridade, a presen√ßa ou aus√™ncia de palavras-chave espec√≠ficas, ou metadados associados aos documentos [^3]. A filtragem tem como objetivo eliminar o "ru√≠do" no contexto recuperado, melhorando a precis√£o e a relev√¢ncia da informa√ß√£o fornecida ao LLM.

**Reranking**, por outro lado, consiste em reordenar os documentos ou trechos recuperados com base em um novo crit√©rio de relev√¢ncia. Modelos de reranking, como LLMs ou *cross-encoders* baseados em *sentence-transformers*, s√£o frequentemente utilizados para avaliar a relev√¢ncia dos documentos em rela√ß√£o √† query do usu√°rio e reorden√°-los de acordo [^3]. O reranking visa priorizar os documentos mais relevantes, mesmo que n√£o tenham sido inicialmente classificados como os mais importantes pelo sistema de recupera√ß√£o inicial.

#### T√©cnicas de Filtragem

A filtragem pode ser implementada de diversas formas, dependendo das caracter√≠sticas dos documentos e da query do usu√°rio. Algumas t√©cnicas comuns incluem:

*   **Filtragem por similaridade:** Define-se um limiar de similaridade (e.g., baseado em similaridade de cossenos) e removem-se os documentos cuja similaridade com a query √© inferior a esse limiar [^3]. Esta t√©cnica √© √∫til para eliminar documentos que s√£o apenas superficialmente relacionados √† query.

> üí° **Exemplo Num√©rico:** Suponha que temos uma query e 3 documentos com as seguintes similaridades de cosseno: Documento 1: 0.85, Documento 2: 0.60, Documento 3: 0.30. Se definirmos um limiar de similaridade de 0.5, o Documento 3 seria filtrado, pois sua similaridade (0.30) √© inferior ao limiar.
>
> | Documento | Similaridade | Filtrado? |
> | --------- | ------------ | --------- |
> | 1         | 0.85         | N√£o       |
> | 2         | 0.60         | N√£o       |
> | 3         | 0.30         | Sim       |

*   **Filtragem por palavras-chave:** Requer a presen√ßa ou aus√™ncia de certas palavras-chave nos documentos. Por exemplo, se a query se refere a um t√≥pico espec√≠fico, a filtragem pode garantir que apenas documentos que mencionam esse t√≥pico sejam inclu√≠dos no contexto.

> üí° **Exemplo Num√©rico:** Uma query √© "impacto das mudan√ßas clim√°ticas na agricultura". Se a filtragem exigir a presen√ßa da palavra-chave "agricultura", documentos que n√£o mencionam "agricultura" ser√£o removidos. Suponha que temos 3 documentos. O Documento 1 cont√©m "agricultura", o Documento 2 cont√©m "mudan√ßas clim√°ticas e economia", e o Documento 3 cont√©m "agricultura sustent√°vel". O Documento 2 seria filtrado.
>
> | Documento | Conte√∫do                                  | Cont√©m "agricultura"? | Filtrado? |
> | --------- | ----------------------------------------- | --------------------- | --------- |
> | 1         | Impacto na agricultura                     | Sim                   | N√£o       |
> | 2         | Mudan√ßas clim√°ticas e economia             | N√£o                   | Sim       |
> | 3         | Agricultura sustent√°vel                    | Sim                   | N√£o       |

*   **Filtragem por metadados:** Utiliza informa√ß√µes adicionais associadas aos documentos, como data de publica√ß√£o, autor, categoria, etc., para filtrar os resultados. Por exemplo, pode-se filtrar documentos que s√£o muito antigos ou que pertencem a uma categoria irrelevante.

> üí° **Exemplo Num√©rico:** Uma query √© sobre "√∫ltimas pesquisas em IA". Se a filtragem for configurada para reter apenas documentos publicados nos √∫ltimos 2 anos, documentos mais antigos ser√£o filtrados. Suponha que a data atual √© 2024. O Documento 1 foi publicado em 2023, o Documento 2 em 2020, e o Documento 3 em 2024. O Documento 2 seria filtrado.
>
> | Documento | Data de Publica√ß√£o | Filtrado? |
> | --------- | ------------------ | --------- |
> | 1         | 2023               | N√£o       |
> | 2         | 2020               | Sim       |
> | 3         | 2024               | N√£o       |

Para complementar as t√©cnicas de filtragem, podemos considerar abordagens que combinam m√∫ltiplos crit√©rios.

**Proposi√ß√£o 1** (Filtragem H√≠brida): A filtragem pode ser aprimorada combinando m√∫ltiplos crit√©rios, como similaridade, palavras-chave e metadados, utilizando operadores l√≥gicos (AND, OR, NOT) ou fun√ß√µes de pondera√ß√£o.

*Exemplo:* Um filtro h√≠brido poderia selecionar documentos que (tenham uma similaridade acima de um limiar *OU* contenham uma palavra-chave espec√≠fica) *E* (tenham sido publicados em um per√≠odo recente).

Essa abordagem permite uma filtragem mais precisa e adaptada √†s necessidades espec√≠ficas da aplica√ß√£o.

#### T√©cnicas de Reranking

O reranking pode ser realizado utilizando diferentes tipos de modelos, cada um com suas vantagens e desvantagens:

*   **LLMs para reranking:** LLMs podem ser utilizados para avaliar a relev√¢ncia dos documentos em rela√ß√£o √† query, gerando um score de relev√¢ncia para cada documento. Esse score pode ser baseado na probabilidade de o LLM gerar a query a partir do documento, ou em outras m√©tricas de relev√¢ncia. Utilizar LLMs para reranking pode ser computacionalmente caro, mas pode resultar em melhorias significativas na qualidade do contexto fornecido ao LLM.

*   ***Cross-encoders* para reranking:** *Cross-encoders* baseados em *sentence-transformers* s√£o modelos treinados especificamente para avaliar a similaridade sem√¢ntica entre duas frases ou documentos. Eles processam a query e o documento simultaneamente, permitindo que capturem rela√ß√µes complexas entre eles. *Cross-encoders* s√£o geralmente mais eficientes do que LLMs para reranking, mas podem n√£o ser t√£o precisos em algumas situa√ß√µes.

    A arquitetura dos *cross-encoders* permite uma avalia√ß√£o contextualizada da relev√¢ncia. Ao contr√°rio dos *bi-encoders*, que codificam a query e o documento separadamente, o *cross-encoder* processa ambos em conjunto, permitindo que a aten√ß√£o do modelo se concentre nas intera√ß√µes entre as palavras e os conceitos presentes na query e no documento.

> üí° **Exemplo Num√©rico:** Suponha que a pontua√ß√£o inicial dada por um modelo de recupera√ß√£o (e.g., BM25) e a pontua√ß√£o ap√≥s o reranking com um Cross-Encoder s√£o apresentadas abaixo.
>
> Query: "Melhores restaurantes italianos em S√£o Paulo"
>
> | Documento | Pontua√ß√£o BM25 | Pontua√ß√£o Cross-Encoder | Ranking Inicial | Ranking Final |
> | --------- | --------------- | ----------------------- | --------------- | ------------- |
> | 1         | 0.8           | 0.95                    | 1               | 1             |
> | 2         | 0.75          | 0.80                    | 2               | 3             |
> | 3         | 0.7           | 0.85                    | 3               | 2             |
> | 4         | 0.65          | 0.70                    | 4               | 4             |
>
> Neste caso, o Cross-Encoder reordenou os Documentos 2 e 3 com base em uma avalia√ß√£o mais precisa da relev√¢ncia. Embora o Documento 2 tivesse uma pontua√ß√£o inicial ligeiramente superior no BM25, o Cross-Encoder reconheceu que o Documento 3 era mais relevante para a query.

Al√©m dos m√©todos j√° apresentados, podemos considerar o uso de modelos de *rank aggregation* para combinar diferentes scores de relev√¢ncia.

**Teorema 2** (Rank Aggregation): Diferentes modelos de reranking podem gerar scores de relev√¢ncia distintos. Combinar esses scores usando t√©cnicas de *rank aggregation* (e.g., Borda count, Markov chain ranking) pode resultar em um reranking mais robusto e preciso.

*Estrat√©gia de Prova:* A ideia √© que diferentes modelos capturam diferentes aspectos da relev√¢ncia. Ao combinar seus scores, podemos mitigar os vieses de cada modelo individual e obter uma avalia√ß√£o mais abrangente. T√©cnicas como Borda count atribuem pontos a cada documento com base em sua posi√ß√£o no ranking de cada modelo, e o ranking final √© determinado pela soma dos pontos.

> üí° **Exemplo Num√©rico:** Vamos supor que temos dois modelos de reranking: um Cross-Encoder e um LLM. Queremos combinar os resultados usando Borda Count.
>
> | Documento | Ranking Cross-Encoder | Ranking LLM | Pontos (Cross-Encoder) | Pontos (LLM) | Pontua√ß√£o Total | Ranking Final |
> | --------- | --------------------- | ----------- | ---------------------- | ------------- | --------------- | ------------- |
> | 1         | 1                     | 2           | 4                      | 3             | 7               | 1             |
> | 2         | 2                     | 1           | 3                      | 4             | 7               | 1             |
> | 3         | 3                     | 3           | 2                      | 2             | 4               | 3             |
> | 4         | 4                     | 4           | 1                      | 1             | 2               | 4             |
>
> Neste exemplo, cada modelo fornece um ranking para os documentos. Com Borda Count, o documento na posi√ß√£o 1 recebe 4 pontos, o da posi√ß√£o 2 recebe 3 pontos, e assim por diante. Os pontos s√£o somados para cada documento, e o ranking final √© determinado pela pontua√ß√£o total. Documentos 1 e 2 empatam, ent√£o a ordem √© mantida da pontua√ß√£o inicial.
>

#### LlamaIndex e Postprocessors

LlamaIndex oferece uma variedade de *postprocessors* para implementar t√©cnicas de filtragem e reranking [^3]. Esses *postprocessors* podem ser facilmente integrados ao pipeline de RAG, permitindo que os desenvolvedores personalizem o processo de recupera√ß√£o de acordo com suas necessidades espec√≠ficas.

Por exemplo, LlamaIndex oferece *postprocessors* para filtrar documentos com base em um limiar de similaridade, reranquear documentos utilizando um *cross-encoder* pr√©-treinado, ou combinar diferentes t√©cnicas de filtragem e reranking.

Para facilitar a escolha e configura√ß√£o dos *postprocessors*, √© √∫til categoriz√°-los e fornecer exemplos de uso.

**Lema 3** (Categoriza√ß√£o de Postprocessors): Os *postprocessors* do LlamaIndex podem ser categorizados com base em sua funcionalidade principal (filtragem, reranking, combina√ß√£o) e nos crit√©rios que utilizam (similaridade, palavras-chave, metadados, modelos de linguagem).

Essa categoriza√ß√£o facilita a identifica√ß√£o do *postprocessor* mais adequado para cada caso de uso. Al√©m disso, exemplos de configura√ß√£o e uso de cada *postprocessor* podem auxiliar os desenvolvedores na implementa√ß√£o das t√©cnicas de filtragem e reranking.

#### Considera√ß√µes e Trade-offs

A escolha das t√©cnicas de filtragem e reranking e seus par√¢metros (e.g., o limiar de similaridade) deve ser feita com cuidado, considerando os *trade-offs* entre precis√£o, recall e efici√™ncia computacional.

*   **Precis√£o** refere-se √† propor√ß√£o de documentos relevantes entre os documentos recuperados.
*   **Recall** refere-se √† propor√ß√£o de documentos relevantes que foram recuperados.
*   **Efici√™ncia computacional** refere-se ao tempo e aos recursos necess√°rios para realizar a filtragem e o reranking.

Um limiar de similaridade muito alto, por exemplo, pode aumentar a precis√£o, mas reduzir o recall, pois alguns documentos relevantes podem ser filtrados. Da mesma forma, o uso de um LLM para reranking pode melhorar a precis√£o, mas aumentar significativamente o tempo de processamento.

> üí° **Exemplo Num√©rico:** Imagine que voc√™ tem um sistema de busca de artigos cient√≠ficos.
>
> *   **Cen√°rio 1: Limiar de similaridade alto (0.9)**
>     *   Precis√£o: 95% (quase todos os artigos retornados s√£o relevantes)
>     *   Recall: 50% (metade dos artigos relevantes existentes n√£o s√£o encontrados)
>     *   Interpreta√ß√£o: O sistema √© muito preciso, mas perde muitos artigos importantes.
> *   **Cen√°rio 2: Limiar de similaridade baixo (0.3)**
>     *   Precis√£o: 60% (muitos artigos irrelevantes s√£o retornados)
>     *   Recall: 90% (quase todos os artigos relevantes s√£o encontrados)
>     *   Interpreta√ß√£o: O sistema encontra quase todos os artigos relevantes, mas inclui muitos artigos desnecess√°rios.
> *   **Cen√°rio 3: Reranking com LLM**
>     *   Precis√£o: 85% (melhora em rela√ß√£o ao cen√°rio 2)
>     *   Recall: 80% (perda pequena em rela√ß√£o ao cen√°rio 2, mas melhor precis√£o)
>     *   Tempo de resposta: Aumenta em 2 segundos por query (trade-off com efici√™ncia)
>     *   Interpreta√ß√£o: O LLM consegue priorizar os artigos mais relevantes, melhorando a precis√£o sem perder muito recall, mas aumenta o tempo de processamento.

A otimiza√ß√£o das t√©cnicas de filtragem e reranking requer uma an√°lise cuidadosa dos dados e experimentos para encontrar o equil√≠brio ideal entre precis√£o, recall e efici√™ncia computacional.

Para auxiliar na otimiza√ß√£o, podemos introduzir o conceito de m√©tricas compostas que consideram m√∫ltiplos aspectos do desempenho.

**Defini√ß√£o 4** (M√©trica F-beta para RAG): A m√©trica F-beta (FŒ≤) pode ser adaptada para avaliar o desempenho de sistemas RAG, ponderando a import√¢ncia da precis√£o e do recall.

$$F_\beta = (1 + \beta^2) \cdot \frac{\text{Precis√£o} \cdot \text{Recall}}{(\beta^2 \cdot \text{Precis√£o}) + \text{Recall}}$$

O par√¢metro Œ≤ controla o peso relativo da precis√£o e do recall. Um valor de Œ≤ > 1 enfatiza o recall, enquanto um valor de Œ≤ < 1 enfatiza a precis√£o. Essa m√©trica permite avaliar o impacto das t√©cnicas de filtragem e reranking no desempenho geral do sistema RAG.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos dois sistemas RAG, A e B, com os seguintes resultados:
>
> *   Sistema A: Precis√£o = 0.8, Recall = 0.6
> *   Sistema B: Precis√£o = 0.6, Recall = 0.8
>
> Queremos avaliar qual sistema √© melhor usando a m√©trica F-beta com Œ≤ = 0.5 (√™nfase na precis√£o) e Œ≤ = 2 (√™nfase no recall).
>
> *   **Para Œ≤ = 0.5:**
>
>     $F_{0.5}(A) = (1 + 0.5^2) \cdot \frac{0.8 \cdot 0.6}{(0.5^2 \cdot 0.8) + 0.6} = 1.25 \cdot \frac{0.48}{0.2 + 0.6} = 1.25 \cdot \frac{0.48}{0.8} = 0.75$
>
>     $F_{0.5}(B) = (1 + 0.5^2) \cdot \frac{0.6 \cdot 0.8}{(0.5^2 \cdot 0.6) + 0.8} = 1.25 \cdot \frac{0.48}{0.15 + 0.8} = 1.25 \cdot \frac{0.48}{0.95} = 0.63$
>
>     Neste caso, o Sistema A tem um F-beta maior (0.75) que o Sistema B (0.63), indicando que √© melhor quando a precis√£o √© mais importante.
> *   **Para Œ≤ = 2:**
>
>     $F_{2}(A) = (1 + 2^2) \cdot \frac{0.8 \cdot 0.6}{(2^2 \cdot 0.8) + 0.6} = 5 \cdot \frac{0.48}{3.2 + 0.6} = 5 \cdot \frac{0.48}{3.8} = 0.63$
>
>     $F_{2}(B) = (1 + 2^2) \cdot \frac{0.6 \cdot 0.8}{(2^2 \cdot 0.6) + 0.8} = 5 \cdot \frac{0.48}{2.4 + 0.8} = 5 \cdot \frac{0.48}{3.2} = 0.75$
>
>     Neste caso, o Sistema B tem um F-beta maior (0.75) que o Sistema A (0.63), indicando que √© melhor quando o recall √© mais importante.

### Conclus√£o

As t√©cnicas de reranking e filtragem representam um passo crucial no aprimoramento da recupera√ß√£o em sistemas RAG. Ao refinar os resultados da recupera√ß√£o inicial, essas t√©cnicas garantem que o LLM receba um contexto mais relevante e preciso, resultando em respostas mais informativas e confi√°veis [^3]. A escolha das t√©cnicas e par√¢metros adequados depende das caracter√≠sticas espec√≠ficas dos dados e da aplica√ß√£o, e requer uma an√°lise cuidadosa dos *trade-offs* envolvidos. LlamaIndex oferece uma variedade de ferramentas para facilitar a implementa√ß√£o e personaliza√ß√£o dessas t√©cnicas, permitindo que os desenvolvedores otimizem o processo de recupera√ß√£o para suas necessidades espec√≠ficas.

### Refer√™ncias

[^3]: Contexto fornecido: Reranking and filtering refine initial retrieval results through techniques like filtering based on similarity score, keywords, or metadata, and reranking them with models such as LLMs or sentence-transformer cross-encoders. LlamaIndex offers a variety of postprocessors for these tasks. This is the final step before feeding the refined context to the LLM.
<!-- END -->