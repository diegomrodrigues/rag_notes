## Reranking e Filtragem para Aprimoramento da RecuperaÃ§Ã£o em RAG

### IntroduÃ§Ã£o

Em sistemas de Retrieval-Augmented Generation (RAG), a etapa de recuperaÃ§Ã£o Ã© crucial para fornecer ao Large Language Model (LLM) o contexto relevante para gerar respostas precisas e informativas. No entanto, a recuperaÃ§Ã£o inicial pode, por vezes, resultar em documentos ou trechos que nÃ£o sÃ£o totalmente relevantes ou que contÃªm informaÃ§Ãµes redundantes. Para refinar os resultados da recuperaÃ§Ã£o inicial, tÃ©cnicas de **reranking** e **filtragem** sÃ£o aplicadas antes de alimentar o LLM com o contexto final [^3].

### Conceitos Fundamentais

**Filtragem** consiste em remover documentos ou trechos irrelevantes com base em critÃ©rios predefinidos. Os critÃ©rios podem incluir um limiar de similaridade, a presenÃ§a ou ausÃªncia de palavras-chave especÃ­ficas, ou metadados associados aos documentos [^3]. A filtragem tem como objetivo eliminar o "ruÃ­do" no contexto recuperado, melhorando a precisÃ£o e a relevÃ¢ncia da informaÃ§Ã£o fornecida ao LLM.

**Reranking**, por outro lado, consiste em reordenar os documentos ou trechos recuperados com base em um novo critÃ©rio de relevÃ¢ncia. Modelos de reranking, como LLMs ou *cross-encoders* baseados em *sentence-transformers*, sÃ£o frequentemente utilizados para avaliar a relevÃ¢ncia dos documentos em relaÃ§Ã£o Ã  query do usuÃ¡rio e reordenÃ¡-los de acordo [^3]. O reranking visa priorizar os documentos mais relevantes, mesmo que nÃ£o tenham sido inicialmente classificados como os mais importantes pelo sistema de recuperaÃ§Ã£o inicial.

#### TÃ©cnicas de Filtragem

A filtragem pode ser implementada de diversas formas, dependendo das caracterÃ­sticas dos documentos e da query do usuÃ¡rio. Algumas tÃ©cnicas comuns incluem:

*   **Filtragem por similaridade:** Define-se um limiar de similaridade (e.g., baseado em similaridade de cossenos) e removem-se os documentos cuja similaridade com a query Ã© inferior a esse limiar [^3]. Esta tÃ©cnica Ã© Ãºtil para eliminar documentos que sÃ£o apenas superficialmente relacionados Ã  query.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos uma query e 3 documentos com as seguintes similaridades de cosseno: Documento 1: 0.85, Documento 2: 0.60, Documento 3: 0.30. Se definirmos um limiar de similaridade de 0.5, o Documento 3 seria filtrado, pois sua similaridade (0.30) Ã© inferior ao limiar.
>
> | Documento | Similaridade | Filtrado? |
> | --------- | ------------ | --------- |
> | 1         | 0.85         | NÃ£o       |
> | 2         | 0.60         | NÃ£o       |
> | 3         | 0.30         | Sim       |

*   **Filtragem por palavras-chave:** Requer a presenÃ§a ou ausÃªncia de certas palavras-chave nos documentos. Por exemplo, se a query se refere a um tÃ³pico especÃ­fico, a filtragem pode garantir que apenas documentos que mencionam esse tÃ³pico sejam incluÃ­dos no contexto.

> ðŸ’¡ **Exemplo NumÃ©rico:** Uma query Ã© "impacto das mudanÃ§as climÃ¡ticas na agricultura". Se a filtragem exigir a presenÃ§a da palavra-chave "agricultura", documentos que nÃ£o mencionam "agricultura" serÃ£o removidos. Suponha que temos 3 documentos. O Documento 1 contÃ©m "agricultura", o Documento 2 contÃ©m "mudanÃ§as climÃ¡ticas e economia", e o Documento 3 contÃ©m "agricultura sustentÃ¡vel". O Documento 2 seria filtrado.
>
> | Documento | ConteÃºdo                                  | ContÃ©m "agricultura"? | Filtrado? |
> | --------- | ----------------------------------------- | --------------------- | --------- |
> | 1         | Impacto na agricultura                     | Sim                   | NÃ£o       |
> | 2         | MudanÃ§as climÃ¡ticas e economia             | NÃ£o                   | Sim       |
> | 3         | Agricultura sustentÃ¡vel                    | Sim                   | NÃ£o       |

*   **Filtragem por metadados:** Utiliza informaÃ§Ãµes adicionais associadas aos documentos, como data de publicaÃ§Ã£o, autor, categoria, etc., para filtrar os resultados. Por exemplo, pode-se filtrar documentos que sÃ£o muito antigos ou que pertencem a uma categoria irrelevante.

> ðŸ’¡ **Exemplo NumÃ©rico:** Uma query Ã© sobre "Ãºltimas pesquisas em IA". Se a filtragem for configurada para reter apenas documentos publicados nos Ãºltimos 2 anos, documentos mais antigos serÃ£o filtrados. Suponha que a data atual Ã© 2024. O Documento 1 foi publicado em 2023, o Documento 2 em 2020, e o Documento 3 em 2024. O Documento 2 seria filtrado.
>
> | Documento | Data de PublicaÃ§Ã£o | Filtrado? |
> | --------- | ------------------ | --------- |
> | 1         | 2023               | NÃ£o       |
> | 2         | 2020               | Sim       |
> | 3         | 2024               | NÃ£o       |

Para complementar as tÃ©cnicas de filtragem, podemos considerar abordagens que combinam mÃºltiplos critÃ©rios.

**ProposiÃ§Ã£o 1** (Filtragem HÃ­brida): A filtragem pode ser aprimorada combinando mÃºltiplos critÃ©rios, como similaridade, palavras-chave e metadados, utilizando operadores lÃ³gicos (AND, OR, NOT) ou funÃ§Ãµes de ponderaÃ§Ã£o.

*Exemplo:* Um filtro hÃ­brido poderia selecionar documentos que (tenham uma similaridade acima de um limiar *OU* contenham uma palavra-chave especÃ­fica) *E* (tenham sido publicados em um perÃ­odo recente).

Essa abordagem permite uma filtragem mais precisa e adaptada Ã s necessidades especÃ­ficas da aplicaÃ§Ã£o.

#### TÃ©cnicas de Reranking

O reranking pode ser realizado utilizando diferentes tipos de modelos, cada um com suas vantagens e desvantagens:

*   **LLMs para reranking:** LLMs podem ser utilizados para avaliar a relevÃ¢ncia dos documentos em relaÃ§Ã£o Ã  query, gerando um score de relevÃ¢ncia para cada documento. Esse score pode ser baseado na probabilidade de o LLM gerar a query a partir do documento, ou em outras mÃ©tricas de relevÃ¢ncia. Utilizar LLMs para reranking pode ser computacionalmente caro, mas pode resultar em melhorias significativas na qualidade do contexto fornecido ao LLM.

*   ***Cross-encoders* para reranking:** *Cross-encoders* baseados em *sentence-transformers* sÃ£o modelos treinados especificamente para avaliar a similaridade semÃ¢ntica entre duas frases ou documentos. Eles processam a query e o documento simultaneamente, permitindo que capturem relaÃ§Ãµes complexas entre eles. *Cross-encoders* sÃ£o geralmente mais eficientes do que LLMs para reranking, mas podem nÃ£o ser tÃ£o precisos em algumas situaÃ§Ãµes.

    A arquitetura dos *cross-encoders* permite uma avaliaÃ§Ã£o contextualizada da relevÃ¢ncia. Ao contrÃ¡rio dos *bi-encoders*, que codificam a query e o documento separadamente, o *cross-encoder* processa ambos em conjunto, permitindo que a atenÃ§Ã£o do modelo se concentre nas interaÃ§Ãµes entre as palavras e os conceitos presentes na query e no documento.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que a pontuaÃ§Ã£o inicial dada por um modelo de recuperaÃ§Ã£o (e.g., BM25) e a pontuaÃ§Ã£o apÃ³s o reranking com um Cross-Encoder sÃ£o apresentadas abaixo.
>
> Query: "Melhores restaurantes italianos em SÃ£o Paulo"
>
> | Documento | PontuaÃ§Ã£o BM25 | PontuaÃ§Ã£o Cross-Encoder | Ranking Inicial | Ranking Final |
> | --------- | --------------- | ----------------------- | --------------- | ------------- |
> | 1         | 0.8           | 0.95                    | 1               | 1             |
> | 2         | 0.75          | 0.80                    | 2               | 3             |
> | 3         | 0.7           | 0.85                    | 3               | 2             |
> | 4         | 0.65          | 0.70                    | 4               | 4             |
>
> Neste caso, o Cross-Encoder reordenou os Documentos 2 e 3 com base em uma avaliaÃ§Ã£o mais precisa da relevÃ¢ncia. Embora o Documento 2 tivesse uma pontuaÃ§Ã£o inicial ligeiramente superior no BM25, o Cross-Encoder reconheceu que o Documento 3 era mais relevante para a query.

AlÃ©m dos mÃ©todos jÃ¡ apresentados, podemos considerar o uso de modelos de *rank aggregation* para combinar diferentes scores de relevÃ¢ncia.

**Teorema 2** (Rank Aggregation): Diferentes modelos de reranking podem gerar scores de relevÃ¢ncia distintos. Combinar esses scores usando tÃ©cnicas de *rank aggregation* (e.g., Borda count, Markov chain ranking) pode resultar em um reranking mais robusto e preciso.

*EstratÃ©gia de Prova:* A ideia Ã© que diferentes modelos capturam diferentes aspectos da relevÃ¢ncia. Ao combinar seus scores, podemos mitigar os vieses de cada modelo individual e obter uma avaliaÃ§Ã£o mais abrangente. TÃ©cnicas como Borda count atribuem pontos a cada documento com base em sua posiÃ§Ã£o no ranking de cada modelo, e o ranking final Ã© determinado pela soma dos pontos.

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos supor que temos dois modelos de reranking: um Cross-Encoder e um LLM. Queremos combinar os resultados usando Borda Count.
>
> | Documento | Ranking Cross-Encoder | Ranking LLM | Pontos (Cross-Encoder) | Pontos (LLM) | PontuaÃ§Ã£o Total | Ranking Final |
> | --------- | --------------------- | ----------- | ---------------------- | ------------- | --------------- | ------------- |
> | 1         | 1                     | 2           | 4                      | 3             | 7               | 1             |
> | 2         | 2                     | 1           | 3                      | 4             | 7               | 1             |
> | 3         | 3                     | 3           | 2                      | 2             | 4               | 3             |
> | 4         | 4                     | 4           | 1                      | 1             | 2               | 4             |
>
> Neste exemplo, cada modelo fornece um ranking para os documentos. Com Borda Count, o documento na posiÃ§Ã£o 1 recebe 4 pontos, o da posiÃ§Ã£o 2 recebe 3 pontos, e assim por diante. Os pontos sÃ£o somados para cada documento, e o ranking final Ã© determinado pela pontuaÃ§Ã£o total. Documentos 1 e 2 empatam, entÃ£o a ordem Ã© mantida da pontuaÃ§Ã£o inicial.
>

#### LlamaIndex e Postprocessors

LlamaIndex oferece uma variedade de *postprocessors* para implementar tÃ©cnicas de filtragem e reranking [^3]. Esses *postprocessors* podem ser facilmente integrados ao pipeline de RAG, permitindo que os desenvolvedores personalizem o processo de recuperaÃ§Ã£o de acordo com suas necessidades especÃ­ficas.

Por exemplo, LlamaIndex oferece *postprocessors* para filtrar documentos com base em um limiar de similaridade, reranquear documentos utilizando um *cross-encoder* prÃ©-treinado, ou combinar diferentes tÃ©cnicas de filtragem e reranking.

Para facilitar a escolha e configuraÃ§Ã£o dos *postprocessors*, Ã© Ãºtil categorizÃ¡-los e fornecer exemplos de uso.

**Lema 3** (CategorizaÃ§Ã£o de Postprocessors): Os *postprocessors* do LlamaIndex podem ser categorizados com base em sua funcionalidade principal (filtragem, reranking, combinaÃ§Ã£o) e nos critÃ©rios que utilizam (similaridade, palavras-chave, metadados, modelos de linguagem).

Essa categorizaÃ§Ã£o facilita a identificaÃ§Ã£o do *postprocessor* mais adequado para cada caso de uso. AlÃ©m disso, exemplos de configuraÃ§Ã£o e uso de cada *postprocessor* podem auxiliar os desenvolvedores na implementaÃ§Ã£o das tÃ©cnicas de filtragem e reranking.

#### ConsideraÃ§Ãµes e Trade-offs

A escolha das tÃ©cnicas de filtragem e reranking e seus parÃ¢metros (e.g., o limiar de similaridade) deve ser feita com cuidado, considerando os *trade-offs* entre precisÃ£o, recall e eficiÃªncia computacional.

*   **PrecisÃ£o** refere-se Ã  proporÃ§Ã£o de documentos relevantes entre os documentos recuperados.
*   **Recall** refere-se Ã  proporÃ§Ã£o de documentos relevantes que foram recuperados.
*   **EficiÃªncia computacional** refere-se ao tempo e aos recursos necessÃ¡rios para realizar a filtragem e o reranking.

Um limiar de similaridade muito alto, por exemplo, pode aumentar a precisÃ£o, mas reduzir o recall, pois alguns documentos relevantes podem ser filtrados. Da mesma forma, o uso de um LLM para reranking pode melhorar a precisÃ£o, mas aumentar significativamente o tempo de processamento.

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine que vocÃª tem um sistema de busca de artigos cientÃ­ficos.
>
> *   **CenÃ¡rio 1: Limiar de similaridade alto (0.9)**
>     *   PrecisÃ£o: 95% (quase todos os artigos retornados sÃ£o relevantes)
>     *   Recall: 50% (metade dos artigos relevantes existentes nÃ£o sÃ£o encontrados)
>     *   InterpretaÃ§Ã£o: O sistema Ã© muito preciso, mas perde muitos artigos importantes.
> *   **CenÃ¡rio 2: Limiar de similaridade baixo (0.3)**
>     *   PrecisÃ£o: 60% (muitos artigos irrelevantes sÃ£o retornados)
>     *   Recall: 90% (quase todos os artigos relevantes sÃ£o encontrados)
>     *   InterpretaÃ§Ã£o: O sistema encontra quase todos os artigos relevantes, mas inclui muitos artigos desnecessÃ¡rios.
> *   **CenÃ¡rio 3: Reranking com LLM**
>     *   PrecisÃ£o: 85% (melhora em relaÃ§Ã£o ao cenÃ¡rio 2)
>     *   Recall: 80% (perda pequena em relaÃ§Ã£o ao cenÃ¡rio 2, mas melhor precisÃ£o)
>     *   Tempo de resposta: Aumenta em 2 segundos por query (trade-off com eficiÃªncia)
>     *   InterpretaÃ§Ã£o: O LLM consegue priorizar os artigos mais relevantes, melhorando a precisÃ£o sem perder muito recall, mas aumenta o tempo de processamento.

A otimizaÃ§Ã£o das tÃ©cnicas de filtragem e reranking requer uma anÃ¡lise cuidadosa dos dados e experimentos para encontrar o equilÃ­brio ideal entre precisÃ£o, recall e eficiÃªncia computacional.

Para auxiliar na otimizaÃ§Ã£o, podemos introduzir o conceito de mÃ©tricas compostas que consideram mÃºltiplos aspectos do desempenho.

**DefiniÃ§Ã£o 4** (MÃ©trica F-beta para RAG): A mÃ©trica F-beta (FÎ²) pode ser adaptada para avaliar o desempenho de sistemas RAG, ponderando a importÃ¢ncia da precisÃ£o e do recall.

$$F_\beta = (1 + \beta^2) \cdot \frac{\text{PrecisÃ£o} \cdot \text{Recall}}{(\beta^2 \cdot \text{PrecisÃ£o}) + \text{Recall}}$$

O parÃ¢metro Î² controla o peso relativo da precisÃ£o e do recall. Um valor de Î² > 1 enfatiza o recall, enquanto um valor de Î² < 1 enfatiza a precisÃ£o. Essa mÃ©trica permite avaliar o impacto das tÃ©cnicas de filtragem e reranking no desempenho geral do sistema RAG.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos dois sistemas RAG, A e B, com os seguintes resultados:
>
> *   Sistema A: PrecisÃ£o = 0.8, Recall = 0.6
> *   Sistema B: PrecisÃ£o = 0.6, Recall = 0.8
>
> Queremos avaliar qual sistema Ã© melhor usando a mÃ©trica F-beta com Î² = 0.5 (Ãªnfase na precisÃ£o) e Î² = 2 (Ãªnfase no recall).
>
> *   **Para Î² = 0.5:**
>
>     $F_{0.5}(A) = (1 + 0.5^2) \cdot \frac{0.8 \cdot 0.6}{(0.5^2 \cdot 0.8) + 0.6} = 1.25 \cdot \frac{0.48}{0.2 + 0.6} = 1.25 \cdot \frac{0.48}{0.8} = 0.75$
>
>     $F_{0.5}(B) = (1 + 0.5^2) \cdot \frac{0.6 \cdot 0.8}{(0.5^2 \cdot 0.6) + 0.8} = 1.25 \cdot \frac{0.48}{0.15 + 0.8} = 1.25 \cdot \frac{0.48}{0.95} = 0.63$
>
>     Neste caso, o Sistema A tem um F-beta maior (0.75) que o Sistema B (0.63), indicando que Ã© melhor quando a precisÃ£o Ã© mais importante.
> *   **Para Î² = 2:**
>
>     $F_{2}(A) = (1 + 2^2) \cdot \frac{0.8 \cdot 0.6}{(2^2 \cdot 0.8) + 0.6} = 5 \cdot \frac{0.48}{3.2 + 0.6} = 5 \cdot \frac{0.48}{3.8} = 0.63$
>
>     $F_{2}(B) = (1 + 2^2) \cdot \frac{0.6 \cdot 0.8}{(2^2 \cdot 0.6) + 0.8} = 5 \cdot \frac{0.48}{2.4 + 0.8} = 5 \cdot \frac{0.48}{3.2} = 0.75$
>
>     Neste caso, o Sistema B tem um F-beta maior (0.75) que o Sistema A (0.63), indicando que Ã© melhor quando o recall Ã© mais importante.

### ConclusÃ£o

As tÃ©cnicas de reranking e filtragem representam um passo crucial no aprimoramento da recuperaÃ§Ã£o em sistemas RAG. Ao refinar os resultados da recuperaÃ§Ã£o inicial, essas tÃ©cnicas garantem que o LLM receba um contexto mais relevante e preciso, resultando em respostas mais informativas e confiÃ¡veis [^3]. A escolha das tÃ©cnicas e parÃ¢metros adequados depende das caracterÃ­sticas especÃ­ficas dos dados e da aplicaÃ§Ã£o, e requer uma anÃ¡lise cuidadosa dos *trade-offs* envolvidos. LlamaIndex oferece uma variedade de ferramentas para facilitar a implementaÃ§Ã£o e personalizaÃ§Ã£o dessas tÃ©cnicas, permitindo que os desenvolvedores otimizem o processo de recuperaÃ§Ã£o para suas necessidades especÃ­ficas.

### ReferÃªncias

[^3]: Contexto fornecido: Reranking and filtering refine initial retrieval results through techniques like filtering based on similarity score, keywords, or metadata, and reranking them with models such as LLMs or sentence-transformer cross-encoders. LlamaIndex offers a variety of postprocessors for these tasks. This is the final step before feeding the refined context to the LLM.
<!-- END -->