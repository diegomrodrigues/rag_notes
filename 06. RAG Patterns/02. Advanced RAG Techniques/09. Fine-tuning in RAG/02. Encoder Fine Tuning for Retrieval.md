## Fine-tuning de Encoders e Rankers para Melhoria da Qualidade de Recupera√ß√£o em RAG

### Introdu√ß√£o

Este cap√≠tulo aprofunda o tema do fine-tuning em RAG (Retrieval-Augmented Generation), com foco espec√≠fico no fine-tuning de encoders e rankers para aprimorar a qualidade da recupera√ß√£o, particularmente em datasets de dom√≠nio restrito. Exploraremos como o fine-tuning de encoders impacta a performance de retrieval e como o fine-tuning de rankers, utilizando cross-encoders, pode refinar os resultados recuperados por meio da melhoria de scores pairwise.

### Fine-tuning de Encoders

O fine-tuning de encoders em sistemas RAG visa adaptar o modelo de embedding para representar melhor a sem√¢ntica espec√≠fica do dom√≠nio de interesse. Essa adapta√ß√£o √© crucial, especialmente quando lidamos com datasets de dom√≠nio restrito, onde as nuances da linguagem e os termos t√©cnicos espec√≠ficos podem n√£o ser adequadamente capturados por modelos pr√©-treinados em corpora mais gerais [^2].

O processo de fine-tuning envolve ajustar os pesos do encoder utilizando um dataset de treinamento espec√≠fico do dom√≠nio. Este dataset pode ser constru√≠do de diversas formas, como por meio de anota√ß√µes manuais, heur√≠sticas ou t√©cnicas de data augmentation. O objetivo √© ensinar ao encoder a associar consultas (queries) a documentos relevantes de maneira mais precisa.

Formalmente, podemos expressar o objetivo do fine-tuning como a minimiza√ß√£o de uma fun√ß√£o de perda $\mathcal{L}$ que quantifica a diferen√ßa entre as representa√ß√µes (embeddings) geradas pelo encoder para consultas e documentos relevantes. Por exemplo, uma fun√ß√£o de perda comum √© a *contrastive loss*, que visa aproximar os embeddings de pares consulta-documento relevantes e afastar os embeddings de pares consulta-documento irrelevantes:

$$
\mathcal{L} = \sum_{(q, d^+)} \sum_{d^-} \max(0, m - s(q, d^+) + s(q, d^-))
$$

Onde:

*   $q$ representa uma consulta.
*   $d^+$ representa um documento relevante para a consulta $q$.
*   $d^-$ representa um documento irrelevante para a consulta $q$.
*   $s(q, d)$ √© uma fun√ß√£o de similaridade (e.g., cosine similarity) entre os embeddings da consulta $q$ e do documento $d$.
*   $m$ √© um par√¢metro de margem que controla a separa√ß√£o desejada entre os scores de pares relevantes e irrelevantes.

A escolha da fun√ß√£o de perda e do dataset de treinamento s√£o fatores cr√≠ticos para o sucesso do fine-tuning. √â importante considerar a natureza do dom√≠nio e as caracter√≠sticas dos dados ao definir esses elementos.

**Exemplo:** Suponha que estejamos trabalhando com um sistema RAG para recupera√ß√£o de informa√ß√µes em documentos jur√≠dicos. Um encoder pr√©-treinado pode ter dificuldade em distinguir entre diferentes tipos de contratos ou em identificar a relev√¢ncia de um documento para uma consulta espec√≠fica sobre um caso legal. Ao fine-tunear o encoder com um dataset de pares consulta-documento jur√≠dico, podemos ensin√°-lo a capturar as nuances da linguagem jur√≠dica e a representar melhor a sem√¢ntica dos documentos nesse dom√≠nio.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma consulta $q$ = "cl√°usula de rescis√£o contratual" e dois documentos:
>
> *   $d^+$ = "O presente contrato poder√° ser rescindido por qualquer das partes..." (documento relevante)
> *   $d^-$ = "O imposto sobre produtos industrializados incide sobre..." (documento irrelevante)
>
> Antes do fine-tuning, os embeddings gerados pelo encoder poderiam resultar em uma similaridade $s(q, d^+) = 0.6$ e $s(q, d^-) = 0.5$. Ap√≥s o fine-tuning, espera-se que a similaridade com o documento relevante aumente e a similaridade com o documento irrelevante diminua.
>
> Assumindo uma margem $m = 0.1$, a *contrastive loss* antes do fine-tuning seria:
>
> $$\mathcal{L} = \max(0, 0.1 - 0.6 + 0.5) = \max(0, 0) = 0$$
>
> Ap√≥s o fine-tuning, com embeddings ajustados, poder√≠amos ter $s(q, d^+) = 0.8$ e $s(q, d^-) = 0.2$. A *contrastive loss* seria ent√£o:
>
> $$\mathcal{L} = \max(0, 0.1 - 0.8 + 0.2) = \max(0, -0.5) = 0$$
>
> No entanto, durante o processo de treinamento, se, em um determinado momento, $s(q, d^+) = 0.4$ e $s(q, d^-) = 0.7$, ent√£o a loss seria:
>
> $$\mathcal{L} = \max(0, 0.1 - 0.4 + 0.7) = \max(0, 0.4) = 0.4$$
>
> O objetivo do fine-tuning √© minimizar essa loss, ajustando os pesos do encoder para que ele produza embeddings que reflitam melhor a relev√¢ncia entre consultas e documentos.

**Proposi√ß√£o 1:** Uma alternativa √† *contrastive loss* √© a *triplet loss*, que considera triplets de consulta, documento relevante e documento irrelevante simultaneamente.

<!-- NEW CONTENT -->
A *triplet loss* √© definida como:

$$
\mathcal{L} = \sum_{(q, d^+, d^-)} \max(0, m - s(q, d^+) + s(q, d^-))
$$

Onde os termos s√£o definidos como anteriormente. A diferen√ßa principal reside na forma como os dados s√£o amostrados para o treinamento. A *triplet loss* explora diretamente a rela√ß√£o entre um exemplo √¢ncora (a consulta), um exemplo positivo (documento relevante) e um exemplo negativo (documento irrelevante), buscando otimizar a dist√¢ncia relativa entre eles no espa√ßo de embeddings.

> üí° **Exemplo Num√©rico:**
>
> Usando a *triplet loss* com a mesma consulta e documentos do exemplo anterior, e considerando $m=0.1$, antes do fine-tuning (onde $s(q, d^+) = 0.6$ e $s(q, d^-) = 0.5$), a perda seria:
>
> $$\mathcal{L} = \max(0, 0.1 - 0.6 + 0.5) = 0$$.
>
> Durante o treinamento, se $s(q, d^+) = 0.4$ e $s(q, d^-) = 0.7$, a perda seria:
>
> $$\mathcal{L} = \max(0, 0.1 - 0.4 + 0.7) = 0.4$$.
>
> O treinamento visa minimizar essa perda, ajustando os embeddings de forma que $s(q, d^+)$ seja significativamente maior que $s(q, d^-)$, respeitando a margem $m$.

<!-- END NEW CONTENT -->

### Fine-tuning de Rankers com Cross-Encoders

Ap√≥s a etapa inicial de retrieval, onde um conjunto de documentos relevantes √© recuperado com base na similaridade de seus embeddings com a consulta, a etapa de *re-ranking* visa refinar a ordena√ß√£o desses documentos, apresentando ao usu√°rio os documentos mais relevantes em primeiro lugar [^2]. O fine-tuning de rankers, utilizando cross-encoders, √© uma t√©cnica poderosa para alcan√ßar esse objetivo.

Diferentemente dos bi-encoders, que geram embeddings independentes para consultas e documentos, os *cross-encoders* processam a consulta e o documento em conjunto, permitindo que o modelo capture intera√ß√µes complexas entre eles. Isso resulta em scores de relev√¢ncia mais precisos, especialmente em dom√≠nios onde a relev√¢ncia depende de rela√ß√µes sutis entre a consulta e o documento.

O fine-tuning de um cross-encoder envolve treinar o modelo para prever a relev√¢ncia de pares consulta-documento. O dataset de treinamento √© composto por pares (consulta, documento) rotulados como relevantes ou irrelevantes. A fun√ß√£o de perda utilizada geralmente √© uma fun√ß√£o de *pairwise ranking*, que penaliza o modelo quando ele atribui um score de relev√¢ncia maior a um documento irrelevante do que a um documento relevante para a mesma consulta.

Uma fun√ß√£o de perda comum para fine-tuning de rankers √© a *hinge loss* adaptada para ranking:

$$
\mathcal{L} = \sum_{q} \sum_{(d_i, d_j) \in \mathcal{P}_q} \max(0, 1 - f(q, d_i) + f(q, d_j))
$$

Onde:

*   $q$ √© uma consulta.
*   $\mathcal{P}_q$ √© o conjunto de pares de documentos $(d_i, d_j)$ para a consulta $q$, onde $d_i$ √© mais relevante que $d_j$.
*   $f(q, d)$ √© o score de relev√¢ncia atribu√≠do pelo cross-encoder ao par (consulta $q$, documento $d$).

**Exemplo:** Considere um cen√°rio em que o sistema RAG √© utilizado para responder a perguntas sobre artigos cient√≠ficos. A etapa inicial de retrieval pode retornar diversos artigos relacionados aos termos da pergunta. No entanto, nem todos os artigos ser√£o igualmente relevantes para responder √† pergunta de forma precisa. Um cross-encoder fine-tuneado com um dataset de pares (pergunta, artigo) rotulados com base na sua relev√¢ncia pode re-rankear os artigos, colocando em primeiro lugar aqueles que cont√™m a resposta direta √† pergunta ou que abordam o t√≥pico de forma mais aprofundada.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma pergunta $q$ = "Qual o impacto da muta√ß√£o no gene BRCA1 no risco de c√¢ncer de mama?" e dois artigos recuperados:
>
> *   $d_i$ = "Mutations in BRCA1 are associated with a significantly increased risk of breast cancer." (altamente relevante)
> *   $d_j$ = "BRCA1 is a gene involved in DNA repair." (relevante, mas menos diretamente relacionado ao *risco*)
>
> Antes do fine-tuning do cross-encoder, os scores poderiam ser $f(q, d_i) = 0.7$ e $f(q, d_j) = 0.6$. Neste caso, a hinge loss seria:
>
> $$\mathcal{L} = \max(0, 1 - 0.7 + 0.6) = \max(0, 0.9) = 0.9$$.
>
> O fine-tuning visa aumentar a diferen√ßa entre os scores, de forma que $f(q, d_i)$ seja significativamente maior que $f(q, d_j)$. Ap√≥s o fine-tuning, poder√≠amos ter $f(q, d_i) = 0.9$ e $f(q, d_j) = 0.5$, resultando em:
>
> $$\mathcal{L} = \max(0, 1 - 0.9 + 0.5) = \max(0, 0.6) = 0.6$$.
>
> Embora a loss ainda n√£o seja zero, ela diminuiu, indicando que o modelo est√° aprendendo a classificar o documento mais relevante com um score mais alto.
>
> | Documento   | Score Inicial | Score Ap√≥s Fine-tuning |
> | :---------- | :------------ | :--------------------- |
> | $d_i$       | 0.7           | 0.9                    |
> | $d_j$       | 0.6           | 0.5                    |
>
> A tabela ilustra o impacto do fine-tuning nos scores de relev√¢ncia.

A utiliza√ß√£o de cross-encoders, apesar de proporcionar maior precis√£o, implica um custo computacional mais elevado em compara√ß√£o com bi-encoders. Isso ocorre porque o cross-encoder precisa processar cada par (consulta, documento) individualmente, o que torna invi√°vel aplic√°-lo a todos os documentos do corpus. Portanto, os cross-encoders s√£o geralmente utilizados na etapa de re-ranking, ap√≥s a etapa inicial de retrieval ter reduzido o n√∫mero de documentos a serem considerados.

**Teorema 1:** A complexidade computacional do re-ranking com cross-encoders √© linear no n√∫mero de documentos recuperados pelo encoder, mas a constante de proporcionalidade √© significativamente maior do que a do encoder.

*Prova.* Sejam $N$ o n√∫mero total de documentos no corpus, e $k$ o n√∫mero de documentos recuperados pelo encoder. O encoder calcula embeddings para todos os $N$ documentos e a consulta, e a similaridade √© calculada entre a consulta e cada documento. O cross-encoder, no entanto, precisa processar *cada um* dos $k$ documentos recuperados *juntamente* com a consulta. Assim, enquanto a complexidade do encoder pode ser expressa como $O(N + Q)$, onde $Q$ √© o custo de codificar a consulta, a complexidade do cross-encoder √© $O(k \cdot (Q + D))$, onde $D$ √© o custo de codificar um documento utilizando o cross-encoder. Dado que $k << N$ tipicamente, o gargalo passa a ser o custo computacional *por par*, que √© inerentemente maior em cross-encoders.

> üí° **Exemplo Num√©rico:**
>
> Imagine um corpus com $N = 1,000,000$ documentos. Um bi-encoder pode recuperar os $k = 100$ documentos mais relevantes.
>
> Suponha que o custo de codificar a consulta com o bi-encoder seja $Q = 100$ unidades de tempo e o custo de codificar um documento seja tamb√©m $D = 100$ unidades de tempo. O custo da etapa de encoding no bi-encoder seria aproximadamente $1,000,000 + 100 = 1,000,100$ unidades.
>
> No cross-encoder, o custo de processar cada par (consulta, documento) √© significativamente maior, digamos $D' = 500$ unidades (devido √† necessidade de processar a consulta e o documento em conjunto).  O custo total para re-rankear os 100 documentos seria $100 * (100 + 500) = 60,000$ unidades.
>
> Embora $60,000 << 1,000,100$, o custo ainda √© consider√°vel, especialmente se o re-ranking precisar ser feito em tempo real para muitas consultas. Isso ilustra a necessidade de equilibrar precis√£o e efici√™ncia ao escolher entre bi-encoders e cross-encoders.

### Conclus√£o

O fine-tuning de encoders e rankers √© uma estrat√©gia eficaz para aprimorar a qualidade da recupera√ß√£o em sistemas RAG, especialmente em dom√≠nios espec√≠ficos [^2]. O fine-tuning de encoders permite adaptar o modelo de embedding para representar melhor a sem√¢ntica do dom√≠nio, enquanto o fine-tuning de rankers com cross-encoders refina a ordena√ß√£o dos resultados recuperados, apresentando ao usu√°rio os documentos mais relevantes. A escolha da fun√ß√£o de perda, do dataset de treinamento e da arquitetura do modelo s√£o fatores cr√≠ticos para o sucesso do fine-tuning.

### Refer√™ncias
[^1]: (Refer√™ncia gen√©rica ao contexto geral de RAG e LLMs)
[^2]: Encoder fine-tuning has demonstrated retrieval quality increases, especially for narrow domain datasets. Ranker fine-tuning utilizes a cross-encoder to re-rank retrieved results, improving pairwise scores.
<!-- END -->