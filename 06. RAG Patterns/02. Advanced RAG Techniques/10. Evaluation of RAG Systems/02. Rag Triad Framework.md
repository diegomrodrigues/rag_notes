## Avalia√ß√£o de Sistemas RAG com o Framework da Tr√≠ade RAG

### Introdu√ß√£o

A avalia√ß√£o de sistemas Retrieval-Augmented Generation (RAG) √© crucial para garantir a efic√°cia na gera√ß√£o de respostas relevantes e fundamentadas. O framework da tr√≠ade RAG prop√µe uma avalia√ß√£o abrangente focada em tr√™s dimens√µes principais: relev√¢ncia do contexto recuperado para a query, *groundedness* (fundamenta√ß√£o) da resposta e relev√¢ncia da resposta para a query [^1]. Esta abordagem multidimensional permite uma an√°lise mais detalhada do desempenho do sistema RAG, identificando √°reas espec√≠ficas para melhoria. Al√©m disso, m√©tricas avan√ßadas como *Mean Reciprocal Rank* (MRR) complementam a taxa de acerto (*hit rate*), fornecendo uma avalia√ß√£o mais refinada da qualidade do contexto recuperado [^1].

Para complementar essa avalia√ß√£o, √© importante considerar tamb√©m a efici√™ncia do sistema RAG em termos de tempo de resposta e custo computacional. M√©tricas como lat√™ncia e utiliza√ß√£o de recursos podem fornecer insights valiosos sobre a escalabilidade e a viabilidade do sistema em cen√°rios de produ√ß√£o.

### Conceitos Fundamentais

O framework da tr√≠ade RAG aborda a avalia√ß√£o de sistemas RAG atrav√©s de tr√™s pilares interconectados:

1.  **Relev√¢ncia do Contexto Recuperado:** Avalia se o contexto recuperado √© pertinente √† query do usu√°rio. Um contexto irrelevante compromete a capacidade do sistema de gerar respostas precisas e informativas.

2.  ***Groundedness* da Resposta:** Mede o grau em que a resposta gerada √© fundamentada no contexto recuperado. Uma resposta bem fundamentada √© baseada em evid√™ncias extra√≠das do contexto, minimizando o risco de alucina√ß√µes ou informa√ß√µes fabricadas.

3.  **Relev√¢ncia da Resposta para a Query:** Avalia se a resposta gerada atende √† necessidade de informa√ß√£o expressa na query do usu√°rio. Uma resposta relevante fornece a informa√ß√£o solicitada de forma clara e concisa.

A avalia√ß√£o da relev√¢ncia do contexto pode ser quantificada atrav√©s de m√©tricas como *hit rate* e *Mean Reciprocal Rank* (MRR) [^1]. O *hit rate* mede a propor√ß√£o de vezes que um documento relevante para a query √© recuperado entre os *n* primeiros resultados. No entanto, o *hit rate* n√£o leva em considera√ß√£o a ordem dos resultados. O MRR, por outro lado, considera a posi√ß√£o do primeiro documento relevante na lista de resultados.

> üí° **Exemplo Num√©rico: Hit Rate**
>
> Imagine que voc√™ tem um sistema RAG e submete 10 queries. Define que, para considerar um sucesso (*hit*), um documento relevante deve estar entre os 3 primeiros resultados (n=3). Se, para 7 das 10 queries, pelo menos um documento relevante aparece entre os 3 primeiros, o *hit rate* √© 7/10 = 0.7 ou 70%. Este n√∫mero indica a frequ√™ncia com que o sistema consegue encontrar documentos relevantes dentro do top 3 resultados.

O MRR √© calculado como a m√©dia dos rec√≠procos dos ranks do primeiro documento relevante para cada query:

$$
MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{rank_i}
$$

onde:

*   $|Q|$ √© o n√∫mero total de queries.
*   $rank_i$ √© o rank do primeiro documento relevante para a query $i$.

Por exemplo, se tivermos tr√™s queries e os primeiros documentos relevantes aparecerem nas posi√ß√µes 1, 3 e 2, respectivamente, o MRR seria:

$$
MRR = \frac{1}{3} \left( \frac{1}{1} + \frac{1}{3} + \frac{1}{2} \right) = \frac{1}{3} \left( 1 + 0.333 + 0.5 \right) = \frac{1.833}{3} \approx 0.611
$$

Um MRR mais alto indica que os documentos relevantes tendem a aparecer em posi√ß√µes mais altas na lista de resultados, o que √© desej√°vel.

> üí° **Exemplo Num√©rico: MRR em um Sistema de Busca**
>
> Suponha que voc√™ tem um sistema de busca e avalia-o com 5 queries. Os resultados mostram as seguintes posi√ß√µes para o primeiro documento relevante em cada query:
>
> *   Query 1: Posi√ß√£o 1
> *   Query 2: Posi√ß√£o 2
> *   Query 3: Posi√ß√£o 5
> *   Query 4: Posi√ß√£o 1
> *   Query 5: Posi√ß√£o 3
>
> Calculando o MRR:
>
> $MRR = \frac{1}{5} \left( \frac{1}{1} + \frac{1}{2} + \frac{1}{5} + \frac{1}{1} + \frac{1}{3} \right) = \frac{1}{5} (1 + 0.5 + 0.2 + 1 + 0.33) = \frac{2.03}{5} \approx 0.61$
>
> Este MRR de 0.61 indica que, em m√©dia, o primeiro documento relevante aparece relativamente alto nos resultados da busca.

A avalia√ß√£o de *groundedness* e relev√¢ncia da resposta pode ser realizada por meio de avaliadores humanos ou utilizando modelos de linguagem treinados para avaliar a qualidade da informa√ß√£o gerada. Em ambos os casos, √© essencial definir crit√©rios claros e objetivos para garantir a consist√™ncia e a confiabilidade da avalia√ß√£o.

Al√©m do MRR, outra m√©trica √∫til para avaliar a qualidade da ordena√ß√£o dos resultados √© o Normalized Discounted Cumulative Gain (NDCG).

**Teorema 1** (Normalized Discounted Cumulative Gain - NDCG): O NDCG mede a relev√¢ncia dos documentos recuperados, ponderando a relev√¢ncia por um fator de desconto logar√≠tmico baseado na posi√ß√£o do documento.

A f√≥rmula do NDCG √© dada por:

$$
NDCG@k = \frac{DCG@k}{IDCG@k}
$$

Onde:

$$
DCG@k = \sum_{i=1}^{k} \frac{rel_i}{\log_2(i+1)}
$$

e *rel<sub>i</sub>* √© a relev√¢ncia do documento na posi√ß√£o *i*, e *IDCG@k* √© o DCG ideal para os *k* documentos mais relevantes.

O NDCG@k normaliza o DCG pelo DCG ideal, garantindo que os valores estejam entre 0 e 1, facilitando a compara√ß√£o entre diferentes queries.

**Exemplo:** Se tivermos 5 documentos com relev√¢ncias [3, 2, 3, 0, 1] e k=5, ent√£o:

DCG@5 = (3/log2(2)) + (2/log2(3)) + (3/log2(4)) + (0/log2(5)) + (1/log2(6)) ‚âà 3 + 1.26 + 1.5 + 0 + 0.39 = 6.15

Se a ordena√ß√£o ideal fosse [3, 3, 2, 1, 0], ent√£o:

IDCG@5 = (3/log2(2)) + (3/log2(3)) + (2/log2(4)) + (1/log2(5)) + (0/log2(6)) ‚âà 3 + 1.89 + 1 + 0.43 + 0 = 6.32

NDCG@5 = 6.15 / 6.32 ‚âà 0.97

O NDCG √© particularmente √∫til quando os n√≠veis de relev√¢ncia dos documentos variam (por exemplo, "altamente relevante", "relevante", "pouco relevante", "irrelevante"), pois ele leva em considera√ß√£o a granularidade da relev√¢ncia.

> üí° **Exemplo Num√©rico: Comparando Sistemas com NDCG**
>
> Suponha que temos dois sistemas RAG, Sistema A e Sistema B, e queremos compar√°-los usando NDCG@3. Avaliamos ambos os sistemas com a mesma query e obtemos os seguintes resultados (relev√¢ncia de 0 a 3):
>
> *   **Sistema A:** Relev√¢ncias = [3, 2, 1, 0, 0]
> *   **Sistema B:** Relev√¢ncias = [2, 3, 0, 1, 0]
>
> Para o Sistema A:
> $DCG@3 = \frac{3}{\log_2(2)} + \frac{2}{\log_2(3)} + \frac{1}{\log_2(4)} \approx 3 + 1.26 + 0.5 = 4.76$
> Ordena√ß√£o Ideal: [3, 2, 1], ent√£o $IDCG@3 \approx 4.76$
> $NDCG@3 = \frac{4.76}{4.76} = 1.0$
>
> Para o Sistema B:
> $DCG@3 = \frac{2}{\log_2(2)} + \frac{3}{\log_2(3)} + \frac{0}{\log_2(4)} \approx 2 + 1.89 + 0 = 3.89$
> Ordena√ß√£o Ideal: [3, 2, 0], ent√£o $IDCG@3 \approx 4.76$
> $NDCG@3 = \frac{3.89}{4.76} \approx 0.82$
>
> Neste caso, o Sistema A tem um NDCG@3 de 1.0, enquanto o Sistema B tem um NDCG@3 de 0.82. Isso indica que o Sistema A ordenou os documentos de forma mais relevante nos tr√™s primeiros resultados do que o Sistema B.

**Exemplo de Aplica√ß√£o:**

Considere um sistema RAG projetado para responder a perguntas sobre artigos cient√≠ficos. Para avaliar o sistema usando o framework da tr√≠ade RAG, podemos seguir os seguintes passos:

1.  **Relev√¢ncia do Contexto:** Submeta uma s√©rie de queries ao sistema e avalie se os artigos recuperados s√£o relevantes para cada query. Utilize *hit rate* e MRR (ou NDCG) para quantificar a relev√¢ncia do contexto.

2.  ***Groundedness* da Resposta:** Avalie se as respostas geradas pelo sistema s√£o baseadas em informa√ß√µes encontradas nos artigos recuperados. Verifique se as afirma√ß√µes feitas na resposta podem ser verificadas nos artigos.

> üí° **Exemplo Num√©rico: Avalia√ß√£o de Groundedness**
>
> Digamos que avaliamos 100 respostas geradas pelo sistema RAG. Para cada resposta, verificamos se cada afirma√ß√£o feita na resposta pode ser encontrada no contexto recuperado. Definimos as seguintes categorias:
>
> *   Totalmente Fundamentada: Todas as afirma√ß√µes s√£o suportadas pelo contexto.
> *   Parcialmente Fundamentada: Algumas afirma√ß√µes s√£o suportadas, mas outras n√£o.
> *   N√£o Fundamentada: Nenhuma afirma√ß√£o √© suportada pelo contexto.
>
> Os resultados s√£o:
>
> *   Totalmente Fundamentada: 60 respostas
> *   Parcialmente Fundamentada: 30 respostas
> *   N√£o Fundamentada: 10 respostas
>
> Podemos calcular a porcentagem de respostas totalmente fundamentadas como 60/100 = 60%. Este n√∫mero indica a propor√ß√£o de respostas que s√£o totalmente suportadas pelo contexto recuperado. Idealmente, buscar√≠amos um valor o mais pr√≥ximo poss√≠vel de 100%. As respostas "Parcialmente Fundamentadas" e "N√£o Fundamentadas" indicam √°reas onde o sistema RAG precisa ser aprimorado para garantir maior *groundedness*.

3.  **Relev√¢ncia da Resposta:** Avalie se as respostas geradas pelo sistema atendem √†s necessidades de informa√ß√£o expressas nas queries. Verifique se as respostas s√£o completas, precisas e f√°ceis de entender.

> üí° **Exemplo Num√©rico: Relev√¢ncia da Resposta Avaliada por Humanos**
>
> Ap√≥s verificar a *groundedness*, envie as mesmas 100 respostas para avaliadores humanos. Pe√ßa para eles avaliarem a relev√¢ncia da resposta para a query original em uma escala de 1 a 5 (1 = Irrelevante, 5 = Altamente Relevante).
>
> Ap√≥s coletar as avalia√ß√µes, calcule a pontua√ß√£o m√©dia de relev√¢ncia. Por exemplo, se a pontua√ß√£o m√©dia for 4.2, isso indica que, em m√©dia, os avaliadores consideraram as respostas como "relevantes" ou "altamente relevantes" para as queries.
>
> Al√©m da m√©dia, tamb√©m √© √∫til analisar a distribui√ß√£o das pontua√ß√µes. Se houver uma grande varia√ß√£o nas pontua√ß√µes (por exemplo, muitas respostas com pontua√ß√µes de 1 ou 2, e outras com pontua√ß√µes de 4 ou 5), isso pode indicar que o sistema RAG tem um desempenho inconsistente e que algumas queries s√£o melhor atendidas do que outras.

### Conclus√£o

O framework da tr√≠ade RAG oferece uma abordagem abrangente e estruturada para a avalia√ß√£o de sistemas RAG [^1]. Ao considerar a relev√¢ncia do contexto, o *groundedness* da resposta e a relev√¢ncia da resposta, este framework permite uma an√°lise detalhada do desempenho do sistema, identificando √°reas para melhoria e garantindo a gera√ß√£o de respostas precisas, informativas e fundamentadas. A utiliza√ß√£o de m√©tricas como MRR e NDCG complementa a avalia√ß√£o, fornecendo uma vis√£o mais refinada da qualidade do contexto recuperado [^1].

Al√©m disso, a avalia√ß√£o da efici√™ncia do sistema em termos de lat√™ncia e custo √© crucial para garantir a viabilidade em cen√°rios de produ√ß√£o. M√©tricas como tempo de resposta por query e utiliza√ß√£o de recursos (CPU, mem√≥ria) podem ser monitoradas e otimizadas para melhorar o desempenho geral do sistema RAG.

> üí° **Exemplo Num√©rico: Avalia√ß√£o de Lat√™ncia**
>
> Imagine que voc√™ est√° executando um sistema RAG em produ√ß√£o e monitora o tempo de resposta para cada query. Ap√≥s um per√≠odo de coleta de dados, voc√™ observa as seguintes estat√≠sticas:
>
> *   Tempo m√©dio de resposta: 500 ms
> *   Desvio padr√£o: 200 ms
> *   Tempo de resposta m√°ximo: 1500 ms
>
> Al√©m disso, voc√™ divide as queries em diferentes categorias (por exemplo, queries simples vs. queries complexas) e observa que as queries complexas t√™m um tempo m√©dio de resposta significativamente maior do que as queries simples.
>
> Esta an√°lise de lat√™ncia fornece informa√ß√µes importantes sobre o desempenho do sistema RAG. O tempo m√©dio de resposta indica a velocidade geral do sistema, enquanto o desvio padr√£o indica a variabilidade dos tempos de resposta. O tempo de resposta m√°ximo pode ser usado para identificar potenciais gargalos ou problemas de desempenho. A an√°lise por categoria de query pode revelar √°reas onde a otimiza√ß√£o √© necess√°ria (por exemplo, melhorar o desempenho em queries complexas).

### Refer√™ncias
[^1]: Informa√ß√µes fornecidas no contexto.
<!-- END -->